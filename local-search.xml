<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>百度:基于大模型的广告检索（复赛）</title>
    <link href="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2%EF%BC%88%E5%A4%8D%E8%B5%9B%EF%BC%89/"/>
    <url>/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2%EF%BC%88%E5%A4%8D%E8%B5%9B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>百度</tag>
      
      <tag>大模型</tag>
      
      <tag>搜广推</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BIGAI2—微调Qwen-7B生成1-Hop描述</title>
    <link href="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/"/>
    <url>/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在生成QA时，我们希望生成的数据具有<mark>一定的复杂度</mark>，并且具有<mark>生活化</mark>的特点。</p><p>最基本的一个想法是：当我们需要某个子图（Region Graph）中的物体信息，便直接从已有信息Info中查找，并送入GPT生成QA；然而，基于这种方法所生成的QA十分的简单。我利用GPT-4简单生成了一个Demo，如下图：</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Base.png" alt="Demo:直接通过知识图谱生成的QA"></p><p>很显然这种数据不具有<mark>多样性</mark>，也不满足我们的要求。我们认为在若干定语修饰后的QA可以极大地丰富数据多样性：</p><ul><li>地毯左面紧挨的儿童椅和行李箱旁的双人床哪个更大？</li><li>空调下面的挂画和墙壁右边的衣服哪个数量更多？</li></ul><p>本质上，我们希望对原有数据增加一定的定语——<mark>将程式化的短语转变为流畅丰富的句子</mark>，效果如下图：</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Improve.png" alt="1-Hop描述（右）可以丰富QA内容"></p><p>生成大批量数据离不开大模型的帮助。出于对计算资源以及生成效率的考虑，我们没有选择API或是参数量更大的模型，而是在本地部署Qwen-7B，并期待通过微调可以让其生成我们希望的1-Hop描述，如下图框出部分：</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/%E6%B5%81%E7%A8%8B.png" alt="生成QA数据集流程"></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="原理浅析"><a href="#原理浅析" class="headerlink" title="原理浅析"></a>原理浅析</h2><p>除了SFT之外，人们采用更少计算资源进行微调的方法称为PEFT，其中又包含若干种方法。</p><p>我在网上找到的一个时间轴，可以很好地展示发展历程，如下图：</p><p><img src="https://oscimg.oschina.net/oscnet/9ac87619-bf4f-41d6-9c94-d7a8cb030acb.png" alt="主流微调方法的发展"></p><p>因为显卡资源的限制，我们采用P-tuning来进行微调，从而生成1-Hop描述，<a href="https://arxiv.org/abs/2103.10385">论文原文</a>。</p><p>与Prefix-tuning直接给Prompt增加前缀不同，P-tuning在Prompt中随意加入Embedding后的Virtual Token，如下图：</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/P-tuning.png" alt="P-tuning结构示意图"></p><h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><p>考虑到时间成本以及Token成本，我们决定在Info中选取<mark>部分数据</mark>，将这一部分数据和利用GPT-4生成的1-Hop描述拼接在一起，形成了微调用的训练数据集。</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/%E6%95%B0%E6%8D%AE.png" alt="准备微调数据"></p><p>通过将人工制作的模板（如下图）与Info中的信息作为Prompt，我们利用GPT-4进行生成；生成的内容需要进一步通过正则表达式的处理，最终成为1-Hop描述。</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT1.png" alt="模板1"></p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT2.png" alt="模板2"></p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT3.png" alt="模板3"></p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>Qwen-7B接收到的Prompt由另一套模板以及Info中的信息组成，如下图：</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Prompt.png" alt="询问功能的模板"></p><p>代码需要放到HGX集群上跑，但是需要排队很久。之前微调的时候loss从14下降到8左右。</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/HGX.png" alt="GPU集群"></p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1><p>整个对Info进行增强生成1-Hop描述的过程很慢，最终也没有将整个map的3000个视角（1000个场景*3个视角）遍历完。</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/%E7%BB%93%E6%9E%9C.png" alt="结果"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>BIGAI</tag>
      
      <tag>实习</tag>
      
      <tag>数据生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
