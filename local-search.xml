<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>图</title>
    <link href="/2024/10/10/%E5%9B%BE/"/>
    <url>/2024/10/10/%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="拓扑排序">拓扑排序</h1><ul><li>有向无环图（DAG）才有拓扑排序</li></ul><p><a href="https://leetcode.cn/problems/course-schedule/">207. Course Schedule</a></p><ol type="1"><li>将入度为0的点放入队列中</li><li>将队列中的点弹出，已弹出的点入度置为-1,</li><li>在邻接矩阵该点对应行中所有点的入度减1</li><li>直到该队列没有点后，观察入度列表是否均为-1</li></ol><h1 id="前缀树">前缀树</h1><p><a href="https://leetcode.cn/problems/implement-trie-prefix-tree/">208. Implement Trie (Prefix Tree)</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>图</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>词向量</title>
    <link href="/2024/10/09/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    <url>/2024/10/09/%E8%AF%8D%E5%90%91%E9%87%8F/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="word2vec">1、Word2Vec</h1><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/sequence_model/word_representation/word2vec.html#word2vec">Word2Vec: 一种词向量的训练方法</a></li></ol><h2 id="cbow">1.1 CBOW</h2><ul><li>利用上下文单词预测中心词</li><li>训练快</li></ul><h2 id="skip-gram">1.2 Skip-gram</h2><ul><li>利用中心词预测上下文窗口词</li><li>训练速度慢、不会回避生僻词</li></ul><h1 id="glove">2、GloVe</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>词向量</tag>
      
      <tag>Embedding</tag>
      
      <tag>Word2Vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>正则化、MLE、MAP</title>
    <link href="/2024/10/08/%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81MLE%E3%80%81MAP/"/>
    <url>/2024/10/08/%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81MLE%E3%80%81MAP/</url>
    
    <content type="html"><![CDATA[<h1 id="正则化">正则化</h1><ul><li>L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择</li><li>L2正则化可以防止模型过拟合；一定程度上，L1也可以防止过拟合</li></ul><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://blog.csdn.net/jinping_shi/article/details/52433975">机器学习中正则化项L1和L2的直观理解</a></li><li><a href="https://www.cnblogs.com/zingp/p/10375691.html">深入理解L1、L2正则化</a></li></ol><h1 id="极大似然估计-mle">极大似然估计 MLE</h1><p>考研时候概率论章节会有一道题要求最大似然估计（MLE），当时的理解是在观测到样本并且已知分布的情况下，估计最有可能让样本拟合的参数。</p><p>最近上课的时候提到最大后验概率（MAP）在形式上和MLE差不太多，我的问题是为什么它们会同时存在呢，有什么本质上的区别吗？</p><hr><p>根据查找资料，发现MLE，MAP分别是由是频率学派和贝叶斯学派提出的，区别在于前者认为。MLE和MAP，都是总体的（某个指标的）分布类型已知前提下的一种参数估计方法。这两种方法的目标就是对分布的未知参数进行估计，以得到确定的分布。在小样本的情况下，MLE容易产生过拟合。</p><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://blog.csdn.net/wxc971231/article/details/115054494">一文看懂 “极大似然估计” 与 “最大后验估计” —— 极大似然估计篇</a></li></ol><h1 id="最大后验概率-map">最大后验概率 MAP</h1><p>MAP在MAP的基础上，还考虑了数据的先验概率，在数据较少或者需要结合先验知识的情况下进行更准确的参数估计。在数据集变大后，MLE和MAP的差异不大。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>正则化</tag>
      
      <tag>贝叶斯</tag>
      
      <tag>最大后验概率</tag>
      
      <tag>最大似然估计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>主题模型</title>
    <link href="/2024/10/03/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/10/03/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="topic-model-主题模型">〇、Topic Model 主题模型</h1><p>主题模型是提取文本抽象主题相似度的一种<mark>统计模型</mark>，弥补了TF-IDF未考虑语义的缺陷</p><p><strong>参考</strong>：</p><ol type="1"><li><p><a href="https://blog.csdn.net/qq_16555103/article/details/95373563">主题模型LDA、NMF、LSA</a></p></li><li><p><a href="https://blog.csdn.net/m0_56642803/article/details/134586485">27、潜在语义分析（LSA）</a></p></li><li><p><a href="https://www.bilibili.com/video/BV1Nt4y137iS/?spm_id_from=333.337.search-card.all.click&amp;vd_source=19303cc41e28593826f02ec94c4fc790">词向量 | glove | 原理简介+代码简析 | global vectors for word representation【python-glove】</a></p></li></ol><h1 id="lsa-潜在语义分析">1、LSA 潜在语义分析</h1><p><strong>定义</strong>：无监督学习方法，主要用于文本的话题分析，通过<mark>对词语-文档矩阵进行奇异值分解</mark>得到Document-Topic矩阵<span class="math inline">\(U_k\)</span>和Term-Topic矩阵<span class="math inline">\(V_k^T\)</span></p><p><strong>优点</strong>：降维；捕捉文本的潜在语义关系</p><p><strong>缺点</strong>：缺乏可解释性；对数据预处理方式敏感</p><p><strong>pLSA</strong> <strong>概率潜在语义分析</strong>：</p><h1 id="lda-隐含狄利克雷分布">2、LDA 隐含狄利克雷分布</h1><p><strong>定义</strong>：最终得到每个主题的代表性词汇分布，以及每篇文档的主题概率分布</p><h1 id="nmf-非负矩阵分解">3、NMF 非负矩阵分解</h1><p><strong>定义</strong>：通过<mark>分解单词-文档矩阵矩阵</mark>得到单词-主题矩阵和主题-文档矩阵（均为非负矩阵），得到文档主题</p><p><strong>优点</strong>：减少噪音</p><p><strong>缺点</strong>：非凸优化，可能陷入局部最优，对初始化很敏感；对数据预处理方式和损失函数选择很敏感</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>SVD</tag>
      
      <tag>分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>降维</title>
    <link href="/2024/10/03/%E9%99%8D%E7%BB%B4/"/>
    <url>/2024/10/03/%E9%99%8D%E7%BB%B4/</url>
    
    <content type="html"><![CDATA[<h1 id="pca-主成分分析">1、PCA 主成分分析</h1><p><strong>本质</strong>：将高维向量投影到低维度（另一组正交基底），可理解为在假定变量间存在线性关系的前提下，通过某些向量的线性组合替代了另一些向量</p><p><strong>方法</strong>：求样本的协方差矩阵，目的是要该矩阵在新的空间中非对角元的值为零，并且对角线上的值按降序排列</p><p><strong>优点</strong>：适合可视化，在特征工程很重要</p><p><strong>缺点</strong>：可解释性较差，对于变量间的非线性关系，PCA不能捕捉到</p><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://www.showmeai.tech/article-detail/198">图解机器学习 | 降维算法详解</a></li><li><a href="https://blog.csdn.net/heyijia0327/article/details/26763903">SVD 与 PCA 的直观解释(4): PCA 主成分分析</a></li></ol><h1 id="svd-奇异值分解">2、SVD 奇异值分解</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>SVD</tag>
      
      <tag>机器学习</tag>
      
      <tag>PCA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搜索</title>
    <link href="/2024/10/03/%E6%90%9C%E7%B4%A2/"/>
    <url>/2024/10/03/%E6%90%9C%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="tf-idf-词频-逆文档频率">1、TF-IDF 词频-逆文档频率</h1><p>TF-IDF用来衡量一个单词的重要程度，当其在<mark>一个文档里出现频率越大</mark>、且<mark>其他文档出现频率越少</mark>时，说明这个单词越<mark>独特</mark> <span class="math display">\[\text{TF-IDF(t,d,D)=TF(t,d)×IDF(t,D)}\]</span> <strong>优点</strong>：无需训练，高效</p><p><strong>缺点</strong>：并没有考虑语义，对于常用词以及大数据集效果不好</p><h1 id="bm25">2、 BM25</h1><p>用来计算Query和Doc之间的匹配度，由Query和其中单词的相关性、每个单词的权重（IDF）、每个单词和文档之间的相关性来衡量；其中BM25并未像TF-IDF一样通过计算TF来确定相关性，因为：<mark>每一个词对于文档相关性的分数不会超过一个特定的阈值</mark>，当词出现的次数达到一个阈值后，其影响不再线性增长，而这个阈值会跟文档本身有关</p><p><strong>优点</strong>：比TF-IDF效果好，更具有鲁棒性</p><p><strong>缺点</strong>：并没有考虑语义，对于稀疏语料以及短文档效果不好</p><p><strong>参考</strong>：</p><p><a href="https://blog.csdn.net/laobai1015/article/details/120143102">BM25算法 原理简介</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>搜索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·IV</title>
    <link href="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/"/>
    <url>/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;  margin-top: 20px;}</style><h1 id="一进度">一、进度</h1><p><strong>9月30日~10月20日</strong></p><p>对Moment进行Prompt-Tuning</p><h1 id="二调研">二、调研</h1><ol type="1"><li><a href="https://arxiv.org/abs/2405.14616">TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting</a></li><li><a href="https://arxiv.org/abs/2405.00946">SparseTSF: Modeling Long-term Time Series Forecasting with 1k Parameters</a></li></ol><h2 id="timemixer">1. TimeMixer</h2><p><img src="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/mixer.png"></p><h2 id="sparsetsf">2. SparseTSF</h2><p><img src="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/sparse.png"></p><h1 id="三moment结合prompt">三、MOMENT结合Prompt</h1><h2 id="阅读prompt-tuning代码">1. 阅读Prompt-Tuning代码</h2><ul><li>Prompt-Tuning需要训练一个将Prompt映射到另一个向量的矩阵，从而让输出向量和Input在输入Encoder后更好地优化损失</li><li>我应该需要写一个<code>get_peft_model</code>的函数，将<code>moment</code>和Prompt捆绑在一起，返回一个<code>PeftModel</code>类</li><li>但如何做到samplewise是一个比较难的事情</li></ul><p><img src="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/ptuning1.png"></p><ul><li>其中PeftModel有一些必须实现的方法：<ul><li><code>from_pretrained</code></li><li><code>prompt_encoder</code></li><li><code>save_prompt_embedding</code></li><li><code>get_prompt</code></li><li><code>save_model</code></li></ul></li></ul><p><img src="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/ptuning2.png"></p><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://blog.csdn.net/qq_35812205/article/details/131647749">【LLM】Prompt tuning大模型微调实战</a></li><li><a href="https://huggingface.co/docs/peft/en/index">Hugging Face</a></li></ol><h2 id="修改l2pmomentpipeline类">2. 修改L2pMOMENTPipeline类</h2><h3 id="计算key和embed前对x_embed的处理8764102481024">2.1 计算Key和Embed前对x_embed的处理（[8,7,64,1024]—[8,1024]）</h3><p><img src="/p1.png"></p><h3 id="prompt和embed拼接时形状没有对齐">2.2 Prompt和Embed拼接时形状没有对齐</h3><ul><li><code>batched_prompt</code>和<code>x_embed</code>维度不对齐，通过复制将前者和后者对齐</li><li>因为moment encoder输入维度为512，取x_embed前prompt_len个位置留给prompt，拼接</li></ul><p><img src="/image-20241012114217149.png"></p><h3 id="将拼接完成待输入进encoder的input-mask进行修改">2.3 将拼接完成，待输入进Encoder的Input Mask进行修改</h3><p><img src="/image-20241012144836693.png"></p><p>1个Epoch跑完后，进入测试前报错：</p><p><img src="/image-20241012150046186.png"></p><p><img src="/image-20241012145951021.png"></p><h2 id="fine-tune">3. Fine-Tune</h2><h3 id="有prompt初始化为0-zero-shot">3.1 有Prompt（初始化为0） Zero Shot</h3><p><img src="/image-20241012160919884.png"></p><h3 id="无prompt-fine-tune">3.2 无Prompt Fine-Tune</h3><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">数据</td><td style="text-align: center;"><code>train_dataset = InformerDataset(data_split="train", random_seed=13, forecast_horizon=192)</code></td></tr><tr class="even"><td style="text-align: center;">训练权重</td><td style="text-align: center;"><code>head.linear.weight,head.linear.bias</code></td></tr><tr class="odd"><td style="text-align: center;">损失函数</td><td style="text-align: center;"><code>torch.nn.MSELoss()</code></td></tr><tr class="even"><td style="text-align: center;">优化器</td><td style="text-align: center;"><code>torch.optim.Adam(model.parameters(), lr=1e-4)</code></td></tr><tr class="odd"><td style="text-align: center;">学习率</td><td style="text-align: center;"><code>OneCycleLR(optimizer, max_lr=1e-4, total_steps=total_steps, pct_start=0.3)</code></td></tr><tr class="even"><td style="text-align: center;">Epoch</td><td style="text-align: center;">1</td></tr><tr class="odd"><td style="text-align: center;">Test MSE</td><td style="text-align: center;">0.420</td></tr><tr class="even"><td style="text-align: center;">Test MAE</td><td style="text-align: center;">0.430</td></tr></tbody></table><p><img src="/2024/09/29/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7IV/ft.png"></p><h3 id="有prompt-fine-tune">3.3 有Prompt Fine-Tune</h3><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">I</th><th style="text-align: center;">II</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">prompt_length</td><td style="text-align: center;">4</td><td style="text-align: center;">4</td></tr><tr class="even"><td style="text-align: center;">embedding_key</td><td style="text-align: center;">mean</td><td style="text-align: center;">mean</td></tr><tr class="odd"><td style="text-align: center;">prompt_key_init</td><td style="text-align: center;">zero</td><td style="text-align: center;">zero</td></tr><tr class="even"><td style="text-align: center;">pool_size</td><td style="text-align: center;">8</td><td style="text-align: center;">8</td></tr><tr class="odd"><td style="text-align: center;">top_k</td><td style="text-align: center;">1</td><td style="text-align: center;">1</td></tr><tr class="even"><td style="text-align: center;">Epoch</td><td style="text-align: center;">1</td><td style="text-align: center;">10</td></tr><tr class="odd"><td style="text-align: center;">Test MSE</td><td style="text-align: center;">0.439</td><td style="text-align: center;">0.434</td></tr><tr class="even"><td style="text-align: center;">Test MAE</td><td style="text-align: center;">0.449</td><td style="text-align: center;">0.441</td></tr><tr class="odd"><td style="text-align: center;">备注</td><td style="text-align: center;">可能是Input_mask的问题</td><td style="text-align: center;">如果Forecast模式，Prompt也会被当做数据原本的一段</td></tr></tbody></table><figure><img src="/image-20241012141513092.png" alt="I"><figcaption aria-hidden="true">I</figcaption></figure><figure><img src="/image-20241012160103166.png" alt="II"><figcaption aria-hidden="true">II</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>类增量学习</tag>
      
      <tag>增量学习</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>动态规划</title>
    <link href="/2024/09/24/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2024/09/24/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p><strong>解题步骤：</strong></p><ol type="1"><li>确定dp数组以及下标的含义</li><li>确定递推公式</li><li>dp数组如何初始化</li><li>确定遍历顺序</li><li>举例推导dp数组</li></ol><h1 id="背包问题">1、 背包问题</h1><h2 id="背包">1.1 0-1背包</h2><ul><li>遍历背包和物品的顺序十分重要！</li></ul><p><a href="https://programmercarl.com/%E8%83%8C%E5%8C%85%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%8001%E8%83%8C%E5%8C%85-1.html#%E6%80%9D%E8%B7%AF">二维数组解决0-1背包问题</a></p><ul><li>先遍历物品，再遍历背包：比较的是<mark>不放该物品的最大值</mark>和<mark>留出该物品空间时的最大值+该物品的价值</mark>中的较大者</li><li>先遍历背包，再遍历物品：同上面，是一样的，都是找到左上的值</li></ul><p><a href="https://programmercarl.com/%E8%83%8C%E5%8C%85%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%8001%E8%83%8C%E5%8C%85-2.html#%E7%AE%97%E6%B3%95%E5%85%AC%E5%BC%80%E8%AF%BE">一维数组解决0-1背包问题</a></p><ul><li>只能<mark>先遍历物品</mark>（否则每个<code>dp[j]</code>就只能放一个东西），再倒序遍历背包（<mark>倒序</mark>可以保证物品只放入一次）</li></ul><h2 id="完全背包">1.2 完全背包</h2><p><a href="https://programmercarl.com/%E8%83%8C%E5%8C%85%E6%80%BB%E7%BB%93%E7%AF%87.html#%E6%80%BB%E7%BB%93">代码随想录总结</a></p><ul><li><p><strong>装满最大最小个数</strong>：嵌套顺序无所谓<code>dp[i] = min(dp[i], dp[i-num]+1)或者dp[i] = max(dp[i], dp[i-num]+1)</code></p></li><li><p><strong>True，False</strong>：<code>dp[i] = dp[i] or dp[i-num]</code></p></li><li><p><strong>排列</strong>：先遍历背包，再遍历物品</p><p><img src="/2024/09/24/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/排列.png"></p></li><li><p><strong>组合</strong>：先遍历物品，再遍历背包</p><p><img src="/2024/09/24/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/组合.png"></p></li></ul><h1 id="打家劫舍">2、打家劫舍</h1><h1 id="股票问题">3、股票问题</h1><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-with-cooldown/">309. Best Time to Buy and Sell Stock with Cooldown</a></p><ul><li>同一天可能有多个状态<code>dp[i][0],dp[i][1]</code>（结束是否有股票？）</li></ul><h1 id="子序列问题">4、子序列问题</h1><p>可以用二维数组来记录，该数组大小要比数组大1，方便初始化；</p><p>注意区分最长公共子序列是否<mark>需要连续</mark>，如果不需要连续，则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> nums1[i-<span class="hljs-number">1</span>]==nums2[j-<span class="hljs-number">1</span>]:dp[i][j]=dp[i-<span class="hljs-number">1</span>][j-<span class="hljs-number">1</span>]+<span class="hljs-number">1</span><br><span class="hljs-keyword">else</span>:dp[i][j]=<span class="hljs-built_in">max</span>(dp[i-<span class="hljs-number">1</span>][j],dp[i][j-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><a href="https://leetcode.cn/problems/maximum-length-of-repeated-subarray/">718. Maximum Length of Repeated Subarray</a></p><p><a href="https://leetcode.cn/problems/edit-distance/">72. Edit Distance</a></p><p>编辑距离的题目在<mark>初始化</mark>的时候不太一样</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>动态规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贪心算法</title>
    <link href="/2024/09/23/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
    <url>/2024/09/23/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p><a href="https://leetcode.cn/problems/maximum-subarray/">53. Maximum Subarray</a></p><p><a href="https://leetcode.cn/problems/best-time-to-buy-and-sell-stock-ii/">122. Best Time to Buy and Sell Stock II</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>贪心算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·III</title>
    <link href="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/"/>
    <url>/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="时间线">〇、时间线</h1><p><strong>9月19日</strong></p><ol type="1"><li>阅读有关时间序列在线增量学习的工作</li></ol><h1 id="一相关工作">一、相关工作</h1><ol type="1"><li><a href="https://arxiv.org/abs/2309.12659">OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling</a></li><li><a href="https://arxiv.org/abs/2202.11672">Learning Fast and Slow for Online Time Series Forecasting</a></li><li><a href="https://arxiv.org/abs/2106.13008">Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</a></li></ol><h2 id="onenet">1、OneNet</h2><h2 id="背景">1.1 背景</h2><ul><li><p>在线学习：对按序到达的数据进行学习，在每一步学习并更新未来数据的最佳预测值</p></li><li><p>遗憾/悔值 Regret &amp; 遗憾界：与离线学习最优损失之差的求和，用于评估在线学习的性能</p></li><li><p>概念漂移 Concept Shift：随着新数据的到来，数据整体的<mark>分布产生了变化</mark></p></li><li><p>指数加权平均 EWA：建模时间序列的一种方法，每个观测值权重是按照指数衰减的方式确定的，<mark>最近的观测值被赋予更高的权重</mark></p><ul><li>Slow Switch Phenomenon：对新到达的数据的Concept Shift会很慢适应</li></ul></li><li><p>指数梯度下降 EGD：借鉴EWA思想的用于在线学习的梯度下降方法，可以得到<mark>更小的遗憾界</mark></p></li><li><p>Cross-Time/Cross-Variable：假设变量独立，仅依赖时间/仅依赖变量（表：<mark>同时处理两种依赖性不能</mark>带来性能提升）</p><ul><li>变量独立性（Cross-Time）对于提高模型在概念漂移情况下的鲁棒性至关重要，Cross-Variable往往会过拟合；但缺乏变量间的信息在变量较少的数据集上效果不佳，如在ETT这些变量较少的数据集上，只考虑时间依赖效果不佳</li></ul><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/onenet表格.png"></p></li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://welts.xyz/2022/02/28/online/">在线学习简介</a></li><li><a href="https://blog.csdn.net/weixin_47692652/article/details/128641345">在线学习(online learning)——Chapter 1 What is online learning</a></li><li><a href="https://www.lamda.nju.edu.cn/mlt2023/Slides/chapter_8.pdf">遗憾界</a></li></ol><h2 id="onenet-结构">1.2 OneNet 结构</h2><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/onenet.png"></p><ul><li>f1：Cross-Time预测器</li><li>f2：Cross-Variable预测器</li></ul><h3 id="proposition-1online-convex-programming-boundocp">1.2.1 Proposition 1：Online Convex Programming Bound（OCP）</h3><ul><li>之前工作的问题：EGD在好的表现和缓慢适应新样本的速度中存在trade-off</li><li>本篇工作的改进：将长期信息和短期信息结合，OCP的作用是通过这些信息对两种预测器的权重进行调整</li></ul><figure><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/ocp.png" alt="OCP的Regret Bound"><figcaption aria-hidden="true">OCP的Regret Bound</figcaption></figure><h3 id="proposition-2k-step-re-initialize-algorithm">1.2.2 Proposition 2：K-step re-initialize algorithm</h3><ul><li>使用EGD更新长时间信息对应的权重w；b对应短时间信息权重；利用强化学习框架RvS从w和短期内专家的表现中学习b</li></ul><h3 id="decoupled-strategies">1.2.3 Decoupled Strategies</h3><ul><li>Cross-Time预测器总比Cross-Variable表现好的时候，前者的权重会趋于1，损失函数对后者的梯度趋于0，从而不被训练。因此对于预测器和OCP的损失函数分别定义为<span class="math inline">\(\mathcal L(\widetilde{y}_1,y)+\mathcal L(\widetilde{y}_2,y)\)</span>和<span class="math inline">\(\mathcal L(w_1*\widetilde{y}_1+w_2*\widetilde{y}_2,y)\)</span></li></ul><h2 id="fsnet">2. FSNet</h2><figure><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/fsnet.png" alt="基于TCN Backbone"><figcaption aria-hidden="true">基于TCN Backbone</figcaption></figure><h2 id="autoformer">3. Autoformer</h2><h3 id="贡献">3.1 贡献</h3><ul><li>Autoformer嵌入了分解模块，将时间序列数据分解</li><li>基于随机过程的Auto-Correlation机制，比自注意力机制提高了计算效率和信息利用率</li></ul><h3 id="原理">3.2 原理</h3><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/ppt1.png"></p><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/ppt2.png"></p><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/ppt3.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>增量学习</tag>
      
      <tag>在线学习</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>回溯</title>
    <link href="/2024/09/19/%E5%9B%9E%E6%BA%AF/"/>
    <url>/2024/09/19/%E5%9B%9E%E6%BA%AF/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><ul><li><p>如果解决一个问题有多个步骤，每一个步骤有多种方法，找出所有方法即可使用回溯</p></li><li><p>回溯算法是在一棵树上的深度优先遍历；不论是排列还是组合，都要注意结果去重，如三数之和为0的例子：<code>[-1,-1,1,0]</code></p></li><li><p>注意列表要用拷贝：<code>res.append(temp[:])</code></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">recur</span>(<span class="hljs-params">**kwargs</span>):<br>    <span class="hljs-keyword">if</span> condition:<br>        <span class="hljs-comment">#存放结果</span><br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-keyword">for</span> 每一个元素: <br>        <span class="hljs-comment">#处理节点</span><br>        recur(**kwargs)<br>        <span class="hljs-comment">#回溯，撤销处理结果</span><br></code></pre></td></tr></table></figure><h1 id="排列">1、排列</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/permutations-ii/">47. Permutations II</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 需要借助一个哈希表来表示同层元素是否被用过</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">permuteUnique</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        res,temp=[],[]<br>        nums=<span class="hljs-built_in">sorted</span>(nums)<br>        used=[<span class="hljs-literal">False</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums))]<br>        <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">recur</span>():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(temp)==<span class="hljs-built_in">len</span>(nums):<br>                res.append(temp[:])<br>                <span class="hljs-keyword">return</span> <br><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> used[i]:<br><span class="hljs-comment"># 遇到相同的数字，只记录第一个的排列，这是为什么要先排序的原因                    </span><br>                    <span class="hljs-keyword">if</span> i&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i]==nums[i-<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span> used[i-<span class="hljs-number">1</span>]==<span class="hljs-literal">False</span> :<span class="hljs-keyword">continue</span><br>                    <br>                    temp.append(nums[i])<br>                    used[i]=<span class="hljs-literal">True</span><br>                    recur()<br>                    used[i]=<span class="hljs-literal">False</span><br>                    temp.pop()<br>        <br>        recur()<br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure></li><li><p><a href="https://leetcode.cn/problems/3sum/">15. 3Sum</a></p><blockquote><p>对数组先排序，之后设立一个<code>pre</code>变量记录前一个遍历的数字，当正在遍历的数字和之前不同时，继续</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.res=[]<br>        <span class="hljs-variable language_">self</span>.temp=[]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        nums=<span class="hljs-built_in">sorted</span>(nums)<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">recur</span>(<span class="hljs-params">temp,left</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(temp)&gt;<span class="hljs-number">3</span>:<span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.temp)==<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(temp)==<span class="hljs-number">3</span>:<span class="hljs-variable language_">self</span>.res.append(<span class="hljs-variable language_">self</span>.temp[:])<br>            pre=<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(left,<span class="hljs-built_in">len</span>(nums)):<br>                <span class="hljs-keyword">if</span> nums[i]==pre:<span class="hljs-keyword">continue</span><br>                pre=nums[i]<br>                <span class="hljs-variable language_">self</span>.temp.append(nums[i])<br>                recur(<span class="hljs-variable language_">self</span>.temp,i+<span class="hljs-number">1</span>)<br>                <span class="hljs-variable language_">self</span>.temp.pop()<br><br>        recur(<span class="hljs-variable language_">self</span>.temp,<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.res<br></code></pre></td></tr></table></figure></li><li><p><a href="https://leetcode.cn/problems/permutations/">46. Permutations</a></p><blockquote><p>没有重复数字的全排列，在递归函数的遍历时不是从i+1处再次调用递归（组合用的）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.temp=[]<br>        <span class="hljs-variable language_">self</span>.res=[]<br>        <span class="hljs-variable language_">self</span>.tf=[]<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">permute</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        <span class="hljs-variable language_">self</span>.tf=[<span class="hljs-literal">False</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums))]<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">recur</span>():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.temp)==<span class="hljs-built_in">len</span>(nums):<br>                <span class="hljs-variable language_">self</span>.res.append(<span class="hljs-variable language_">self</span>.temp[:])<br>                <span class="hljs-keyword">return</span><br>            <br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(nums)):<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.tf[i]:<br>                    <span class="hljs-variable language_">self</span>.temp.append(nums[i])<br>                    <span class="hljs-variable language_">self</span>.tf[i]=<span class="hljs-literal">True</span><br>                    recur()<br>                    <span class="hljs-variable language_">self</span>.tf[i]=<span class="hljs-literal">False</span><br>                    <span class="hljs-variable language_">self</span>.temp.pop()<br>            <br>        <br>        recur()<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.res<br></code></pre></td></tr></table></figure></li></ol><h1 id="组合">2、组合</h1><ul><li>深度优先遍历剪枝的前提是<mark>数组有序</mark></li></ul><p><a href="https://leetcode.cn/problems/non-decreasing-subsequences/">491. Non-decreasing Subsequences</a></p><blockquote><p>同一父节点下的同层上使用过的元素就不能再使用了</p></blockquote><p><img src="https://pic.leetcode.cn/1674875192-kXIWfU-image.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>回溯</tag>
      
      <tag>搜索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN、GRU与LSTM</title>
    <link href="/2024/09/18/RNN%E3%80%81GRU%E4%B8%8ELSTM/"/>
    <url>/2024/09/18/RNN%E3%80%81GRU%E4%B8%8ELSTM/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="rnn">1、RNN</h1><p><img src="https://zh.d2l.ai/_images/rnn.svg"></p><h1 id="lstm">2、LSTM</h1><p><img src="https://zh.d2l.ai/_images/lstm-3.svg"></p><h1 id="gru">3、GRU</h1><p><img src="https://zh.d2l.ai/_images/gru-3.svg"></p><h1 id="参考">📕 参考</h1><ol type="1"><li><a href="https://blog.csdn.net/sinat_28015305/article/details/109355828">RNN, LSTM, GRU模型的作用, 构建, 优劣势比较,attention机制</a></li><li><a href="https://zh.d2l.ai/chapter_recurrent-modern/lstm.html">动手学深度学习</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>RNN</tag>
      
      <tag>LSTM</tag>
      
      <tag>GRU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二叉树</title>
    <link href="/2024/09/12/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <url>/2024/09/12/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>迭代简单理解就是可以通过循环来解决（for、while）， 而递归则是通过重复调用自身来解决问题；递归的时候隐式地维护了一个栈，而迭代的时候需要显式地将这个栈模拟出来</p><ul><li>DFS使用递归、迭代实现均可</li><li>BFS需要靠队列实现，即迭代</li></ul><h1 id="二叉树的迭代遍历">1、二叉树的迭代遍历</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/binary-tree-preorder-traversal/">144. Binary Tree Preorder Traversal</a></p><p>前序遍历每次先处理中间节点，先将根节点放入栈中，然后将右孩子、左孩子依次加入，这样出栈的时候才是中左右的顺序。</p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%89%8D%E5%BA%8F%E9%81%8D%E5%8E%86%EF%BC%88%E8%BF%AD%E4%BB%A3%E6%B3%95%EF%BC%89.gif"></p></li><li><p><a href="https://leetcode.cn/problems/binary-tree-inorder-traversal/">94. Binary Tree Inorder Traversal</a></p><p>但是，中序遍历、后序遍历的逻辑都不太一样，因为<mark>访问节点和处理节点的顺序不一致</mark></p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%EF%BC%88%E8%BF%AD%E4%BB%A3%E6%B3%95%EF%BC%89.gif"></p></li><li><p><a href="https://leetcode.cn/problems/binary-tree-postorder-traversal/">145. Binary Tree Postorder Traversal</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 后续遍历的迭代</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">postorderTraversal</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>()<br>        <br>        res = <span class="hljs-built_in">list</span>()<br>        stack = <span class="hljs-built_in">list</span>()<br>        prev = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">while</span> root <span class="hljs-keyword">or</span> stack:<br>            <span class="hljs-keyword">while</span> root:<br>                stack.append(root)<br>                root = root.left<br>            root = stack.pop()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root.right <span class="hljs-keyword">or</span> root.right == prev:<br>                res.append(root.val)<br>                prev = root<br>                root = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">else</span>:<br>                stack.append(root)<br>                root = root.right<br>        <br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure></li></ol><h1 id="bfs二叉树的层序遍历">2、BFS：二叉树的层序遍历</h1><ul><li><strong>方法</strong>：利用BFS，以下为模板，<mark>可以运用在许多题中</mark></li></ul><p><a href="https://leetcode.cn/problems/binary-tree-level-order-traversal/">102. Binary Tree Level Order Traversal</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">levelOrder</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root :<span class="hljs-keyword">return</span> []<br>    stack=[root]<br>    res=[]<br>    <span class="hljs-keyword">while</span>(stack):<br>        queue_len=<span class="hljs-built_in">len</span>(stack)<br>        layer=[]<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(queue_len):<br>            temp=stack.pop(<span class="hljs-number">0</span>)<br>            layer.append(temp.val)<br>            <span class="hljs-keyword">if</span> temp.left:stack.append(temp.left)<br>            <span class="hljs-keyword">if</span> temp.right:stack.append(temp.right)<br>        res.append(layer)<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><h1 id="二叉树的深度与高度">3、二叉树的深度与高度</h1><ul><li>二叉树节点的深度：指从根节点到该节点的最长简单路径边的条数。</li><li>二叉树节点的高度（只能后序遍历）：指从该节点到叶子节点的最长简单路径边的条数。</li></ul><p><a href="https://leetcode.cn/problems/balanced-binary-tree/">110. Balanced Binary Tree</a></p><h1 id="dfs岛屿类问题">4、DFS：岛屿类问题</h1><ul><li><a href="https://leetcode.cn/problems/number-of-islands/solutions/211211/dao-yu-lei-wen-ti-de-tong-yong-jie-fa-dfs-bian-li-">判断是否越界以及是否染过色，染色，DFS</a></li></ul><p><a href="https://leetcode.cn/problems/number-of-islands/">200. Number of Islands</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numIslands</span>(<span class="hljs-params">self, grid: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        num=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">dfs</span>(<span class="hljs-params">r,c</span>):<br>            <span class="hljs-keyword">if</span> r&gt;=<span class="hljs-built_in">len</span>(grid) <span class="hljs-keyword">or</span> r&lt;<span class="hljs-number">0</span> <span class="hljs-keyword">or</span> c&gt;=<span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>]) <span class="hljs-keyword">or</span> c&lt;<span class="hljs-number">0</span> <span class="hljs-keyword">or</span> grid[r][c]!=<span class="hljs-string">&#x27;1&#x27;</span>:<span class="hljs-keyword">return</span><br>            grid[r][c]=<span class="hljs-string">&#x27;2&#x27;</span><br>        <br>            dfs(r,c+<span class="hljs-number">1</span>)<br>            dfs(r,c-<span class="hljs-number">1</span>)<br>            dfs(r-<span class="hljs-number">1</span>,c)<br>            dfs(r+<span class="hljs-number">1</span>,c)<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(grid)):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(grid[<span class="hljs-number">0</span>])):<br>                <span class="hljs-keyword">if</span> dfs(i,j)==<span class="hljs-number">1</span>:num+=<span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> num<br></code></pre></td></tr></table></figure><h1 id="dfs前序中序后序遍历">5、DFS：前序、中序、后序遍历</h1><p><a href="https://leetcode.cn/problems/construct-binary-tree-from-inorder-and-postorder-traversal/">106. Construct Binary Tree from Inorder and Postorder Traversal</a></p><p>根据中序、后序递归构建二叉树时，注意到后序遍历结果是按<mark>左子树、右子树、根</mark>排列的，因此递归时先递归右子树，再递归左子树</p><h1 id="二叉搜索树-bst">6、二叉搜索树 BST</h1><ul><li>其左子树中的所有节点的值都小于等于该节点的值，而其右子树中的所有节点的值都大于或等于该节点的值</li><li>中序遍历二叉搜索树得到的关键码序列是一个递增序列</li></ul><p><a href="https://leetcode.cn/problems/validate-binary-search-tree/">98. Validate Binary Search Tree</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.pre=-<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isValidBST</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.isValidBST(root.left):<span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> root.val&lt;=<span class="hljs-variable language_">self</span>.pre:<span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-variable language_">self</span>.pre=root.val<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.isValidBST(root.right)<br></code></pre></td></tr></table></figure><h1 id="二叉树最近公共祖先">7、二叉树最近公共祖先</h1><p><a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/">236. Lowest Common Ancestor of a Binary Tree</a></p><p>可以参考这种解法，非常简洁：<a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/solutions/240096/236-er-cha-shu-de-zui-jin-gong-gong-zu-xian-hou-xu/">二叉树的最近公共祖先（DFS ，清晰图解）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lowestCommonAncestor</span>(<span class="hljs-params">self, root: <span class="hljs-string">&#x27;TreeNode&#x27;</span>, p: <span class="hljs-string">&#x27;TreeNode&#x27;</span>, q: <span class="hljs-string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="hljs-string">&#x27;TreeNode&#x27;</span>:<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root <span class="hljs-keyword">or</span> root==p <span class="hljs-keyword">or</span> root==q:<span class="hljs-keyword">return</span> root<br><br>        left=<span class="hljs-variable language_">self</span>.lowestCommonAncestor(root.left,p,q)<br>        right=<span class="hljs-variable language_">self</span>.lowestCommonAncestor(root.right,p,q)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> left:<span class="hljs-keyword">return</span> right<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> right:<span class="hljs-keyword">return</span> left<br>                <br>        <br>        <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>二叉树</tag>
      
      <tag>递归</tag>
      
      <tag>迭代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>栈、队列</title>
    <link href="/2024/09/11/%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97/"/>
    <url>/2024/09/11/%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="用队列实现栈">1、用队列实现栈</h1><p><a href="https://leetcode.cn/problems/implement-stack-using-queues/">225. Implement Stack using Queues</a></p><figure><img src="https://assets.leetcode-cn.com/solution-static/225/225_fig1.gif" alt="当push的时候，将栈中元素接到备用栈，并互换身份"><figcaption aria-hidden="true">当push的时候，将栈中元素接到备用栈，并互换身份</figcaption></figure><p>要注意，第二个数组只是辅助数组，所以有<code>array1,array2=array2,[]</code></p><h1 id="单调栈">2、单调栈</h1><p>本质是空间换时间，记录已经遍历过的元素</p><h1 id="其他">3、其他</h1><p><a href="https://leetcode.cn/problems/sliding-window-maximum/">239. Sliding Window Maximum</a></p><p>​ 数组、区间内的最大（小）值，可以考虑队列/栈</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>栈</tag>
      
      <tag>队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>双均值策略</title>
    <link href="/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/"/>
    <url>/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<p>在聚宽上编写双均值策略，对三支股票从2024年初到现在进行交易，最终结果亏了不到5%，入门还算可以</p><p><img src="/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/结果.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入函数库</span><br><span class="hljs-keyword">from</span> jqdata <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 初始化函数，设定基准等等</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize</span>(<span class="hljs-params">context</span>):<br>    <span class="hljs-comment"># 设定沪深300作为基准</span><br>    set_benchmark(<span class="hljs-string">&#x27;000300.XSHG&#x27;</span>)<br>    <span class="hljs-comment"># 开启动态复权模式(真实价格)</span><br>    set_option(<span class="hljs-string">&#x27;use_real_price&#x27;</span>, <span class="hljs-literal">True</span>)<br>    g.security=[<span class="hljs-string">&#x27;002686.XSHE&#x27;</span>,<span class="hljs-string">&#x27;600771.XSHG&#x27;</span>,<span class="hljs-string">&#x27;601216.XSHG&#x27;</span>]<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_data</span>(<span class="hljs-params">context, data</span>):<br>    <span class="hljs-keyword">for</span> security <span class="hljs-keyword">in</span> g.security:<br>        close_data = attribute_history(security, <span class="hljs-number">60</span>, <span class="hljs-string">&#x27;1d&#x27;</span>, [<span class="hljs-string">&#x27;close&#x27;</span>])<br> <br>        MA5 = close_data[<span class="hljs-string">&#x27;close&#x27;</span>][-<span class="hljs-number">5</span>:].mean()<br>        MA60 = close_data[<span class="hljs-string">&#x27;close&#x27;</span>].mean()<br>        <br>        current_price =close_data[<span class="hljs-string">&#x27;close&#x27;</span>][-<span class="hljs-number">1</span>]<br>        <br>        <span class="hljs-comment"># 取得当前的现金</span><br>        cash = context.portfolio.cash<br>        position=context.portfolio.positions<br>        <br>        <span class="hljs-comment"># 如果5日均价高出60天天平均价5%, 则现金0.2买入</span><br>        <span class="hljs-keyword">if</span> MA5 &gt; MA60*<span class="hljs-number">1.05</span>:<br>            <span class="hljs-keyword">if</span> security <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> position:order_value(security, cash*<span class="hljs-number">0.2</span>)<br>            <span class="hljs-keyword">elif</span> current_price &lt; position[security].avg_cost: order(security, cash*<span class="hljs-number">0.1</span>)<br>        <span class="hljs-comment"># 如果5日均价低于60天均价的90%, 则空仓卖出</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> security <span class="hljs-keyword">in</span> position <span class="hljs-keyword">and</span> MA5&lt;MA60*<span class="hljs-number">0.9</span>:order_target(security, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>投资</category>
      
    </categories>
    
    
    <tags>
      
      <tag>量化</tag>
      
      <tag>金融</tag>
      
      <tag>经济</tag>
      
      <tag>聚宽</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>字符串</title>
    <link href="/2024/09/10/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    <url>/2024/09/10/%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="kmp算法">1、KMP算法</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>字符串</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>参数高效微调</title>
    <link href="/2024/09/07/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/09/07/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="低秩分解-lora">低秩分解 LoRA</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LoRALinear</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features, out_features, merge, rank=<span class="hljs-number">16</span>, lora_alpha=<span class="hljs-number">16</span>, dropout=<span class="hljs-number">0.5</span></span>):<br>        <span class="hljs-built_in">super</span>(LoRALinear, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.in_features = in_features<br>        <span class="hljs-variable language_">self</span>.out_features = out_features<br>        <span class="hljs-variable language_">self</span>.merge = merge<br>        <span class="hljs-variable language_">self</span>.rank = rank<br>        <span class="hljs-variable language_">self</span>.dropout_rate = dropout<br>        <span class="hljs-variable language_">self</span>.lora_alpha = lora_alpha<br>        <br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(in_features, out_features)<br>        <span class="hljs-keyword">if</span> rank &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.lora_b = nn.Parameter(torch.zeros(out_features, rank))<br>            <span class="hljs-variable language_">self</span>.lora_a = nn.Parameter(torch.zeros(rank, in_features))<br>            <span class="hljs-variable language_">self</span>.scale = <span class="hljs-variable language_">self</span>.lora_alpha / <span class="hljs-variable language_">self</span>.rank<br>            <span class="hljs-variable language_">self</span>.linear.weight.requires_grad = <span class="hljs-literal">False</span><br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.dropout_rate &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(<span class="hljs-variable language_">self</span>.dropout_rate)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.dropout = nn.Identity()<br>        <br>        <span class="hljs-variable language_">self</span>.initial_weights()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initial_weights</span>(<span class="hljs-params">self</span>):<br>        nn.init.kaiming_uniform_(<span class="hljs-variable language_">self</span>.lora_a, a=math.sqrt(<span class="hljs-number">5</span>))<br>        nn.init.zeros_(<span class="hljs-variable language_">self</span>.lora_b)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.rank &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.merge:<br>            output = F.linear(x, <span class="hljs-variable language_">self</span>.linear.weight + <span class="hljs-variable language_">self</span>.lora_b @ <span class="hljs-variable language_">self</span>.lora_a * <span class="hljs-variable language_">self</span>.scale, <span class="hljs-variable language_">self</span>.linear.bias)<br>            output = <span class="hljs-variable language_">self</span>.dropout(output)<br>            <span class="hljs-keyword">return</span> output<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.dropout(<span class="hljs-variable language_">self</span>.linear(x))<br><br></code></pre></td></tr></table></figure><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://www.bilibili.com/video/BV1dr421w7J5/?spm_id_from=333.337.search-card.all.click&amp;vd_source=19303cc41e28593826f02ec94c4fc790">【研1基本功 （真的很简单）LoRA 低秩微调】大模型微调基本方法1 —— bonus "Focal loss"</a></li><li><a href="https://dwexzknzsh8.feishu.cn/docx/VkYud3H0zoDTrrxNX5lce0S4nDh">尽量“手撕”代码系列</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>微调</tag>
      
      <tag>PEFT</tag>
      
      <tag>LoRA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·II</title>
    <link href="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/"/>
    <url>/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;  margin-top: 20px;}</style><h1 id="时间线">〇、时间线</h1><p><strong>9月4日~9月6日</strong></p><ol type="1"><li>冻结Transformer参数，只利用MOMENT的分类器在Benchmark上进行训练与测试，在3个简单数据集上效果很好（见2.2）</li><li>利用t-SNE进行降维，没有得到有启发的信息；随后尝试调参（Batchsize、Lr）、解冻Transformer参数来观察<code>GrabMyo</code>上的效果</li><li>阅读综述及相关论文</li><li>改小训练集，重新训练<code>Grabmyo</code>，效果都不是太好</li></ol><p><strong>9月9日~9月13日</strong></p><ol type="1"><li>阅读EASE</li></ol><h1 id="一相关工作">一、相关工作</h1><ol type="1"><li><a href="https://arxiv.org/abs/2402.02713">Position: What Can Large Language Models Tell Us about Time Series Analysis</a></li><li><a href="https://arxiv.org/abs/2403.14735">Foundation Models for Time Series Analysis: A Tutorial and Survey</a></li><li><a href="https://arxiv.org/abs/2401.16386">综述：Continual Learning with Pre-Trained Models: A Survey</a></li><li><a href="https://arxiv.org/abs/2303.07338">SimpleCIL &amp; APER：Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need</a></li><li><a href="https://arxiv.org/abs/2403.12030">APER的升级：Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning</a></li><li><a href="https://arxiv.org/abs/2307.02251">高维投影增加可分性：RanPAC: Random Projections and Pre-trained Models for Continual Learning</a></li><li><a href="https://arxiv.org/abs/2303.05118">SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model</a></li></ol><h2 id="基于预训练模型的增量学习综述">1.3 基于预训练模型的增量学习：综述</h2><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/结果.png"></p><h2 id="simplecil-aper">1.4 SimpleCIL &amp; APER</h2><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER.png" alt="APER框架"><figcaption aria-hidden="true">APER框架</figcaption></figure><h3 id="表示">1.4.1 表示</h3><ul><li>训练任务：<span class="math inline">\(\{\mathcal D^1,\mathcal D^2,\cdots,\mathcal D^b\}\)</span></li><li>Prototype：<span class="math inline">\({\rm p_i}=\frac{1}{K}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)\phi(x_j)，K=\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)，\mathbb I(\cdot)\)</span>为示性函数</li><li>分类器：<span class="math inline">\(f(x)=W^T\phi(x)，\phi(\cdot)\)</span>为embedding函数，<span class="math inline">\(W\in\mathbb R^{d\times |y_b|}\)</span>是分类头</li></ul><h3 id="算法">1.4.2 算法</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER算法.png"></p><ol type="1"><li><p>PTM添加parameter-efficient module后在<span class="math inline">\(\mathcal D^1\)</span>上微调得到AdaPTM</p><ul><li><p>按序微调模型会导致灾难性遗忘</p><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/Adapt.png" alt="微调前T个任务的结果"><figcaption aria-hidden="true">微调前T个任务的结果</figcaption></figure></li><li><p>因为使用prototype-based分类器，因此多次微调会导致前后prototype的特征不兼容</p><blockquote><p>Since we utilize a prototype-based classifier, tuning the model with multiple stages will result in incompatible features between former and new prototypes.</p></blockquote></li></ul></li><li><p>将PTM和AdaPTM的embedding层冻结（只训练分了里头和AdaModule里的参数）</p></li><li><p>对于每一个任务，计算其数据的Prototype并替代分类头的权重：<span class="math inline">\({\rm p_i}=\frac{1}{K}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)[\phi^{*}(x_j),\phi(x_j)]\)</span></p></li><li><p>通过计算余弦相似度来获得分类结果</p></li></ol><h3 id="adapt方法aper框架图的中右部分">1.4.3 Adapt方法（APER框架图的中、右部分）</h3><ul><li>Visual Prompt Tuning（VPT）：适合于ViT<ul><li>VPT-Deep：在每一个注意力层加Prompt</li><li>VPT-Shallow：只在第一层加Prompt</li></ul></li><li>Scale &amp; Shift（SSF）：<span class="math inline">\(x_{output}=\gamma\otimes x_{input}+\beta\)</span>，从而将分布适应于新任务</li><li>Adapter：<span class="math inline">\({\rm MLP}(x_l)+{\rm RELU}(x_{l}W_{down})W_{up}\)</span></li><li>Batch Normalization Tuning：适合于CNN-based模型</li></ul><h3 id="实验结果">1.4.4 实验结果</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER消融.png"></p><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER结果.png"></p><h2 id="aper的升级ease">1.5 APER的升级：EASE</h2><h3 id="简介">1.5.1 简介</h3><ul><li>前人工作的缺陷：对于L2P而言，其中一个缺点是Prompt会随着更新而遗忘先前的知识</li><li>本篇工作的改进：“拓展子空间“将先前子空间（Subspace，可理解为某任务所映射到的空间）的Prototype映射到本任务的子空间，从而减少遗忘<ul><li>个人理解：SimpleCIL利用PTM作为编码器，从而获得Prototype向量，然而该编码器未必有很好的效果；本篇工作通过在Transformer中加入Adapter层实现参数高效微调，从而优化编码效果，得到更好的Prototype向量；最终分类的原理通SimpleCIL，都是对Prototype计算余弦相似度。</li></ul></li></ul><h3 id="模型">1.5.2 模型</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/EASE.png"></p><ul><li><p>定义任务：<span class="math inline">\(\{\mathcal D^1,\mathcal D^2,\cdots,\mathcal D^B\}\)</span>，共有B个任务，每个任务可能包含多个类别，不同任务中的类别不同</p></li><li><p>Adapter层：加在PTM内Transformer的MLP旁，记为<span class="math inline">\(\mathcal A\)</span>，每一个任务都初始化一个Adapter层，即一个<mark>子空间</mark>（Subspace）。具体形式：<span class="math inline">\(x_o=\sigma(x_iW_{down})W_{up}+{\rm MLP}(X_i)，\sigma(\cdot)\)</span>为非线性激活函数，将使用Adapter层的embedding函数记为<span class="math inline">\(\phi(x;\mathcal A)\)</span></p></li><li><p>定义子空间<span class="math inline">\(b\)</span>（也可以说任务b对应的子空间）中第<span class="math inline">\(i\)</span>类的Prototype：<span class="math inline">\(p_{i,b}=\frac{1}{N}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)\phi(x_j;\mathcal A_b)\)</span>，N为第i类的样本数量</p></li><li><p>定义某样本的embedding：<span class="math inline">\(\Phi(x)=[\phi (x;\mathcal A_1),\cdots,\phi (x;\mathcal A_b)]\in \mathbb R^{bd}\)</span>，即样本<span class="math inline">\(x\)</span>在所有子空间的embedding拼接</p></li><li><p>定义类别<span class="math inline">\(i\)</span>的分类器：<span class="math inline">\(\mathcal P_i=[p_{i,1},p_{i,2},\cdots,p_{i,b}]\in \mathbb R^{bd}\)</span>，即类别<span class="math inline">\(i\)</span>在所有子空间的Prototype向量的拼接</p></li><li><p>定义任务b的Prototype：<span class="math inline">\({\rm P}_{1,1}={\rm Concat}[p_{1,1};\cdots p_{|y_1|,1}] \in \mathbb R^{|y_1|\times d}\)</span>，下标中的前一个‘1’代表第几个任务，后一个‘1’代表第几个子空间；即第一个任务在第一个子空间的Prototype可以用第一个任务中所有样本在第一个子空间的Prototype拼接来表示</p><ul><li>同理我们可以得到<span class="math inline">\({\rm P}_{2,1}={\rm Concat}[p_{2,1};\cdots p_{|y_2|,1}] \in \mathbb R^{|y_2|\times d}\)</span>和<span class="math inline">\({\rm P}_{2,2}={\rm Concat}[p_{2,2};\cdots p_{|y_2|,2}] \in \mathbb R^{|y_2|\times d}\)</span>。但是我们不能<mark>直接</mark>得到第一个任务在第二个子空间的Prototype，作者认为是Prototype和embedding间维度的不一致。（这个我没想明白，我觉得一个合理的原因是没有保留第一任务的样本，从而无法计算）</li><li>因此，作者提出了Semantic Guided Prototype Complement，即基于<span class="math inline">\({\rm P}_{o,o},{\rm P}_{n,o},{\rm P}_{n,n}\)</span>估计<span class="math inline">\({\rm P}_{o,n}\)</span>，其中前两个向量被称作共现空间（co-occurence space）,前一个角标的<span class="math inline">\(o,n\)</span>分别代表<mark>旧类别、新类别</mark>，后一个代表旧子空间、新子空间</li></ul></li><li><p>定义相似度（可以类比为两向量的点积）：   $$  {\rm Sim}_{i,j}=\frac{ {\rm P}_{o,o}[i]}{ {\Vert {\rm P}_{o,o} \Vert}_2}\frac{ {\rm P}_{n,o}[j]}{  {\Vert {\rm P}_{n,o} \Vert}_2}  $$  </p><ul><li><p>进一步归一化，可以理解为<span class="math inline">\(i\)</span>对<span class="math inline">\(j\)</span>的相似度（<mark>原文的分母下标用的是<span class="math inline">\(j\)</span>，这里改成了<span class="math inline">\(k\)</span></mark>）：   $$  {\rm Sim}_{i,j}=\frac{e^{{\rm Sim}_{i,j}}}{\sum \limits_k e^{{\rm Sim}_{i,k}} }  $$  ​</p></li><li><p>最终有（可以理解为<span class="math inline">\({\rm P}_{o,o}[i]\)</span>和<span class="math inline">\({\rm P}_{n,o}[j]\)</span>之间的关系在<span class="math inline">\({\rm P}_{o,n}[i]\)</span>和<span class="math inline">\({\rm P}_{n,n}[j]\)</span>间也适用：</p>  $$  {\rm P}_{o,n}[i]=\sum \limits_j {\rm  Sim}_{i,j}\times {\rm P}_{n,n}[j]  $$  ​</li><li><p>补全矩阵有：</p>  $$  \begin{bmatrix}      \rm P_{1,1}& \rm P_{1,2}& \cdots  & \rm P_{1,B} \\      \rm P_{2,1}& \rm P_{2,2}& \cdots  & \rm P_{2,B} \\      \vdots & \vdots & \ddots & \vdots \\      \rm P_{B,1}& \rm P_{B,2}& \cdots  & \rm P_{B,B}    \end{bmatrix}  $$  ​</li></ul></li><li><p>计算任务b中输入<span class="math inline">\(x\)</span>在第<span class="math inline">\(i\)</span>个Adapter层的embedding与任务b在子空间<span class="math inline">\(i\)</span>的Prototype的乘积：</p>  $$  \left[\mathbf{P}_{b, 1}, \mathbf{P}_{b, 2}, \cdots, \mathbf{P}_{b, B}\right]^{\top} \Phi(\mathbf{x})=\sum_{i} \mathbf{P}_{b, i}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{i}\right)  $$  <ul><li><p>但因为任务b最主要学习的特征存放在第<span class="math inline">\(b\)</span>个Adapter层，因此进行加权：</p>  $$  \mathbf{P}_{b, b}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{b}\right)+\alpha \sum_{i \neq b} \mathbf{P}_{b, i}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{i}\right)  $$​      </li></ul></li></ul><h2 id="高维投影增加可分性ranpac">1.6 高维投影增加可分性：RanPAC</h2><h3 id="简介-1">1.6.1 简介</h3><ul><li><p>前人工作的缺陷：对于NCM（即利用CP）而言，类与类之间存在一定的相关性，的相关系数会很高</p><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/缺陷.png" alt="左图为CP和feature vector的相似度，右图为皮尔逊相关系数"><figcaption aria-hidden="true">左图为CP和feature vector的相似度，右图为皮尔逊相关系数</figcaption></figure></li><li><p>本篇工作的改进：基于随机投影矩阵和Gram Matrix增加类与类之间的可分性</p><ul><li>高斯分布假设数据具有各向同性，而直接从预训练模型提取的特征可能不符合高斯分布</li><li>随着随机投影的维度增加，输出更接近高斯分布，并且向量之间的内积会更大</li><li>借助Gram实现类似LDA的算法，增加类与类之间的可分性</li></ul></li><li><p>本篇工作的问题：与传统方法比，随机投影矩阵参数开销很大；公式的运用需要强大的预训练模型编码器</p></li></ul><h3 id="算法-1">1.6.2 算法</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/RanPAC.png"></p><ol type="1"><li><p>基于首个任务对PTM进行微调（参考了APER的做法）：首个任务的数据比预训练数据更好的拟合了下游任务。然而并没有使用PTM与AdaPTM的表示进行拼接，因为这对于随机投影而言效果不好</p><ul><li><strong>AdaptFormer</strong>：在预训练模型中添加少量的可训练参数</li><li><strong>SSF (Scaling and Shifting Features)</strong>：通过在特征提取过程中添加缩放和平移操作来调整特征表示</li><li><strong>VPT (Visual Prompt Tuning)</strong>：通过在模型输入中引入Prompts，并更新Prompt</li></ul></li><li><p>预测结果Basic（可以类比计算相似度），<span class="math inline">\(s_y\)</span>可以理解为预测的类别（为什么是<span class="math inline">\(\mathbf{G}^{-1}\)</span>，参考Appendix B） <span class="math display">\[y_{\text {test }}=\underset{y^{\prime} \in\{1, \ldots, K\}}{\rm argmax }s_{y^{\prime}}, \quad s_{y}:=\mathbf{f}_{\text {test }}^{\top} \mathbf{G}^{-1} \mathbf{c}_{y}\]</span></p><ul><li><p>K为T个任务后的最大类别数量</p></li><li><p>G为Gram矩阵，由空间内向量相互之间的内积组成，可以看作是没有减去均值的协方差矩阵： <span class="math display">\[\begin{pmatrix}    (a_{1},a_{1}) &amp; \cdots &amp; (a_{1},a_{n}) \\    \vdots &amp; \ddots &amp; \vdots \\    (a_{n},a_{1}) &amp; \cdots &amp; (a_{n},a_{n})  \end{pmatrix} \]</span></p></li><li><p><span class="math inline">\(c_y\)</span>为CPs with averaging dropped</p></li></ul></li><li><p>改进</p><ul><li><p>随机投影矩阵，符合标准正态分布：<span class="math inline">\(W \in \mathbb R^{L×M}\)</span></p></li><li><p>非线性激活函数：<span class="math inline">\(\phi(\cdot)\)</span></p></li><li><p>预训练模型对于任务<span class="math inline">\(\mathcal D_t\)</span>中的第<span class="math inline">\(n\)</span>个样本提取特征：<span class="math inline">\(f_{t,n} \in \mathbb R^L\)</span></p></li><li><p>定义特征经高维投影并非线性激活后的输出向量（可以理解为特征的<mark>高维投影</mark>）：<span class="math inline">\({\rm h}_{t,n}= \phi (f_{t,n}^{\rm T}W)\in \mathbb R^{M}\)</span></p></li><li><p>定义矩阵（由高维投影组成）：<span class="math inline">\({\rm H}\in \mathbb R^{M\times N}，N\)</span>代表样本数量，由<span class="math inline">\({\rm h}_{t,n}\)</span>组成</p></li><li><p>Gram矩阵：<span class="math inline">\(\rm G=HH^T\)</span>，因此这是个<mark>满秩矩阵</mark>，也可以写为<span class="math inline">\(\mathbf{G}=\sum \limits_{t=1}^{T} \sum \limits_{n=1}^{N_{t}} \mathbf{h}_{t, n} \otimes \mathbf{h}_{t, n}\)</span></p></li><li><p><span class="math inline">\(c_y\)</span>矩阵，即由所有Prototype组成：<span class="math inline">\(\mathbf{C}=\sum \limits_{t=1}^{T} \sum \limits_{n=1}^{N_{t}} \mathbf{h}_{t, n} \otimes \mathbf{y}_{t, n}\)</span></p></li><li><p>预测结果Revised，其中<span class="math inline">\({\rm \mathbf{I}}\)</span>为单位矩阵，该项<span class="math inline">\(\lambda{\rm \mathbf{I}}\)</span>借助岭回归中正则化的思想，用来防止过拟合： <span class="math display">\[s_y = \phi({\rm f_{test}}^{\rm T}W)(\mathbf{G} + λ\mathbf{I})^{−1}c_y\]</span></p></li></ul></li></ol><ul><li>记公式的后半部分为<span class="math inline">\({\rm W_o}=(\mathbf{G} + λ\mathbf{I})^{−1}{\rm \mathbf{C}}\)</span>，则<span class="math inline">\({\rm y_{pred} = h_{test}W_o}\)</span></li></ul><h3 id="结果">1.6.3 结果</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/RanPAC结果.png"></p><h1 id="二moment">二、MOMENT</h1><h2 id="测试代码">2.1 测试代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/&quot;</span>)<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/TSCIL/&quot;</span>)<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/TSCIL/utils/&quot;</span>)<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <br><span class="hljs-keyword">from</span> momentfm <span class="hljs-keyword">import</span> MOMENTPipeline<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> lr_scheduler<br><span class="hljs-keyword">from</span> EarlyStop <span class="hljs-keyword">import</span> EarlyStopping<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader,Dataset<br><span class="hljs-keyword">from</span> stream <span class="hljs-keyword">import</span> IncrementalTaskStream,get_cls_order<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>DATA=<span class="hljs-string">&quot;wisdm&quot;</span><br><span class="hljs-comment"># [&#x27;har&#x27;, &#x27;uwave&#x27;, &#x27;dailysports&#x27;, &#x27;grabmyo&#x27;, &#x27;wisdm&#x27;,&#x27;ninapro&#x27;, &#x27;sines&#x27;]</span><br><span class="hljs-comment"># ============================初始化MOMENT============================</span><br>model = MOMENTPipeline.from_pretrained(<br>    <span class="hljs-string">&quot;AutonLab/MOMENT-1-large&quot;</span>, <br>    model_kwargs=&#123;<br>        <span class="hljs-string">&#x27;task_name&#x27;</span>: <span class="hljs-string">&#x27;classification&#x27;</span>,<br>        <span class="hljs-string">&#x27;n_channels&#x27;</span>: <span class="hljs-number">3</span>,<br>        <span class="hljs-string">&#x27;num_class&#x27;</span>: <span class="hljs-number">18</span>,<br>        <span class="hljs-string">&#x27;freeze_encoder&#x27;</span>: <span class="hljs-literal">True</span>, <span class="hljs-comment"># Freeze the patch embedding layer</span><br>        <span class="hljs-string">&#x27;freeze_embedder&#x27;</span>: <span class="hljs-literal">True</span>, <span class="hljs-comment"># Freeze the transformer encoder</span><br>        <span class="hljs-string">&#x27;freeze_head&#x27;</span>: <span class="hljs-literal">False</span>, <span class="hljs-comment"># The linear forecasting head must be trained</span><br>    &#125;, <br>   )<br>model.init()<br>model.to(device)<br><br><span class="hljs-comment"># ============================获取数据集============================</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data, targets</span>):<br>        <span class="hljs-variable language_">self</span>.data = data<br>        <span class="hljs-variable language_">self</span>.targets = targets<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data[idx], <span class="hljs-variable language_">self</span>.targets[idx]<br><br>cls_order = get_cls_order(DATA, <span class="hljs-literal">False</span>)<br>task_stream = IncrementalTaskStream(data=DATA, scenario=<span class="hljs-string">&#x27;class&#x27;</span>, cls_order=cls_order,split=<span class="hljs-string">&#x27;all&#x27;</span>)<br>(x_train, y_train), (x_val, y_val), (x_test, y_test) = task_stream.setup_offline()<br>x_train,x_val,x_test=np.transpose(x_train,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)),np.transpose(x_val,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)),np.transpose(x_test,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><br>train_dataloader = DataLoader(MyDataset(x_train,y_train), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br>val_dataloader = DataLoader(MyDataset(x_val,y_val), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(MyDataset(x_test,y_test), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># ============================超参数设置============================</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br>scheduler=lr_scheduler.StepLR(optimizer, step_size=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.95</span>)<br><br><br>EPOCH = <span class="hljs-number">100</span><br>patience=<span class="hljs-number">20</span><br>Acc,Loss,ValidLoss=[],[],[]<br><br><span class="hljs-comment"># ============================画图函数============================</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Draw</span>(<span class="hljs-params">X,Y,Z</span>):<br>    plt.plot(X,Y,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>,label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>    plt.plot(X,Z,label=<span class="hljs-string">&quot;acc&quot;</span>)<br>    plt.legend()<br>    plt.savefig(<span class="hljs-string">f&#x27;./<span class="hljs-subst">&#123;DATA&#125;</span>_res.png&#x27;</span>) <br><br><br><br><span class="hljs-comment"># ===============================定义测试函数=========================</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">epoch,mode</span>):<br>    <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;test&#x27;</span>:dataloader=test_dataloader<br>    <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&quot;val&quot;</span>:dataloader=val_dataloader<br><br>    model.<span class="hljs-built_in">eval</span>()<br>    correct,total,Validloss=<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data,label <span class="hljs-keyword">in</span> dataloader:<br>            output = model(data.to(torch.<span class="hljs-built_in">float</span>).to(device))<br>            Validloss+= criterion(output.logits, label.to(device))<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(output.logits, dim=<span class="hljs-number">1</span>) <br>            total += label.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 张量之间的比较运算</span><br>            correct += (predicted == label.to(device)).<span class="hljs-built_in">sum</span>().item()<br>        acc = correct / total<br>        ValidLoss.append(Validloss/<span class="hljs-built_in">len</span>(dataloader))<br>        <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;val&#x27;</span>:<br>            Acc.append(acc)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d / %d]: 验证集准确率: %.1f %% &#x27;</span> % (epoch+<span class="hljs-number">1</span>, EPOCH, <span class="hljs-number">100</span> * acc)) <br>        <span class="hljs-keyword">else</span>:<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试集准确率: %.1f %% &#x27;</span> % (<span class="hljs-number">100</span> * acc))<br>    <br><br><br><span class="hljs-comment"># ===============================训练=========================</span><br>early_stopping = EarlyStopping(patience=patience, verbose=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH):<br>    model.train()<br>    EpochLoss=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data,label <span class="hljs-keyword">in</span> train_dataloader:<br><br>        <span class="hljs-comment"># forward [batch_size, n_channels, forecast_horizon]</span><br>        output = model(data.to(torch.<span class="hljs-built_in">float</span>).to(device))<br><br>        <span class="hljs-comment"># backward</span><br>        loss = criterion(output.logits, label.to(device))<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>        <br>        EpochLoss+=loss.item()<br>    Loss.append(EpochLoss/<span class="hljs-built_in">len</span>(train_dataloader))<br>    scheduler.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;学习率:<span class="hljs-subst">&#123;scheduler.get_last_lr()[<span class="hljs-number">0</span>]:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>    test(_,<span class="hljs-string">&#x27;val&#x27;</span>)<br><br>    early_stopping(ValidLoss[-<span class="hljs-number">1</span>], model)<br>    <span class="hljs-keyword">if</span> early_stopping.early_stop:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Early stopping&quot;</span>)<br>        <span class="hljs-keyword">break</span><br><br>test(<span class="hljs-literal">None</span>,<span class="hljs-string">&#x27;test&#x27;</span>)<br>Draw([_+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Loss))],Loss,Acc)<br>torch.save(model,<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;DATA&#125;</span>_ckpt.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="测试结果">2.2 测试结果</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/uwave.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/har.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/sports.png"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/grabmyo.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/wisdm.png"></div></div></div><h2 id="针对grabmyo的调参">2.3 针对<code>GrabMyo</code>的调参</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">数据处理/训练部分</th><th style="text-align: center;">Batchsize</th><th style="text-align: center;">Lr（gamma）</th><th style="text-align: center;">Acc（%）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">concat/cls</td><td style="text-align: center;">512</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">11.6</td></tr><tr class="even"><td style="text-align: center;">concat/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-3（0.95）</td><td style="text-align: center;">12.1</td></tr><tr class="odd"><td style="text-align: center;">mean/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">/</td></tr><tr class="even"><td style="text-align: center;">mean/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">/</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">6.2</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-3（0.95）</td><td style="text-align: center;">6.3</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-4（0.95）</td><td style="text-align: center;">10.0</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-4（0.95）</td><td style="text-align: center;">10.8</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-4（0.95）</td><td style="text-align: center;">11.2</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-5（0.95）</td><td style="text-align: center;">11.2</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈希表</title>
    <link href="/2024/09/04/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <url>/2024/09/04/%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>当需要查询一个元素是否出现过、一个元素是否在集合里、元素出现次数的时候，使用哈希表</p><ol type="1"><li><a href="https://leetcode.cn/problems/4sum-ii/">454. 4Sum II</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>哈希表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型架构、KV Cache、Scaling Law与涌现</title>
    <link href="/2024/09/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"/>
    <url>/2024/09/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="auto-encoder-vs-auto-regressive">1、Auto-encoder vs Auto-regressive</h1><p>从预训练模型的角度上来看，两者的模型结构可能是一样的，不同的是预训练的方式</p><ul><li>自编码模型：预训练时可以看到上下文信息，指BERT等；广义来讲是自编码器（AE），包含VAE</li><li>自回归模型：预训练时只能看到上文或者下文，指GPT、ELMO等；广义来讲就是利用过去观测值预测未来值的模型，也包含RNN、LSTM</li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://blog.csdn.net/weixin_43301333/article/details/128141716?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170848482516800222831072%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=170848482516800222831072&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-128141716-null-null.nonecase&amp;utm_term=encoder&amp;spm=1018.2226.3001.4450">Transformer Encoder-Decoer 结构回顾</a></li><li><a href="https://www.cnblogs.com/AudreyXu/p/17197943.html">自回归和自编码有什么区别？</a></li></ol><h1 id="大模型架构">2、大模型架构</h1><p><img src="https://i-blog.csdnimg.cn/blog_migrate/8aba45c21032e2d07fd783070ad8483a.png"></p><ul><li>Encoder-Decoder（T5）</li><li>Decoder Only（GPT）：在各种下游任务上zero-shot泛化性能最好，few-shot（上下文学习）泛化能力更强<ul><li>预训练任务：Next Token Prediction（任务更难，在数据足够多时，模型学习通用表征的上限更高）</li></ul></li><li>Encoder Only（BERT）：预训练任务只包含分类和预测，不擅长生成任务<ul><li>预训练任务：MLM（预测mask），NSP（句子二分类）</li></ul></li><li>Prefix LM（GLM）</li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://www.zhihu.com/question/588325646/answer/2940298964">为什么现在的LLM都是Decoder only的架构？ - 苏剑林的回答 - 知乎</a></li><li><a href="https://www.zhihu.com/question/588325646/answer/3357252612">为什么现在的LLM都是Decoder only的架构？ - Sam多吃青菜的回答 - 知乎</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>编码器</tag>
      
      <tag>解码器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch加速训练MNIST</title>
    <link href="/2024/08/29/Pytorch%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/08/29/Pytorch%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;  margin-top: 20px;}</style><h1 id="实验过程">0、实验过程</h1><p>为了对比Pytorch中不同方法对于训练速度的提升，采用最基础的<a href="https://blog.csdn.net/qq_45588019/article/details/120935828">MNIST数字识别</a>，基础配置如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">64</span><br>learning_rate = <span class="hljs-number">0.01</span><br>momentum = <span class="hljs-number">0.5</span><br>EPOCH = <span class="hljs-number">5</span><br><br>criterion = torch.nn.CrossEntropyLoss()  <br>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)  <br></code></pre></td></tr></table></figure><p>源码是CPU上跑的，改到GPU上作为一个Baseline：</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">方法</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">CPU</td><td style="text-align: center;">97.9</td><td style="text-align: center;">25.16</td></tr><tr class="even"><td style="text-align: center;">GPU（RTX3080）</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr></tbody></table></div><h1 id="dataloader">1、Dataloader</h1><p>看到<a href="https://blog.csdn.net/qq_28057379/article/details/115427052">网上</a>有说将<code>num_workers</code>设置成CPU数量一样，但也有的说并非越大越好，可能需要进行实验选择最佳的。在这里测试集准确率参考意义并不大，因为差距很小，并且dataloader不会对准确率造成显著的影响，可能与参数的初始化情况有关</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr><tr class="even"><td style="text-align: center;"><code>num_workers=4</code></td><td style="text-align: center;">98.3</td><td style="text-align: center;">7.54</td></tr><tr class="odd"><td style="text-align: center;"><code>pin_memory=True</code></td><td style="text-align: center;">98.7</td><td style="text-align: center;">21.32</td></tr><tr class="even"><td style="text-align: center;"><code>num_workers=4，pin_memory=True</code></td><td style="text-align: center;">98.4</td><td style="text-align: center;"><strong>6.14</strong></td></tr></tbody></table></div><h1 id="自动混合精度-amp">2、自动混合精度 AMP</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">scaler = GradScaler(enabled=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> batch_idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, <span class="hljs-number">0</span>):<br>        inputs, target = data<br>        optimizer.zero_grad()<br><br>        <span class="hljs-keyword">with</span> autocast(enabled=<span class="hljs-literal">True</span>, dtype=torch.float16):<br>            outputs = model(inputs.to(device))<br>            loss = criterion(outputs, target.to(device))<br><br>            scaler.scale(loss).backward()<br>            scaler.step(optimizer)<br>            scaler.update()   <br></code></pre></td></tr></table></figure><p>在这个实验中AMP并未提速，反而还慢了，猜想是因为数据集规模太小。不过在之前的比赛中，在对大模型推理时使用混合精度确实有减少推理时间</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;"><strong>20.40</strong></td></tr><tr class="even"><td style="text-align: center;">AMP</td><td style="text-align: center;">98.5</td><td style="text-align: center;">21.83</td></tr></tbody></table></div><h1 id="优化器">3、优化器</h1><p>除了Baseline中设置了动量参数外，其余皆采用默认参数</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr><tr class="even"><td style="text-align: center;">Adam</td><td style="text-align: center;">95.2</td><td style="text-align: center;">20.43</td></tr><tr class="odd"><td style="text-align: center;">AdamW</td><td style="text-align: center;">97.5</td><td style="text-align: center;">20.62</td></tr><tr class="even"><td style="text-align: center;">RMSprop</td><td style="text-align: center;">97.2</td><td style="text-align: center;"><strong>20.04</strong></td></tr><tr class="odd"><td style="text-align: center;">Adagrad</td><td style="text-align: center;"><strong>98.8</strong></td><td style="text-align: center;">20.31</td></tr></tbody></table></div><h1 id="归一化">4、归一化</h1><h1 id="学习率">5、学习率</h1><h1 id="batchsize">6、BatchSize</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数组</title>
    <link href="/2024/08/28/%E6%95%B0%E7%BB%84/"/>
    <url>/2024/08/28/%E6%95%B0%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="双指针">1、双指针</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">977. Squares of a Sorted Array</a></p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/977.%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E5%B9%B3%E6%96%B9.gif"></p></li></ol><h1 id="滑动窗口">2、滑动窗口</h1><ol type="1"><li><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209. Minimum Size Subarray Sum</a></li></ol><h1 id="模拟行为">3、模拟行为</h1><ol type="1"><li><a href="https://leetcode.cn/problems/spiral-matrix-ii/">59. Spiral Matrix II：模拟顺时针画矩阵的过程</a></li></ol><h1 id="前缀和">4、前缀和</h1><p>重复利用计算过的子数组之和，从而降低区间查询需要累加计算的次数。在涉及计算区间和的问题时非常有用</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>数组</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>归一化</title>
    <link href="/2024/08/27/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>/2024/08/27/%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="batchnorm">1、BatchNorm</h1><ul><li>针对一个Batch内的每个特征进行归一化</li></ul><h1 id="layernorm">2、LayerNorm</h1><ul><li><p>针对一个样本所有特征计算均值和方差，然后对样本进行归一化</p></li><li><p>减少内部协变量偏移（Internal Covariate Shift）： 内部协变量偏移是指在深度神经网络的训练过程中，每一层输入的分布会发生变化，导致网络的训练变得困难。层归一化通过对每一层的输入进行归一化处理，可以减少内部协变量偏移，使得每一层的输入分布更加稳定。</p></li><li><p><strong>稳定梯度</strong>： 层归一化有助于保持每一层输出的均值和方差稳定，从而使得梯度的传播更加稳定。这有助于减少梯度消失或梯度爆炸的问题，提高梯度在网络中的流动性，加快训练速度。<sup><a href="#ref1">[1]</a></sup></p></li><li><p>适合在RNN，LSTM等网络中使用，解决序列问题（不一定等长）<sup><a href="#ref2">[2]</a></sup></p></li></ul><h1 id="instancenorm">3、InstanceNorm</h1><h1 id="rmsnorm">4、RMSNorm</h1><h1 id="参考">参考</h1><ol type="1"><li><a href="ref1"></a><a href="https://www.cnblogs.com/shine-lee/p/11989612.html">Batch Normalization详解</a></li><li><a href="ref2"></a><a href="https://blog.csdn.net/u010159842/article/details/109326409">关于batch normalization和layer normalization的理解</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>归一化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>优化器、激活函数、损失函数与评价指标</title>
    <link href="/2024/08/26/%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    <url>/2024/08/26/%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="优化器">1、优化器</h1><ol type="1"><li><strong>随机梯度下降 SGD</strong> ：随机取mini batch计算梯度<ul><li><strong>优点</strong>：训练速度快</li><li><strong>缺点</strong>：引入噪声，容易陷入局部最优</li></ul></li><li><strong>SGD+Momentum</strong>：加入动量<ul><li><strong>优点</strong>：解决了SGD引入噪声的问题，因为动量的引入有机会逃脱局部最小值</li><li><strong>缺点</strong>：没有对学习率的自适应更新</li></ul></li><li><strong>Adam</strong>：自动调整学习率<ul><li><strong>优点</strong>：对学习率参数不敏感</li></ul></li></ol><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://blog.csdn.net/caip12999203000/article/details/127455203">常用的优化器合集</a></li><li><a href="https://www.bilibili.com/video/BV1bP4y1p7Gq/?spm_id_from=333.337.search-card.all.click&amp;vd_source=19303cc41e28593826f02ec94c4fc790">72 优化算法【动手学深度学习v2】</a></li></ol><h1 id="激活函数">2、激活函数</h1><ol type="1"><li><p><code>sigmoid</code>：<span class="math inline">\(\sigma(x)=\frac{1}{1+e^{-x}}\)</span></p><ul><li><p><span class="math inline">\(\rm max\{\sigma&#39;(x)\}=0.25&lt;1\)</span>，容易出现梯度消失</p></li><li><p>输出均值不为0，导致收敛变慢</p></li></ul></li><li><p><code>ReLU</code></p></li><li><p><code>Tanh</code>：<span class="math inline">\({\rm tanh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></p><ul><li>容易出现梯度消失</li><li><h6 id="zero-centered比sigmoid收敛速度快">zero-centered，比<code>sigmoid</code>收敛速度快</h6></li></ul></li></ol><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://cloud.tencent.com/developer/article/1829621">非零均值？激活函数也太硬核了</a></li><li><a href="https://liam.page/2018/04/17/zero-centered-active-function/">谈谈激活函数以0为中心 的问题</a></li></ol><h1 id="损失函数">3、损失函数</h1><ol type="1"><li><p><strong>交叉熵损失</strong>：刻画了两个概率分布之间的距离，旨在描绘通过概率分布 <span class="math inline">\(q\)</span> 来表达概率分布 <span class="math inline">\(p\)</span> 的困难程度 <span class="math display">\[H(p,q)=-\sum _xp(x)\log q(x)\]</span></p></li><li><p><strong>均方误差损失（MSE）</strong>：常用于回归预测中 <span class="math display">\[MSE=\frac{1}{n}\sum _{i=1}^n(y_i-\hat y_i)^2\]</span></p></li><li><p><strong>Focal Loss</strong>：解决样本中类别不均衡的问题 $$</p><p>$$</p></li></ol><h1 id="评价指标">4、评价指标</h1><h2 id="混淆矩阵-confusion-matrix">3.1 混淆矩阵 Confusion Matrix</h2><p><img src="https://i-blog.csdnimg.cn/blog_migrate/d7a4b71d1f2393e35df6988fd990c2d9.jpeg"></p><ul><li><p>精确率 Accuracy：评估模型总体，当数据分布不均衡时失效 <span class="math display">\[{\rm Acc=\frac{TP+TN}{TP+TN+FT+FP}}\]</span></p></li><li><p>F1分数 F1-score：评估模型总体，适合数据分布不均衡时，是正确率和召回率的调和平均数 <span class="math display">\[{\rm F1=2\cdot\frac{Precision\times Recall}{Precision+Recall}}\]</span></p></li><li><p>准确率 Precision：Pred中，真正为正类的样本有多少 <span class="math display">\[{\rm Precision=\frac{TP}{TP+FP}}\]</span></p></li><li><p>召回率 Recall/灵敏度 Sensitivity：正类的样本中，被识别出来的有多少 <span class="math display">\[{\rm Recall=\frac{TP}{TP+FN}}\]</span></p></li><li><p>特异度 Specificity： <span class="math display">\[{\rm Specificity=\frac{TN}{FP+TN}}\]</span></p></li><li><p><strong>Micro Metrics &amp; Macro Metrics</strong>：前者为加权（数据分别不平衡时），后者为平均（所有类别均等重要）</p></li></ul><h2 id="auc-roc">3.2 AUC-ROC</h2><p><strong>ROC</strong>：假设模型判定某样本为正的概率为<span class="math inline">\(p\)</span>，设定<span class="math inline">\(p’\)</span>为将样本划为正类的阈值，则按照不同阈值进行分类得到的TPR关于FPR的曲线则为ROC曲线，下面的面积就是AUC的值，越大代表分类器的效果越好；</p><p><strong>AUC</strong>：假设总共有m+n个样本，其中正样本m个，负样本n个，总共有<span class="math inline">\(m\times n\)</span>个样本对，计数，正样本预测为正样本的概率值大于负样本预测为正样本的概率值记为1，累加计数，然后除以样本对数目就是AUC的值</p><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://www.bilibili.com/video/BV1wz4y197LU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=19303cc41e28593826f02ec94c4fc790">【小萌五分钟】机器学习 | 模型评估: ROC曲线与AUC值</a></li><li><a href="https://blog.csdn.net/u012762410/article/details/127744611">AUC的三种计算方法及代码</a></li></ol><h2 id="其他">3.3 其他</h2><h3 id="bleu关注precision">3.3.1 BLEU（关注Precision）</h3><p><img src="/2024/08/26/%E4%BC%98%E5%8C%96%E5%99%A8%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/BLEU.png"></p><h3 id="rouge关注recall">3.3.2 ROUGE（关注Recall）</h3><ul><li><p><strong>ROUGE-N</strong>：统计N-gram上的召回率</p><p><img src="https://i-blog.csdnimg.cn/blog_migrate/775755d1d5c6866cae97f9a618437a2f.png"></p></li><li><p><strong>ROUGE-L</strong>：统计最长公共子序列（LCS）的召回率</p></li><li><p><strong>ROUGE-S</strong>：统计可跳跃的N-gram上的召回率</p></li></ul><h3 id="meteor">3.3.3 METEOR</h3><p>该指标考虑了基于整个语料库上的准确率和召回率，并关注翻译结果的语序，引入了chunk penalty</p><h1 id="参考">5、参考</h1><ol type="1"><li><a href="https://blog.csdn.net/seagal890/article/details/105059498">[机器学习笔记] 混淆矩阵（Confusion Matrix）</a></li><li><a href="https://blog.csdn.net/pearl8899/article/details/112452972">【NLG】(六)文本生成评价指标—— ROUGE原理及代码示例</a></li><li><a href="https://blog.csdn.net/pearl8899/article/details/112452652">【NLG】(二)文本生成评价指标—— METEOR原理及代码示例</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>优化器</tag>
      
      <tag>激活函数</tag>
      
      <tag>损失函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer</title>
    <link href="/2024/08/24/Transformer/"/>
    <url>/2024/08/24/Transformer/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="self-attention">1、Self-Attention</h1><ul><li><p><strong>缩放点积自注意力</strong>：<span class="math inline">\(Attention=Softmax(\frac{QK^t}{\sqrt{d_K}})\cdot V\)</span></p><blockquote><p>除以<span class="math inline">\(\sqrt{d}\)</span>：结果不应该随着Key的维度变大而变大；防止Softmax的值过大而导致梯度消失</p></blockquote></li><li><p><strong>时间复杂度</strong>：自注意力分数计算分为以下三个步骤：</p><ol type="1"><li><p>相似度计算：矩阵<span class="math inline">\(Q(n \times d)\)</span>和<span class="math inline">\(K^T(d \times n)\)</span>相乘，即<span class="math inline">\(n^2\cdot d\)</span></p><blockquote><p><span class="math inline">\(M\times N\)</span>和<span class="math inline">\(N\times D\)</span>两个矩阵乘积的时间复杂度为<span class="math inline">\(O(M\cdot N\cdot D)\)</span></p></blockquote></li><li><p>Softmax：时间复杂度为<span class="math inline">\(O(n^2)\)</span></p><blockquote><p>对于<span class="math inline">\(n\times n\)</span>的矩阵，每一行要进行n次指数运算，n-1次加法以及n次除法</p></blockquote></li><li><p>加权平均：可以看作大小为<span class="math inline">\(QK^t(n\times n)\)</span>和<span class="math inline">\(V(n\times d)\)</span>的两个矩阵相乘，即<span class="math inline">\(n^2\cdot d\)</span></p></li><li><p>综上，时间复杂度为<span class="math inline">\(O(n^2 \cdot d)\)</span>，这里，n是序列的长度，d是embedding的维度，该结论同样适用于多头自注意力</p></li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiheadAttention</span>(nn.Module):<br>    <span class="hljs-comment"># n_heads：多头注意力的数量</span><br>    <span class="hljs-comment"># hid_dim：每个词输出的向量维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hid_dim, n_heads, dropout</span>):<br>        <span class="hljs-built_in">super</span>(MultiheadAttention, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.hid_dim = hid_dim<br>        <span class="hljs-variable language_">self</span>.n_heads = n_heads<br><br>        <span class="hljs-comment"># 强制 hid_dim 必须整除 h</span><br>        <span class="hljs-keyword">assert</span> hid_dim % n_heads == <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 定义 W_q 矩阵</span><br>        <span class="hljs-variable language_">self</span>.w_q = nn.Linear(hid_dim, hid_dim)<br>        <span class="hljs-comment"># 定义 W_k 矩阵</span><br>        <span class="hljs-variable language_">self</span>.w_k = nn.Linear(hid_dim, hid_dim)<br>        <span class="hljs-comment"># 定义 W_v 矩阵</span><br>        <span class="hljs-variable language_">self</span>.w_v = nn.Linear(hid_dim, hid_dim)<br>        <span class="hljs-variable language_">self</span>.fc = nn.Linear(hid_dim, hid_dim)<br>        <span class="hljs-variable language_">self</span>.do = nn.Dropout(dropout)<br>        <span class="hljs-comment"># 缩放</span><br>        <span class="hljs-variable language_">self</span>.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, query, key, value, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># K: [64,10,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br>        <span class="hljs-comment"># V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br>        <span class="hljs-comment"># Q: [64,12,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span><br>        bsz = query.shape[<span class="hljs-number">0</span>]<br>        Q = <span class="hljs-variable language_">self</span>.w_q(query)<br>        K = <span class="hljs-variable language_">self</span>.w_k(key)<br>        V = <span class="hljs-variable language_">self</span>.w_v(value)<br>        <span class="hljs-comment"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span><br>        <span class="hljs-comment"># 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50</span><br>        <span class="hljs-comment"># 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span><br>        <span class="hljs-comment"># K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br>        <span class="hljs-comment"># V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span><br>        <span class="hljs-comment"># Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span><br>        <span class="hljs-comment"># 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span><br>        Q = Q.view(bsz, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.n_heads, <span class="hljs-variable language_">self</span>.hid_dim //<br>                   <span class="hljs-variable language_">self</span>.n_heads).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>        K = K.view(bsz, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.n_heads, <span class="hljs-variable language_">self</span>.hid_dim //<br>                   <span class="hljs-variable language_">self</span>.n_heads).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>        V = V.view(bsz, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.n_heads, <span class="hljs-variable language_">self</span>.hid_dim //<br>                   <span class="hljs-variable language_">self</span>.n_heads).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><br>        <span class="hljs-comment"># 第 1 步：Q 乘以 K的转置，除以scale</span><br>        <span class="hljs-comment"># [64,6,12,50] * [64,6,50,10] = [64,6,12,10]</span><br>        <span class="hljs-comment"># attention：[64,6,12,10]</span><br>        attention = torch.matmul(Q, K.permute(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)) / <span class="hljs-variable language_">self</span>.scale<br><br>        <span class="hljs-comment"># 序列填充的内容无需计算注意力，需要被mask掉</span><br>        <span class="hljs-comment"># 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attention = attention.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e10</span>)<br><br>        <span class="hljs-comment"># 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span><br>        <span class="hljs-comment"># 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span><br>        <span class="hljs-comment"># attention: [64,6,12,10]</span><br>        attention = <span class="hljs-variable language_">self</span>.do(torch.softmax(attention, dim=-<span class="hljs-number">1</span>))<br><br>        <span class="hljs-comment"># 第三步，attention结果与V相乘，得到多头注意力的结果</span><br>        <span class="hljs-comment"># [64,6,12,10] * [64,6,10,50] = [64,6,12,50]</span><br>        <span class="hljs-comment"># x: [64,6,12,50]</span><br>        x = torch.matmul(attention, V)<br><br>        <span class="hljs-comment"># 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span><br>        <span class="hljs-comment"># x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span><br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br>        <span class="hljs-comment"># 这里的矩阵转换就是：把多组注意力的结果拼接起来</span><br>        <span class="hljs-comment"># 最终结果就是 [64,12,300]</span><br>        <span class="hljs-comment"># x: [64,12,6,50] -&gt; [64,12,300]</span><br>        x = x.view(bsz, -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.n_heads * (<span class="hljs-variable language_">self</span>.hid_dim // <span class="hljs-variable language_">self</span>.n_heads))<br>        x = <span class="hljs-variable language_">self</span>.fc(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-comment"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span><br>query = torch.rand(<span class="hljs-number">64</span>, <span class="hljs-number">12</span>, <span class="hljs-number">300</span>)<br><span class="hljs-comment"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span><br>key = torch.rand(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">300</span>)<br><span class="hljs-comment"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span><br>value = torch.rand(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>, <span class="hljs-number">300</span>)<br>attention = MultiheadAttention(hid_dim=<span class="hljs-number">300</span>, n_heads=<span class="hljs-number">6</span>, dropout=<span class="hljs-number">0.1</span>)<br>output = attention(query, key, value)<br><span class="hljs-comment">## output: torch.Size([64, 12, 300])</span><br><span class="hljs-built_in">print</span>(output.shape)<br><br></code></pre></td></tr></table></figure><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/transformer.html">百度深度学习百科</a></li><li><a href="https://blog.csdn.net/suibianshen2012/article/details/122141294">self-attention为什么要除以根号d_k</a></li><li><a href="https://blog.csdn.net/qq_36936443/article/details/124124990">注意力机制</a></li><li><a href="https://blog.csdn.net/qq_42750193/article/details/122715902">多头注意力机制 +代码解读</a></li><li><a href="https://blog.csdn.net/Flag_ing/article/details/109129752">PyTorch：view() 与 reshape() 区别详解</a></li></ol><h1 id="transformer">2、Transformer</h1><ol type="1"><li>支持并行计算，提高推理速度👍</li><li>捕捉长距离依赖关系👍</li><li>计算成本高👎</li><li>对序列长度敏感👎</li></ol><h2 id="位置编码-positional-embedding">2.1 位置编码 Positional Embedding</h2><p>Transformer与RNN、LSTM等序列模型不同，不是一步步地处理输入；在同时处理一个序列的时候，需要为每个单词添加一个额外的编码来表示它在序列中的位置，这样模型就能够理解单词在序列中的相对位置。</p><p><strong>参考</strong>：</p><ol type="1"><li><a href="https://blog.csdn.net/m0_37605642/article/details/132866365">【Transformer系列】深入浅出理解Positional Encoding位置编码</a></li><li><a href="https://blog.csdn.net/qq_43391414/article/details/121061766">深入理解transformer中的位置编码</a></li><li><a href="https://github.com/tensorflow/tensor2tensor/issues/1591">Why add positional embedding instead of concatenate?</a></li></ol><h2 id="编码器-encoder">2.2 编码器 Encoder</h2><h2 id="解码器-decoder">2.3 解码器 Decoder</h2><h2 id="其他">2.4 其他</h2><ol type="1"><li><strong>Transformer的权重共享</strong>：<ul><li><strong>Encoder和Decoder的Embedding共享</strong>：会使得词表变大，计算Softmax时间变长；不过共用词表时可以更好表示两种语言中都有的一些数字、符号等；某些语言之间有关联，如英语、德语，因此可以共享一些语义；而像中文、英文之间差异很大，共享就没什么意义</li><li><strong>Decoder中Embedding层和FC层权重共享</strong>：Embedding和FC可以看做是一组逆过程，对于全连接层之前得到的向量x（某个词语的词嵌入表示），如果想最大化概率，最好计算点积的时候乘以一个一模一样的向量</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>Attention</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>链表</title>
    <link href="/2024/08/24/%E9%93%BE%E8%A1%A8/"/>
    <url>/2024/08/24/%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="链表删除头结点">1、链表删除头结点</h1><ul><li>设置虚拟头结点</li><li>将原来链表的头指针向后移动</li></ul><ol type="1"><li><p><a href="https://leetcode.cn/problems/remove-linked-list-elements/">203. Remove Linked List Elements</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 虚拟头节点法</span><br><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeElements</span>(<span class="hljs-params">self, head: <span class="hljs-type">Optional</span>[ListNode], val: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">Optional</span>[ListNode]:<br>        <span class="hljs-comment"># 创建虚拟头部节点以简化删除过程</span><br>        dummy_head = ListNode(<span class="hljs-built_in">next</span> = head)<br>        <br>        <span class="hljs-comment"># 遍历列表并删除值为val的节点</span><br>        current = dummy_head<br>        <span class="hljs-keyword">while</span> current.<span class="hljs-built_in">next</span>:<br>            <span class="hljs-keyword">if</span> current.<span class="hljs-built_in">next</span>.val == val:<br>                current.<span class="hljs-built_in">next</span> = current.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">else</span>:<br>                current = current.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-keyword">return</span> dummy_head.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol><h1 id="反转链表">2、反转链表</h1><figure><img src="https://code-thinking.cdn.bcebos.com/gifs/206.%E7%BF%BB%E8%BD%AC%E9%93%BE%E8%A1%A8.gif" alt="反转链表"><figcaption aria-hidden="true">反转链表</figcaption></figure><ol type="1"><li><p><a href="https://leetcode.cn/problems/reverse-linked-list/">206. Reverse Linked List</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 双指针法</span><br><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverseList</span>(<span class="hljs-params">self, head: ListNode</span>) -&gt; ListNode:<br>        cur = head   <br>        pre = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">while</span> cur:<br>            temp = cur.<span class="hljs-built_in">next</span> <span class="hljs-comment"># 保存一下 cur的下一个节点，因为接下来要改变cur-&gt;next</span><br>            cur.<span class="hljs-built_in">next</span> = pre <span class="hljs-comment">#反转</span><br>            <span class="hljs-comment">#更新pre、cur指针</span><br>            pre = cur<br>            cur = temp<br>        <span class="hljs-keyword">return</span> pre<br></code></pre></td></tr></table></figure></li></ol><h1 id="快慢指针">3、快慢指针</h1><p><strong>删除链表倒数第n个节点</strong></p><ol type="1"><li><p><a href="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/">19. Remove Nth Node From End of List</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params">self, head: ListNode, n: <span class="hljs-built_in">int</span></span>) -&gt; ListNode:<br>        <span class="hljs-comment"># 创建一个虚拟节点，并将其下一个指针设置为链表的头部</span><br>        dummy_head = ListNode(<span class="hljs-number">0</span>, head)<br>        <br>        <span class="hljs-comment"># 创建两个指针，慢指针和快指针，并将它们初始化为虚拟节点</span><br>        slow = fast = dummy_head<br>        <br>        <span class="hljs-comment"># 快指针比慢指针快 n+1 步</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n+<span class="hljs-number">1</span>):<br>            fast = fast.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-comment"># 移动两个指针，直到快速指针到达链表的末尾</span><br>        <span class="hljs-keyword">while</span> fast:<br>            slow = slow.<span class="hljs-built_in">next</span><br>            fast = fast.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-comment"># 通过更新第 (n-1) 个节点的 next 指针删除第 n 个节点</span><br>        slow.<span class="hljs-built_in">next</span> = slow.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-keyword">return</span> dummy_head.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol><p><strong>确定中点</strong></p><ol type="1"><li><p><a href="https://leetcode.cn/problems/sort-list/">148. Sort List</a></p><p>无头结点链表的快慢指针，快指针初始比慢指针快一步</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">slow,fast=head,head.<span class="hljs-built_in">next</span><br><span class="hljs-keyword">while</span> fast <span class="hljs-keyword">and</span> fast.<span class="hljs-built_in">next</span>:<br>    slow=slow.<span class="hljs-built_in">next</span><br>    fast=fast.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol><h1 id="环形链表">4、环形链表</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/linked-list-cycle-ii/">142. Linked List Cycle II</a></p><p>这个题目首先要利用快慢指针（步长分别为1、2）求出一个圈的长度是多少，然后利用删除链表中倒数第n个节点中的思想再次使用快慢指针（一个指针领先头指针一个圈的长度）求出入圈节点；下图为另一种做法：上</p><figure><img src="https://code-thinking.cdn.bcebos.com/gifs/142.%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8II%EF%BC%88%E6%B1%82%E5%85%A5%E5%8F%A3%EF%BC%89.gif" alt="另一种做法"><figcaption aria-hidden="true">另一种做法</figcaption></figure></li></ol><h1 id="归并排序链表">5、归并排序链表</h1><p><a href="https://leetcode.cn/problems/sort-list/">148. Sort List</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>链表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·I</title>
    <link href="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/"/>
    <url>/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="阅读论文">〇、阅读论文</h1><ol type="1"><li><a href="https://arxiv.org/abs/1904.07734">增量学习的三种场景及方法：Three scenarios for continual learning</a></li><li><a href="https://arxiv.org/abs/2112.08654">预训练模型图像类增量学习：Learning to Prompt for Continual Learning</a></li><li><a href="https://arxiv.org/abs/2402.12035">时间序列类增量学习的基准：Class-incremental Learning for Time Series:Benchmark and Evaluation</a></li><li><a href="https://arxiv.org/abs/2204.04799">双提示增量学习：DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning</a></li><li><a href="https://arxiv.org/abs/2402.03885">时间序列大模型：MOMENT: A Family of Open Time-series Foundation Models</a></li><li><a href="https://arxiv.org/abs/2403.20317">卷积提示：Convolutional Prompting meets Language Models for Continual Learning</a></li></ol><h1 id="一相关工作">一、相关工作</h1><h2 id="增量学习的三种场景及方法">1. 增量学习的三种场景及方法</h2><h3 id="实验方法">1.1 实验方法</h3><ol type="1"><li>正则化（SI、EWC）</li><li>重放（Replay）<ol type="1"><li>用之前模型对当前任务的Input进行标注生成伪数据（LwF）</li><li>生成新数据（DGR、DGR+distill）</li><li>原始数据重现（可能会有隐私或内存问题）</li></ol></li><li>Exemplars（特征提取+最近类均值+重放：iCaRL）</li></ol><h3 id="实验结果">1.2 实验结果</h3><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/MNIST结果.png" alt="实验结果对比：正则化的效果普遍不太行"><figcaption aria-hidden="true">实验结果对比：正则化的效果普遍不太行</figcaption></figure><h2 id="预训练模型图像类增量学习">2. 预训练模型图像类增量学习</h2><h3 id="实验方法-1">2.1 实验方法</h3><ol type="1"><li>设Input为<span class="math inline">\(x\)</span>，通过<span class="math inline">\(q(x)\)</span>进行映射</li><li>计算<span class="math inline">\(q(x)\)</span>与Key的余弦相似度并选择TopN（同一Prompt被选择的次数越多，下次被选择的概率越小）</li><li>将TopN向量，以及进入Embedding层的Input拼接，并放入预训练模型</li></ol><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/L2P.png" alt="L2P"><figcaption aria-hidden="true">L2P</figcaption></figure><h4 id="vit">2.1.1 ViT</h4><p>在细读论文时需要一些ViT的知识，于是找到了这篇博客：<a href="https://blog.csdn.net/MengYa_Dream/article/details/126600748">ViT的cls token作用</a></p><ol type="1"><li>论文中假设输入的2D图像<span class="math inline">\(x \in \mathbb{R}^{H \times W \times C}\)</span>中<span class="math inline">\(H\)</span>，<span class="math inline">\(W\)</span>，<span class="math inline">\(C\)</span>分别代表高度，宽度和通道数</li><li><span class="math inline">\(f=f_r \circ f_e\)</span>是一个复合函数，指的是将输入embedding后送入自注意力层的结果</li><li>ViT中，图像需要切分为若干patch；在这里flattened 2D patches <span class="math inline">\(x_p \in \mathbb{R}^{L \times (S^2 \cdot C)}\)</span>，其中<span class="math inline">\(L\)</span>为patches个数，<span class="math inline">\(S\)</span>为patch size</li><li>为了简化表示，<span class="math inline">\(x_p\)</span>的第一个token被当做是<code>[class token]</code></li><li>embedding layer的作用<span class="math inline">\(f_e:\mathbb{R}^{L \times (S^2 \cdot C)} \rightarrow \mathbb{R}^{L \times D}\)</span>，其中<span class="math inline">\(D\)</span>是嵌入维度</li><li>一个Prompt可以看作是<span class="math inline">\(P_e\in \mathbb{R}^{L_p \times D} ,L_p\)</span>是Prompt的长度</li><li>将Prompt和Input拼接起来可以得到<span class="math inline">\(x_p=[P_e;x_e]\)</span></li><li>送入预训练模型后，即<span class="math inline">\(f=f_r(x_p)\)</span></li></ol><h4 id="prompt-pool">2.1.2 Prompt Pool</h4><p>为什么要采用Pool的原因有三：</p><ol type="1"><li>在推理的时候task identity是不得而知的，因此为每个task设置一个prompt是不可行的</li><li>即使推理时上述信息可以得知，那么task-independent prompt可能阻止了相似任务间的信息共享</li><li>如果只用一个prompt，则会造成灾难性遗忘</li></ol><p>于是定义Prompt集合：<span class="math inline">\(P = \{P_1, P_2, \cdots , P_M \}\)</span>，则ViT的输入为：<span class="math inline">\(x_p = [P_{s_1} ; \cdots ; P_{s_N} ; x_e], 1 ≤ N ≤ M\)</span></p><p>Prompt Pool中其实是Key-Value pair：<span class="math inline">\(\{(k_1, P_1), (k_2, P_2), \cdots , (k_M , P_M )\},k_i \in \mathbb{R}^{D_k}\)</span></p><div class="note note-info">            <p>根据论文来看，M为所有Prompt 的数量，是一个可以定义的参数</p>          </div><h4 id="prompt-query">2.1.3 Prompt Query</h4><ol type="1"><li>Query应仅跟输入实例有关，并且没有可学习的参数；定义Query Function：<span class="math inline">\(q : \mathbb{R}^{H\times W \times C} → \mathbb{R}^{D_k}\)</span></li><li>作者用了ViT作为feature extractor：<span class="math inline">\(q(x) = f (x)[0,:]\)</span>，其实就是<code>[class token]</code>对应的向量<ul><li>同时，作者指出用ConvNet也是可行的</li></ul></li><li>定义相似度计算函数<span class="math inline">\(\gamma : \mathbb{R}^{D_k} \times \mathbb{R}^{D_k} \rightarrow \mathbb{R}\)</span>，作者发现余弦相似度就很好</li><li>问题简化为<span class="math inline">\(K_x = \mathop{argmin}\limits_{\{s_i\}^N_{i=1}\in[1,M ]} \sum\limits_{i=1}^N \gamma (q(x), k_{s_i} )\)</span>，其中<span class="math inline">\(K_x\)</span>是所有Key的一个子集<ul><li>为了让每个Prompt都有机会被引用，训练时改写为<span class="math inline">\(K_x = \mathop{argmin}\limits_{\{s_i\}^N_{i=1}\in[1,M ]} \sum\limits_{i=1}^N \gamma (q(x), k_{s_i} )\cdot h_{s_i}\)</span></li></ul></li></ol><h4 id="loss-function">2.1.4 Loss Function</h4><p><span class="math display">\[\mathop{\rm min} \limits_{P,K,\phi} \mathcal{L}(g_{\phi}(f^{avg}_r (x_p)), y) + \lambda\sum\limits_{K_x}  \gamma (q(x), k_{s_i} ),\quad s.t. K_x在2.1.3中已经得到\]</span></p><h3 id="实验结果-1">2.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/CIFAR.png"></p><h2 id="时间序列类增量学习的基准">3. 时间序列类增量学习的基准</h2><h3 id="实验方法-2">3.1 实验方法</h3><h4 id="正则化">3.1.1 正则化</h4><p>基于正则化的方法（LwF、MAS、<span class="math inline">\(DT^2W\)</span>）只在UCI-HAR和UWave等任务较少的简单数据集中表现出明显的优势</p><h4 id="归一化bn-vs-ln">3.1.2 归一化：BN vs LN</h4><p>在大多数情况下使用LN的效果要比BN更好</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/Norm.png" alt="Norm"><figcaption aria-hidden="true">Norm</figcaption></figure><h4 id="rehearsal-baseder-vs-gr">3.1.3 Rehearsal-based：ER vs GR</h4><p>GR在简单数据上较好，甚至好过ER，复杂数据上表现不佳；可能是因为难以生成复杂数据，以及无法控制生成的类别</p><h4 id="meomory-budget在bn和ln上的差异">3.1.4 Meomory budget在BN和LN上的差异</h4><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/memory.png"></p><ul><li>基于ER的方法通常性能会随着Memory budget的增大而提高，到达一定程度会趋于饱和</li><li>在内存足够时，BN在除ASER的方法都与Offline有很大差异，因为随着任务数量增加<span class="math inline">\(B_{M}\)</span>中旧样本比例会减少；LN则接近Offline</li></ul><h4 id="分类器的选择">3.1.5 分类器的选择</h4><p>分类器选择取决于方法与数据集，在ER上的区别并不明显</p><h2 id="双提示增量学习">4. 双提示增量学习</h2><h3 id="实验方法-3">4.1 实验方法</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/dual.png"></p><ol type="1"><li>G-Prompt（唯一，学习任务的一般性）和E-Prompt（每个任务对应一个，学习任务的特殊性）</li><li>利用Prefix-Tuning（效果相比Prompt-Tuning更好，且未改变输入维度）将<span class="math inline">\(p\)</span>分解成<span class="math inline">\(p_{K},p_{V}\in \mathbb R^{L_{p}/2\times D}\)</span>，并且加到多头自注意力层中：<span class="math inline">\(f^{\rm Pre-T}_{\rm prompt}(p,h)={\rm MSA}(h_{Q},[p_{k};h_{K}],[p_{v};h_{V}])\)</span></li><li>训练时，利用<span class="math inline">\(\mathcal L_{\rm match}(x,k_t)=\gamma(q(x),k_t) ,x \in \mathcal D_t\)</span>更新<span class="math inline">\(k_t\)</span>，将G-Prompt和E-Prompt分别加到某些MSA中</li><li>推理时，Input通过Query function映射并选择最相似的E-Prompt</li></ol><h3 id="实验结果-2">4.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/dual_res.png"></p><h2 id="时间序列大模型">5. 时间序列大模型</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/pile.png"></div><div class="group-image-wrap"><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_radar.png"></div></div></div><h3 id="time-series-pile">5.1 Time-series Pile</h3><h3 id="模型结构">5.2 模型结构</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment.png"></p><ol type="1"><li>将时间序列数据分为不重叠的固定长度序列patches（减少内存占用和计算复杂度），映射到D维向量</li><li>在预训练期间，通过使用特殊的掩码嵌入[MASK]来替换 patches嵌入，目标是学习 patches 嵌入</li><li>这些嵌入可以使用轻量级的重建头，而不是与编码器相同大小的解码器，来重建输入时间序列。以便在保持编码器的大部分参数和高级特征不变的同时，对有限数量的可训练参数进行任务特定的微调，从而进行必要的架构修改。</li></ol><h3 id="实验结果-3">5.3 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_res1.png"></p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_res2.png" alt="零样本泛化能力"><figcaption aria-hidden="true">零样本泛化能力</figcaption></figure><h2 id="卷积提示">6. 卷积提示</h2><h3 id="实验方法-4">6.1 实验方法</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/卷积.png"></p><ol type="1"><li>GPT-3给出每个任务中类别的visual features，利用BERT进行word embedding</li><li>根据当前任务<span class="math inline">\(t\)</span>和之前任务中类别的visual features计算相似度，取最大相似度的平均值为<span class="math inline">\(sim_{t}\)</span></li><li><span class="math inline">\(J_{max}\)</span>为每个任务的最大Prompt Generator数量，于是$J_{t}=(1-sim_{t}) J_{max} $</li><li>输入图像，<span class="math inline">\(PN_{\phi}\)</span>（唯一）将前一层的<code>[CLS]</code>投影到和Prompt Keys（共M个，唯一） 相同维度并计算余弦相似度得到<span class="math inline">\(s_{i}\)</span></li><li>对于每一层、每一个头有一套Shared Embedding（唯一）和一套Prompt Generator（M个，对应于Prompt Keys；训练时之前任务对应的Generator会被冻结）；对于<span class="math inline">\(G^K_{h,l,m}\)</span>、<span class="math inline">\(G^V_{h,l,m}\)</span>分别与<span class="math inline">\(SE^{K}_{l,h}\)</span>，<span class="math inline">\(SE^{V}_{l,h}\)</span>进行卷积后，和<span class="math inline">\(s_{i}\)</span>相乘求和，将结果concat到下一层：<span class="math inline">\(P_{l+1}^{K}=\sum\limits_{m=1}^{M}s_{l,m}PC^{K}_{l,h,m}\)</span>，<span class="math inline">\(P_{l+1}^{V}=\sum\limits_{m=1}^{M}s_{l,m}PC^{V}_{l,h,m}\)</span></li></ol><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/相似类别.png" alt="根据visual features计算相似度"><figcaption aria-hidden="true">根据visual features计算相似度</figcaption></figure><h3 id="实验结果-4">6.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/con_cifar100.png"></p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/con_imagenet.png"></p><h1 id="二l2p">二、L2P</h1><h2 id="踩坑">1. 踩坑</h2><p>在服务器上部署之后，Github上提到没有CIFAR-100的话，在<code>datasets.py</code>更改如下字段：</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/教程.png" alt="Github上的教程"><figcaption aria-hidden="true">Github上的教程</figcaption></figure><p>但其实不用改的，本来就是<code>True</code></p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/源码1.png" alt="datasets.py"><figcaption aria-hidden="true">datasets.py</figcaption></figure><p>不过在运行的时候会报这个错：</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/报错.png" alt="报错说没有cifar100_12p这个路径"><figcaption aria-hidden="true">报错说没有cifar100_12p这个路径</figcaption></figure><p>后来找到了一个脚本文件<code>train_cifar100_l2p.sh</code>执行，但本地并没有<code>local_datasets</code>路径：</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/路径.png"></p><p>这个报错上面提到，这个数据集是在<code>torchvision</code>里面<code>cifar.py</code>下载的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[rank0]:   File <span class="hljs-string">&quot;/home/eee/zym/l2p-pytorch/datasets.py&quot;</span>, line <span class="hljs-number">107</span>, <span class="hljs-keyword">in</span> get_dataset<br>[rank0]:   dataset_train = datasets.CIFAR100(args.data_path, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform_train)<br>[rank0]:   File <span class="hljs-string">&quot;/home/eee/anaconda3/envs/zym/lib/python3.8/site-packages/torchvision/datasets/cifar.py&quot;</span>, line <span class="hljs-number">66</span>, <span class="hljs-keyword">in</span> __init__<br>[rank0]:     <span class="hljs-variable language_">self</span>.download()<br></code></pre></td></tr></table></figure><p>新建了<code>local_datasets</code>后运行脚本文件，还是报错；最后发现脚本路径写的有问题，改成下面即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">--data-path ./local_datasets/ \<br></code></pre></td></tr></table></figure><h2 id="分析">2. 分析</h2><h3 id="代码结构">2.1 代码结构</h3><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/结构.png" alt="代码结构"><figcaption aria-hidden="true">代码结构</figcaption></figure><p>关于为什么要出现<code>original_model</code>和<code>model</code>，是因为作者在这里做了消融实验，微调了model中的部分参数。不过从结果来看，并没有比原始论文的要好。</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/冻结.png"></p><h3 id="运行结果">2.2 运行结果</h3><p>注意到Loss出现了负值，是因为采用的多元交叉熵损失函数。</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/loss.png"></p><h1 id="三初步设计">三、初步设计</h1><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/流程.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BIGAI1-基于Langchain构建用于数据生成Agent</title>
    <link href="/2024/08/06/%E5%9F%BA%E4%BA%8ELangchain%E6%9E%84%E5%BB%BA%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90Agent/"/>
    <url>/2024/08/06/%E5%9F%BA%E4%BA%8ELangchain%E6%9E%84%E5%BB%BA%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90Agent/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>类似的文章已经在<a href="https://blog.csdn.net/sersama/article/details/139179060?spm=1001.2014.3001.5502">CSDN</a>发布过了，在这里进行简单总结。</p><h1 id="背景">背景</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>实习</tag>
      
      <tag>BIGAI</tag>
      
      <tag>数据生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>百度-基于大模型的广告检索</title>
    <link href="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/"/>
    <url>/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="比赛内容">比赛内容</h1><h2 id="生成式检索">生成式检索</h2><blockquote><p>在过去的5到10年间，信息检索领域的利用语言理解的进展，例如将表示学习应用于检索，使用预训练语言模型进行知识迁移，在各种任务上取得了显著的改进。但当前信息检索系统的核心结构上与几十年前的经典系统相比也没有发生太大的变化，主要遵循index-retrieval-rank的模式。这一模式历经时间考验，但在端到端优化、错误传播和大规模文档索引需求方面存在局限性。这些局限性催生了生成式检索(GenIR)的发展。</p><p>生成式检索的动机在于突破传统的检索范式，提供一种更灵活、效率更高的检索方法。它旨在直接生成与Query的信息资源ID，而不是依赖传统的index-retrieval-rank过程。GenIR通过将整个语料库的知识编码到模型参数中，从而允许在训练期间针对IR任务进行端到端的优化。这种方法不仅提高了检索效率，也可以更全面地捕获Query和Doc之间的相关性。</p><p>主流训练方案分为两阶段训练以及多任务学习。前者在第一阶段将Doc和DocID建立起联系，第二阶段将Query和DocID建立关联，而后者同时将Doc和Query作为输入，直接得到DocID。在我们的方案中采取的是两阶段训练。</p></blockquote><h2 id="数据说明">数据说明</h2><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/官网数据.png"></p><h1 id="冠军方案">冠军方案</h1><p><a href="https://blog.csdn.net/qq_63598369/article/details/142137384">2024年百度商业AI技术创新大赛冠军经验分享——基于大模型的广告检索赛道</a></p><h1 id="初赛数据分析">初赛：数据分析</h1><h2 id="query落地页核心词长度分布">Query、落地页、核心词长度分布</h2><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/query.png" alt="query"><figcaption aria-hidden="true">query</figcaption></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/lp.png" alt="lp"><figcaption aria-hidden="true">lp</figcaption></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/ct.png" alt="ct"><figcaption aria-hidden="true">ct</figcaption></figure><h2 id="广告id分布">广告ID分布</h2><p>在发现Query中的ID远少于广告ID数量后，我想统计ID的分布。最开始采用柱状图，但由于数据量过大，最后绘制出的图像不直观。因此，我向Kimi问到了一种估计分布密度的方法<a href="https://blog.csdn.net/weixin_39910711/article/details/107307509">KDE</a>，这是一种非参数估计方法，相对于参数估计（如：最大似然估计）不需要对数据的分布有先验的认识：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> gaussian_kde<br><br>json file =<span class="hljs-string">&#x27;/home/zym/桌面/FEIJIANG/Doc/Doc_train_ultra.json&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(json file,<span class="hljs-string">&#x27;r&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<span class="hljs-keyword">as</span> file:<br>    js =[json.loads(line)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file]<br>    <br>tgt_values=[ _[<span class="hljs-string">&#x27;tgt&#x27;</span>]<span class="hljs-keyword">for</span> <span class="hljs-keyword">in</span> js]<br>tgt_array=np.array(tgt_values,dtype=np.float64)<br><br><span class="hljs-comment"># 将列表转换为NumPy数组</span><br>tgt_array = np.array(tgt_values, dtype=np.float64)<br><br><span class="hljs-comment"># 计算核密度估计，使用高斯核</span><br>kde = gaussian_kde(tgt_array)<br><br><span class="hljs-comment"># 绘制平滑的曲线</span><br>x = np.linspace(np.<span class="hljs-built_in">min</span>(tgt_array), np.<span class="hljs-built_in">max</span>(tgt_array), <span class="hljs-number">1000</span>)<br>y = kde(x)<br></code></pre></td></tr></table></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/ID分布.png" alt="初赛数据的广告ID分布"><figcaption aria-hidden="true">初赛数据的广告ID分布</figcaption></figure><h1 id="初赛方案">初赛：方案</h1><h2 id="query增强1利用click重采样query">Query增强1：利用Click重采样Query</h2><p>得到的数据中有Query搜索后结果的点击量，考虑到点击数量越多，说明这条Query的有效性就越高，越能达到用户的目的。因此我们按Click数量对Query进行重采样。比如：“小天鹅洗衣”有4条点击量，我们便再次生成4条放入Query中。</p><p>我们考虑过对新生成的Query进行改写以及小范围的增删，但效果不太理想，于是没有继续做下去。</p><h2 id="doc增强利用bge-m3筛选对query相似度最高的片段">Doc增强：利用BGE-M3筛选对Query相似度最高的片段</h2><p>通过数据分析发现，Doc的长度（主要是广告落地页）平均长度达到上百。在浏览具体数据的时候，发现其中无用的语料很多：想象点开一个广告页，上面可能有各种各样的商品推广内容，以及一些没用的信息。基于此，我们遍历每条Query（只占广告页总量的一部分）并使用BGE-M3将其编码。同样地，我们对核心词和广告落地页的每条语料进行编码，选取其中相似度最高的四条作为我们真正的Doc。</p><p>上面提到，Query对应的广告只占一部分，对于剩下广告的落地页，我们随机进行相同长度的采样。</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/doc.png"></p><h2 id="query增强2利用qwen结合doc生成新的query">Query增强2：利用Qwen结合Doc生成新的Query</h2><p>对于Query的进一步增强，我想到在本地部署一个小参数量的语言模型，输入为Doc中一个片段（太长会导致显存资源暴涨），并附上Prompt，希望Qwen输出10条内容相似，长度相似的Query：</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/query2.png"></p><h2 id="网格搜索最佳参数">网格搜索最佳参数</h2><p>网格搜索需要的时间以及计算资源都很大，因此我们根据经验适度调整了一些参数，对后期的上分起了一些作用：</p><p><strong>训练部分</strong>：因为输入的Query长度很小，我们不希望过度的padding影响encoding，因此我们主要调节的是<code>max_seq_len</code></p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/train.png"></p><p><strong>推理部分</strong>：主要调节<code>top_k</code>和<code>top_p</code>用于挑选候选结果，<code>temperature</code>用来增加多样性</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/infer.png"></p><h1 id="复赛方案">复赛：方案</h1><p>因为复赛的时候结束实习了，因此没法用4090进行高效率的数据增强，加之来到新加坡的最初几天比较忙，就没有深度研究比赛方案。我们的复赛跟初赛的方案差距不是很大，甚至就增强而言可能没有初赛的数据处理完善。复赛的数据量远大于初赛，因此记录下我们的一些尝试。</p><p>数据集中有88wDoc，因为论文中提到直接索引效果较好，因此取落地页连续的短语作为Doc；特别地，观察到Doc长度分布不均，因此在生成Doc时设定为长度越长、采样次数越多。最终得到880wDoc，在第一阶段进行索引学习时4000steps已趋于收敛，遂暂停。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/简单Doc.png" alt="简单构建Doc"><figcaption aria-hidden="true">简单构建Doc</figcaption></figure><p>对于Query，按Click进行增强，从500w得到1200w数据。在上一步得到的权重上训练，效果还可以。</p><h2 id="尝试一semantic-docid">尝试一：Semantic DocID</h2><p>官方所给的数据集中ID是没有规律的，不同的广告内容ID可能邻近，相似的广告ID可能很远，因此这对生成式模型而言，像是在做一个“多分类”的任务。在这篇<a href="https://arxiv.org/abs/2309.13335">Model-enhanced Vector Index</a>论文中提到利用Residual Quantization进行层次聚类效果可能会好，于是考虑用模型对广告内容进行Embedding，但因为本地没有显卡用，并且在AIstudio又不能用Pytorch，因此采用<a href="https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/ERNIE/contents.html">Ernie-3.0-mini</a>。</p><h3 id="q1embedding文件太大">Q1：Embedding文件太大</h3><p>即使采用mini，在对约29000个句子（3%）进行Embedding之后，我的json文件就已经达到了220MB</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/mini.png"></p><p>文件存储格式如下</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/json.png"></p><h3 id="q2内存过大">Q2：内存过大</h3><p>因为词袋方法耗时更长，占用内存更大，后来又再次尝试使用Embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;/home/aistudio/embed.json&quot;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data=[json.loads(line)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br></code></pre></td></tr></table></figure><p>居然在读取的时候内存就满了。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/内存.png" alt="内存满了"><figcaption aria-hidden="true">内存满了</figcaption></figure><h2 id="尝试二词袋">尝试二：词袋</h2><p>这里使用Paddle里的Taskflow进行分词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./embed.json&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(res)+<span class="hljs-number">1</span>)):<br>        seg = Taskflow(<span class="hljs-string">&quot;word segmentation&quot;</span>)temp = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x:x!=,<span class="hljs-string">&#x27;,seg(res[str(i)])))</span><br><span class="hljs-string">        # 使用counter计算每个分词出现的次数</span><br><span class="hljs-string">        word count= counter(temp)</span><br><span class="hljs-string">        #按照出现次数进行升序排序</span><br><span class="hljs-string">        sorted word count = dict(sorted(word count.items(), key=lambda item: item[1]))</span><br><span class="hljs-string">        json.dump(&#123;&quot;tgt&quot;:i,&quot;bow&quot;: sorted word count&#125;,f,ensure ascii=False)</span><br></code></pre></td></tr></table></figure><p>最终分词结果文件很大，没有进一步处理。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/分词.png" alt="分词结果"><figcaption aria-hidden="true">分词结果</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>百度</tag>
      
      <tag>搜广推</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BIGAI2—微调Qwen-7B生成1-Hop描述</title>
    <link href="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/"/>
    <url>/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="背景">背景</h1><p>在生成QA时，我们希望生成的数据具有<mark>一定的复杂度</mark>，并且具有<mark>生活化</mark>的特点。</p><p>最基本的一个想法是：当我们需要某个子图（Region Graph）中的物体信息，便直接从已有信息Info中查找，并送入GPT生成QA；然而，基于这种方法所生成的QA十分的简单。我利用GPT-4简单生成了一个Demo，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Base.png" alt="Demo:直接通过知识图谱生成的QA"><figcaption aria-hidden="true">Demo:直接通过知识图谱生成的QA</figcaption></figure><p>很显然这种数据不具有<mark>多样性</mark>，也不满足我们的要求。我们认为在若干定语修饰后的QA可以极大地丰富数据多样性：</p><ul><li>地毯左面紧挨的儿童椅和行李箱旁的双人床哪个更大？</li><li>空调下面的挂画和墙壁右边的衣服哪个数量更多？</li></ul><p>本质上，我们希望对原有数据增加一定的定语——<mark>将程式化的短语转变为流畅丰富的句子</mark>，效果如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Improve.png" alt="1-Hop描述（右）可以丰富QA内容"><figcaption aria-hidden="true">1-Hop描述（右）可以丰富QA内容</figcaption></figure><p>生成大批量数据离不开大模型的帮助。出于对计算资源以及生成效率的考虑，我们没有选择API或是参数量更大的模型，而是在本地部署Qwen-7B，并期待通过微调可以让其生成我们希望的1-Hop描述，如下图框出部分：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/流程.png" alt="生成QA数据集流程"><figcaption aria-hidden="true">生成QA数据集流程</figcaption></figure><h1 id="方法">方法</h1><h2 id="原理浅析">原理浅析</h2><p>除了SFT之外，人们采用更少计算资源进行微调的方法称为PEFT，其中又包含若干种方法。</p><p>我在网上找到的一个时间轴，可以很好地展示发展历程，如下图：</p><figure><img src="https://oscimg.oschina.net/oscnet/9ac87619-bf4f-41d6-9c94-d7a8cb030acb.png" alt="主流微调方法的发展"><figcaption aria-hidden="true">主流微调方法的发展</figcaption></figure><p>因为显卡资源的限制，我们采用P-tuning来进行微调，从而生成1-Hop描述，<a href="https://arxiv.org/abs/2103.10385">论文原文</a>。</p><p>与Prefix-tuning直接给Prompt增加前缀不同，P-tuning在Prompt中随意加入Embedding后的Virtual Token，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/P-tuning.png" alt="P-tuning结构示意图"><figcaption aria-hidden="true">P-tuning结构示意图</figcaption></figure><h2 id="训练数据">训练数据</h2><p>考虑到时间成本以及Token成本，我们决定在Info中选取<mark>部分数据</mark>，将这一部分数据和利用GPT-4生成的1-Hop描述拼接在一起，形成了微调用的训练数据集。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/数据.png" alt="准备微调数据"><figcaption aria-hidden="true">准备微调数据</figcaption></figure><p>通过将人工制作的模板（如下图）与Info中的信息作为Prompt，我们利用GPT-4进行生成；生成的内容需要进一步通过正则表达式的处理，最终成为1-Hop描述。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT1.png" alt="模板1"><figcaption aria-hidden="true">模板1</figcaption></figure><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT2.png" alt="模板2"><figcaption aria-hidden="true">模板2</figcaption></figure><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT3.png" alt="模板3"><figcaption aria-hidden="true">模板3</figcaption></figure><h2 id="训练过程">训练过程</h2><p>Qwen-7B接收到的Prompt由另一套模板以及Info中的信息组成，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Prompt.png" alt="询问功能的模板"><figcaption aria-hidden="true">询问功能的模板</figcaption></figure><p>代码需要放到HGX集群上跑，但是需要排队很久。之前微调的时候loss从14下降到8左右。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/HGX.png" alt="GPU集群"><figcaption aria-hidden="true">GPU集群</figcaption></figure><h1 id="结果">结果</h1><p>整个对Info进行增强生成1-Hop描述的过程很慢，最终也没有将整个map的3000个视角（1000个场景*3个视角）遍历完。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/结果.png" alt="结果"><figcaption aria-hidden="true">结果</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>微调</tag>
      
      <tag>实习</tag>
      
      <tag>BIGAI</tag>
      
      <tag>数据生成</tag>
      
      <tag>大模型</tag>
      
      <tag>P-tuning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
