<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TSCIL论文笔记·III</title>
    <link href="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/"/>
    <url>/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="时间线">〇、时间线</h1><p><strong>9月19日</strong></p><ol type="1"><li>阅读有关时间序列在线增量学习的工作</li></ol><h1 id="一相关工作">一、相关工作</h1><ol type="1"><li><a href="https://arxiv.org/abs/2309.12659">OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling</a></li><li><a href="https://arxiv.org/abs/2202.11672">Learning Fast and Slow for Online Time Series Forecasting</a></li></ol><h2 id="onenet">1、OneNet</h2><h2 id="背景">1.1 背景</h2><ul><li><p>在线学习：对按序到达的数据进行学习，在每一步学习并更新未来数据的最佳预测值</p></li><li><p>遗憾/悔值 Regret &amp; 遗憾界：与离线学习最优损失之差的求和，用于评估在线学习的性能</p></li><li><p>概念漂移 Concept Shift：随着新数据的到来，数据整体的<mark>分布产生了变化</mark></p></li><li><p>指数加权平均 EWA：建模时间序列的一种方法，每个观测值权重是按照指数衰减的方式确定的，<mark>最近的观测值被赋予更高的权重</mark></p><ul><li>Slow Switch Phenomenon：对新到达的数据的Concept Shift会很慢适应</li></ul></li><li><p>指数梯度下降 EGD：借鉴EWA思想的用于在线学习的梯度下降方法，可以得到<mark>更小的遗憾界</mark></p></li><li><p>Cross-Time/Cross-Variable：假设变量独立，仅依赖时间/仅依赖变量（表：<mark>同时处理两种依赖性不能</mark>带来性能提升）</p><ul><li>变量独立性（Cross-Time）对于提高模型在概念漂移情况下的鲁棒性至关重要，Cross-Variable往往会过拟合；但缺乏变量间的信息在变量较少的数据集上效果不佳，如在ETT这些变量较少的数据集上，只考虑时间依赖效果不佳</li></ul><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/onenet表格.png"></p></li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://welts.xyz/2022/02/28/online/">在线学习简介</a></li><li><a href="https://blog.csdn.net/weixin_47692652/article/details/128641345">在线学习(online learning)——Chapter 1 What is online learning</a></li><li><a href="https://www.lamda.nju.edu.cn/mlt2023/Slides/chapter_8.pdf">遗憾界</a></li></ol><h2 id="onenet-结构">1.2 OneNet 结构</h2><p><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/onenet.png"></p><ul><li>f1：Cross-Time预测器</li><li>f2：Cross-Variable预测器</li></ul><h3 id="proposition-1online-convex-programming-boundocp">1.2.1 Proposition 1：Online Convex Programming Bound（OCP）</h3><ul><li>之前工作的问题：EGD在好的表现和缓慢适应新样本的速度中存在trade-off</li><li>本篇工作的改进：将长期信息和短期信息结合，OCP的作用是通过这些信息对两种预测器的权重进行调整</li></ul><figure><img src="/2024/09/19/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7III/ocp.png" alt="OCP的Regret Bound"><figcaption aria-hidden="true">OCP的Regret Bound</figcaption></figure><h3 id="proposition-2k-step-re-initialize-algorithm">1.2.2 Proposition 2：K-step re-initialize algorithm</h3><ul><li>使用EGD更新长时间信息对应的权重w；b对应短时间信息权重；利用强化学习框架RvS从w和短期内专家的表现中学习b</li></ul><h3 id="decoupled-strategies">1.2.3 Decoupled Strategies</h3><ul><li>Cross-Time预测器总比Cross-Variable表现好的时候，前者的权重会趋于1，损失函数对后者的梯度趋于0，从而不被训练。因此对于预测器和OCP的损失函数分别定义为<span class="math inline">\(\mathcal L(\widetilde{y}_1,y)+\mathcal L(\widetilde{y}_2,y)\)</span>和<span class="math inline">\(\mathcal L(w_1*\widetilde{y}_1+w_2*\widetilde{y}_2,y)\)</span></li></ul><h1 id="二">二、</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>微调</tag>
      
      <tag>增量学习</tag>
      
      <tag>在线学习</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>回溯</title>
    <link href="/2024/09/19/%E5%9B%9E%E6%BA%AF/"/>
    <url>/2024/09/19/%E5%9B%9E%E6%BA%AF/</url>
    
    <content type="html"><![CDATA[<ul><li><p>如果解决一个问题有多个步骤，每一个步骤有多种方法，题目又要我们找出所有的方法，可以使用回溯算法</p></li><li><p>回溯算法是在一棵树上的深度优先遍历</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">recur</span>(<span class="hljs-params">**kwargs</span>):<br>    <span class="hljs-keyword">if</span> condition:<br>        <span class="hljs-comment">#存放结果</span><br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-keyword">for</span> 每一个元素: <br>        <span class="hljs-comment">#处理节点</span><br>        recur(**kwargs)<br>        <span class="hljs-comment">#回溯，撤销处理结果</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>回溯</tag>
      
      <tag>搜索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN、GRU与LSTM</title>
    <link href="/2024/09/18/RNN%E3%80%81GRU%E4%B8%8ELSTM/"/>
    <url>/2024/09/18/RNN%E3%80%81GRU%E4%B8%8ELSTM/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer">]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>RNN</tag>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>二叉树</title>
    <link href="/2024/09/12/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <url>/2024/09/12/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>迭代简单理解就是可以通过循环来解决（for、while）， 而递归则是通过重复调用自身来解决问题；递归的时候隐式地维护了一个栈，而迭代的时候需要显式地将这个栈模拟出来</p><h1 id="二叉树的迭代遍历">1、二叉树的迭代遍历</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/binary-tree-preorder-traversal/">144. Binary Tree Preorder Traversal</a></p><p>前序遍历每次先处理中间节点，先将根节点放入栈中，然后将右孩子、左孩子依次加入，这样出栈的时候才是中左右的顺序。</p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/%E4%BA%8C%E5%8F%89%E6%A0%91%E5%89%8D%E5%BA%8F%E9%81%8D%E5%8E%86%EF%BC%88%E8%BF%AD%E4%BB%A3%E6%B3%95%EF%BC%89.gif"></p></li><li><p><a href="https://leetcode.cn/problems/binary-tree-inorder-traversal/">94. Binary Tree Inorder Traversal</a></p><p>但是，中序遍历、后序遍历的逻辑都不太一样，因为<mark>访问节点和处理节点的顺序不一致</mark></p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%BA%8F%E9%81%8D%E5%8E%86%EF%BC%88%E8%BF%AD%E4%BB%A3%E6%B3%95%EF%BC%89.gif"></p></li><li><p><a href="https://leetcode.cn/problems/binary-tree-postorder-traversal/">145. Binary Tree Postorder Traversal</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 后续遍历的迭代</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">postorderTraversal</span>(<span class="hljs-params">self, root: TreeNode</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>()<br>        <br>        res = <span class="hljs-built_in">list</span>()<br>        stack = <span class="hljs-built_in">list</span>()<br>        prev = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">while</span> root <span class="hljs-keyword">or</span> stack:<br>            <span class="hljs-keyword">while</span> root:<br>                stack.append(root)<br>                root = root.left<br>            root = stack.pop()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root.right <span class="hljs-keyword">or</span> root.right == prev:<br>                res.append(root.val)<br>                prev = root<br>                root = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">else</span>:<br>                stack.append(root)<br>                root = root.right<br>        <br>        <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure></li></ol><h1 id="二叉树的层序遍历bfs">2、二叉树的层序遍历（BFS）</h1><ul><li><strong>方法</strong>：利用BFS，以下为模板，<mark>可以运用在许多题中</mark></li></ul><p><a href="https://leetcode.cn/problems/binary-tree-level-order-traversal/">102. Binary Tree Level Order Traversal</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">levelOrder</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root :<span class="hljs-keyword">return</span> []<br>    stack=[root]<br>    res=[]<br>    <span class="hljs-keyword">while</span>(stack):<br>        queue_len=<span class="hljs-built_in">len</span>(stack)<br>        layer=[]<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(queue_len):<br>            temp=stack.pop(<span class="hljs-number">0</span>)<br>            layer.append(temp.val)<br>            <span class="hljs-keyword">if</span> temp.left:stack.append(temp.left)<br>            <span class="hljs-keyword">if</span> temp.right:stack.append(temp.right)<br>        res.append(layer)<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><h1 id="二叉树的深度与高度">3、二叉树的深度与高度</h1><ul><li>二叉树节点的深度：指从根节点到该节点的最长简单路径边的条数。</li><li>二叉树节点的高度（只能后序遍历）：指从该节点到叶子节点的最长简单路径边的条数。</li></ul><p><a href="https://leetcode.cn/problems/balanced-binary-tree/">110. Balanced Binary Tree</a></p><h1 id="bfs-dfs">4、BFS &amp; DFS</h1><ul><li>BFS需要靠队列实现，即迭代</li><li>DFS既可以用递归，也可以用迭代</li></ul><h1 id="前序中序后序遍历dfs">5、前序、中序、后序遍历（DFS）</h1><p><a href="https://leetcode.cn/problems/construct-binary-tree-from-inorder-and-postorder-traversal/">106. Construct Binary Tree from Inorder and Postorder Traversal</a></p><p>根据中序、后序递归构建二叉树时，注意到后序遍历结果是按<mark>左子树、右子树、根</mark>排列的，因此递归时先递归右子树，再递归左子树</p><h1 id="二叉搜索树-bst">6、二叉搜索树 BST</h1><ul><li>其左子树中的所有节点的值都小于等于该节点的值，而其右子树中的所有节点的值都大于或等于该节点的值</li><li>中序遍历二叉搜索树得到的关键码序列是一个递增序列</li></ul><p><a href="https://leetcode.cn/problems/validate-binary-search-tree/">98. Validate Binary Search Tree</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.pre=-<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isValidBST</span>(<span class="hljs-params">self, root: <span class="hljs-type">Optional</span>[TreeNode]</span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root:<span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.isValidBST(root.left):<span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> root.val&lt;=<span class="hljs-variable language_">self</span>.pre:<span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-variable language_">self</span>.pre=root.val<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.isValidBST(root.right)<br></code></pre></td></tr></table></figure><h1 id="二叉树最近公共祖先">7、二叉树最近公共祖先</h1><p><a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/">236. Lowest Common Ancestor of a Binary Tree</a></p><p>可以参考这种解法，非常简洁：<a href="https://leetcode.cn/problems/lowest-common-ancestor-of-a-binary-tree/solutions/240096/236-er-cha-shu-de-zui-jin-gong-gong-zu-xian-hou-xu/">二叉树的最近公共祖先（DFS ，清晰图解）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lowestCommonAncestor</span>(<span class="hljs-params">self, root: <span class="hljs-string">&#x27;TreeNode&#x27;</span>, p: <span class="hljs-string">&#x27;TreeNode&#x27;</span>, q: <span class="hljs-string">&#x27;TreeNode&#x27;</span></span>) -&gt; <span class="hljs-string">&#x27;TreeNode&#x27;</span>:<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> root <span class="hljs-keyword">or</span> root==p <span class="hljs-keyword">or</span> root==q:<span class="hljs-keyword">return</span> root<br><br>        left=<span class="hljs-variable language_">self</span>.lowestCommonAncestor(root.left,p,q)<br>        right=<span class="hljs-variable language_">self</span>.lowestCommonAncestor(root.right,p,q)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> left:<span class="hljs-keyword">return</span> right<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> right:<span class="hljs-keyword">return</span> left<br>                <br>        <br>        <span class="hljs-keyword">return</span> root<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>二叉树</tag>
      
      <tag>递归</tag>
      
      <tag>迭代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>栈、队列</title>
    <link href="/2024/09/11/%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97/"/>
    <url>/2024/09/11/%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="用队列实现栈">1、用队列实现栈</h1><p><a href="https://leetcode.cn/problems/implement-stack-using-queues/">225. Implement Stack using Queues</a></p><figure><img src="https://assets.leetcode-cn.com/solution-static/225/225_fig1.gif" alt="当push的时候，将栈中元素接到备用栈，并互换身份"><figcaption aria-hidden="true">当push的时候，将栈中元素接到备用栈，并互换身份</figcaption></figure><p>要注意，第二个数组只是辅助数组，所以有<code>array1,array2=array2,[]</code></p><h1 id="其他">2、其他</h1><p><a href="https://leetcode.cn/problems/sliding-window-maximum/">239. Sliding Window Maximum</a></p><p>​ 数组、区间内的最大（小）值，可以考虑队列/栈</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>栈</tag>
      
      <tag>队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>双均值策略</title>
    <link href="/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/"/>
    <url>/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[<p>在聚宽上编写双均值策略，对三支股票从2024年初到现在进行交易，最终结果亏了不到5%，入门还算可以</p><p><img src="/2024/09/11/%E5%8F%8C%E5%9D%87%E5%80%BC%E7%AD%96%E7%95%A5/结果.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入函数库</span><br><span class="hljs-keyword">from</span> jqdata <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 初始化函数，设定基准等等</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize</span>(<span class="hljs-params">context</span>):<br>    <span class="hljs-comment"># 设定沪深300作为基准</span><br>    set_benchmark(<span class="hljs-string">&#x27;000300.XSHG&#x27;</span>)<br>    <span class="hljs-comment"># 开启动态复权模式(真实价格)</span><br>    set_option(<span class="hljs-string">&#x27;use_real_price&#x27;</span>, <span class="hljs-literal">True</span>)<br>    g.security=[<span class="hljs-string">&#x27;002686.XSHE&#x27;</span>,<span class="hljs-string">&#x27;600771.XSHG&#x27;</span>,<span class="hljs-string">&#x27;601216.XSHG&#x27;</span>]<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">handle_data</span>(<span class="hljs-params">context, data</span>):<br>    <span class="hljs-keyword">for</span> security <span class="hljs-keyword">in</span> g.security:<br>        close_data = attribute_history(security, <span class="hljs-number">60</span>, <span class="hljs-string">&#x27;1d&#x27;</span>, [<span class="hljs-string">&#x27;close&#x27;</span>])<br> <br>        MA5 = close_data[<span class="hljs-string">&#x27;close&#x27;</span>][-<span class="hljs-number">5</span>:].mean()<br>        MA60 = close_data[<span class="hljs-string">&#x27;close&#x27;</span>].mean()<br>        <br>        current_price =close_data[<span class="hljs-string">&#x27;close&#x27;</span>][-<span class="hljs-number">1</span>]<br>        <br>        <span class="hljs-comment"># 取得当前的现金</span><br>        cash = context.portfolio.cash<br>        position=context.portfolio.positions<br>        <br>        <span class="hljs-comment"># 如果5日均价高出60天天平均价5%, 则现金0.2买入</span><br>        <span class="hljs-keyword">if</span> MA5 &gt; MA60*<span class="hljs-number">1.05</span>:<br>            <span class="hljs-keyword">if</span> security <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> position:order_value(security, cash*<span class="hljs-number">0.2</span>)<br>            <span class="hljs-keyword">elif</span> current_price &lt; position[security].avg_cost: order(security, cash*<span class="hljs-number">0.1</span>)<br>        <span class="hljs-comment"># 如果5日均价低于60天均价的90%, 则空仓卖出</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> security <span class="hljs-keyword">in</span> position <span class="hljs-keyword">and</span> MA5&lt;MA60*<span class="hljs-number">0.9</span>:order_target(security, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>投资</category>
      
    </categories>
    
    
    <tags>
      
      <tag>量化</tag>
      
      <tag>金融</tag>
      
      <tag>经济</tag>
      
      <tag>聚宽</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>字符串</title>
    <link href="/2024/09/10/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    <url>/2024/09/10/%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="kmp算法">1、KMP算法</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>字符串</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>参数高效微调</title>
    <link href="/2024/09/07/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/09/07/%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·II</title>
    <link href="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/"/>
    <url>/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;  margin-top: 20px;}</style><h1 id="时间线">〇、时间线</h1><p><strong>9月4日~9月6日</strong></p><ol type="1"><li>冻结Transformer参数，只利用MOMENT的分类器在Benchmark上进行训练与测试，在3个简单数据集上效果很好（见2.2）</li><li>利用t-SNE进行降维，没有得到有启发的信息；随后尝试调参（Batchsize、Lr）、解冻Transformer参数来观察<code>GrabMyo</code>上的效果</li><li>阅读综述及相关论文</li><li>改小训练集，重新训练<code>Grabmyo</code>，效果都不是太好</li></ol><p><strong>9月9日~9月13日</strong></p><ol type="1"><li>阅读EASE</li></ol><h1 id="一相关工作">一、相关工作</h1><ol type="1"><li><a href="https://arxiv.org/abs/2402.02713">Position: What Can Large Language Models Tell Us about Time Series Analysis</a></li><li><a href="https://arxiv.org/abs/2403.14735">Foundation Models for Time Series Analysis: A Tutorial and Survey</a></li><li><a href="https://arxiv.org/abs/2401.16386">综述：Continual Learning with Pre-Trained Models: A Survey</a></li><li><a href="https://arxiv.org/abs/2303.07338">SimpleCIL &amp; APER：Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need</a></li><li><a href="https://arxiv.org/abs/2403.12030">APER的升级：Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning</a></li><li><a href="https://arxiv.org/abs/2307.02251">高维投影增加可分性：RanPAC: Random Projections and Pre-trained Models for Continual Learning</a></li><li><a href="https://arxiv.org/abs/2303.05118">SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model</a></li></ol><h2 id="基于预训练模型的增量学习综述">1.3 基于预训练模型的增量学习：综述</h2><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/结果.png"></p><h2 id="simplecil-aper">1.4 SimpleCIL &amp; APER</h2><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER.png" alt="APER框架"><figcaption aria-hidden="true">APER框架</figcaption></figure><h3 id="表示">1.4.1 表示</h3><ul><li>训练任务：<span class="math inline">\(\{\mathcal D^1,\mathcal D^2,\cdots,\mathcal D^b\}\)</span></li><li>Prototype：<span class="math inline">\({\rm p_i}=\frac{1}{K}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)\phi(x_j)，K=\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)，\mathbb I(\cdot)\)</span>为示性函数</li><li>分类器：<span class="math inline">\(f(x)=W^T\phi(x)，\phi(\cdot)\)</span>为embedding函数，<span class="math inline">\(W\in\mathbb R^{d\times |y_b|}\)</span>是分类头</li></ul><h3 id="算法">1.4.2 算法</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER算法.png"></p><ol type="1"><li><p>PTM添加parameter-efficient module后在<span class="math inline">\(\mathcal D^1\)</span>上微调得到AdaPTM</p><ul><li><p>按序微调模型会导致灾难性遗忘</p><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/Adapt.png" alt="微调前T个任务的结果"><figcaption aria-hidden="true">微调前T个任务的结果</figcaption></figure></li><li><p>因为使用prototype-based分类器，因此多次微调会导致前后prototype的特征不兼容</p><blockquote><p>Since we utilize a prototype-based classifier, tuning the model with multiple stages will result in incompatible features between former and new prototypes.</p></blockquote></li></ul></li><li><p>将PTM和AdaPTM的embedding层冻结（只训练分了里头和AdaModule里的参数）</p></li><li><p>对于每一个任务，计算其数据的Prototype并替代分类头的权重：<span class="math inline">\({\rm p_i}=\frac{1}{K}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)[\phi^{*}(x_j),\phi(x_j)]\)</span></p></li><li><p>通过计算余弦相似度来获得分类结果</p></li></ol><h3 id="adapt方法aper框架图的中右部分">1.4.3 Adapt方法（APER框架图的中、右部分）</h3><ul><li>Visual Prompt Tuning（VPT）：适合于ViT<ul><li>VPT-Deep：在每一个注意力层加Prompt</li><li>VPT-Shallow：只在第一层加Prompt</li></ul></li><li>Scale &amp; Shift（SSF）：<span class="math inline">\(x_{output}=\gamma\otimes x_{input}+\beta\)</span>，从而将分布适应于新任务</li><li>Adapter：<span class="math inline">\({\rm MLP}(x_l)+{\rm RELU}(x_{l}W_{down})W_{up}\)</span></li><li>Batch Normalization Tuning：适合于CNN-based模型</li></ul><h3 id="实验结果">1.4.4 实验结果</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER消融.png"></p><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/APER结果.png"></p><h2 id="aper的升级ease">1.5 APER的升级：EASE</h2><h3 id="简介">1.5.1 简介</h3><ul><li>前人工作的缺陷：对于L2P而言，其中一个缺点是Prompt会随着更新而遗忘先前的知识</li><li>本篇工作的改进：“拓展子空间“将先前子空间（Subspace，可理解为某任务所映射到的空间）的Prototype映射到本任务的子空间，从而减少遗忘<ul><li>个人理解：SimpleCIL利用PTM作为编码器，从而获得Prototype向量，然而该编码器未必有很好的效果；本篇工作通过在Transformer中加入Adapter层实现参数高效微调，从而优化编码效果，得到更好的Prototype向量；最终分类的原理通SimpleCIL，都是对Prototype计算余弦相似度。</li></ul></li></ul><h3 id="模型">1.5.2 模型</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/EASE.png"></p><ul><li><p>定义任务：<span class="math inline">\(\{\mathcal D^1,\mathcal D^2,\cdots,\mathcal D^B\}\)</span>，共有B个任务，每个任务可能包含多个类别，不同任务中的类别不同</p></li><li><p>Adapter层：加在PTM内Transformer的MLP旁，记为<span class="math inline">\(\mathcal A\)</span>，每一个任务都初始化一个Adapter层，即一个<mark>子空间</mark>（Subspace）。具体形式：<span class="math inline">\(x_o=\sigma(x_iW_{down})W_{up}+{\rm MLP}(X_i)，\sigma(\cdot)\)</span>为非线性激活函数，将使用Adapter层的embedding函数记为<span class="math inline">\(\phi(x;\mathcal A)\)</span></p></li><li><p>定义子空间<span class="math inline">\(b\)</span>（也可以说任务b对应的子空间）中第<span class="math inline">\(i\)</span>类的Prototype：<span class="math inline">\(p_{i,b}=\frac{1}{N}\sum\limits_{j=1}^{|\mathcal D^b|}\mathbb I(y_j=i)\phi(x_j;\mathcal A_b)\)</span>，N为第i类的样本数量</p></li><li><p>定义某样本的embedding：<span class="math inline">\(\Phi(x)=[\phi (x;\mathcal A_1),\cdots,\phi (x;\mathcal A_b)]\in \mathbb R^{bd}\)</span>，即样本<span class="math inline">\(x\)</span>在所有子空间的embedding拼接</p></li><li><p>定义类别<span class="math inline">\(i\)</span>的分类器：<span class="math inline">\(\mathcal P_i=[p_{i,1},p_{i,2},\cdots,p_{i,b}]\in \mathbb R^{bd}\)</span>，即类别<span class="math inline">\(i\)</span>在所有子空间的Prototype向量的拼接</p></li><li><p>定义任务b的Prototype：<span class="math inline">\({\rm P}_{1,1}={\rm Concat}[p_{1,1};\cdots p_{|y_1|,1}] \in \mathbb R^{|y_1|\times d}\)</span>，下标中的前一个‘1’代表第几个任务，后一个‘1’代表第几个子空间；即第一个任务在第一个子空间的Prototype可以用第一个任务中所有样本在第一个子空间的Prototype拼接来表示</p><ul><li>同理我们可以得到<span class="math inline">\({\rm P}_{2,1}={\rm Concat}[p_{2,1};\cdots p_{|y_2|,1}] \in \mathbb R^{|y_2|\times d}\)</span>和<span class="math inline">\({\rm P}_{2,2}={\rm Concat}[p_{2,2};\cdots p_{|y_2|,2}] \in \mathbb R^{|y_2|\times d}\)</span>。但是我们不能<mark>直接</mark>得到第一个任务在第二个子空间的Prototype，作者认为是Prototype和embedding间维度的不一致。（这个我没想明白，我觉得一个合理的原因是没有保留第一任务的样本，从而无法计算）</li><li>因此，作者提出了Semantic Guided Prototype Complement，即基于<span class="math inline">\({\rm P}_{o,o},{\rm P}_{n,o},{\rm P}_{n,n}\)</span>估计<span class="math inline">\({\rm P}_{o,n}\)</span>，其中前两个向量被称作共现空间（co-occurence space）,前一个角标的<span class="math inline">\(o,n\)</span>分别代表<mark>旧类别、新类别</mark>，后一个代表旧子空间、新子空间</li></ul></li><li><p>定义相似度（可以类比为两向量的点积）：   $$  {\rm Sim}_{i,j}=\frac{ {\rm P}_{o,o}[i]}{ {\Vert {\rm P}_{o,o} \Vert}_2}\frac{ {\rm P}_{n,o}[j]}{  {\Vert {\rm P}_{n,o} \Vert}_2}  $$  </p><ul><li><p>进一步归一化，可以理解为<span class="math inline">\(i\)</span>对<span class="math inline">\(j\)</span>的相似度（<mark>原文的分母下标用的是<span class="math inline">\(j\)</span>，这里改成了<span class="math inline">\(k\)</span></mark>）：   $$  {\rm Sim}_{i,j}=\frac{e^{{\rm Sim}_{i,j}}}{\sum \limits_k e^{{\rm Sim}_{i,k}} }  $$  ​</p></li><li><p>最终有（可以理解为<span class="math inline">\({\rm P}_{o,o}[i]\)</span>和<span class="math inline">\({\rm P}_{n,o}[j]\)</span>之间的关系在<span class="math inline">\({\rm P}_{o,n}[i]\)</span>和<span class="math inline">\({\rm P}_{n,n}[j]\)</span>间也适用：</p>  $$  {\rm P}_{o,n}[i]=\sum \limits_j {\rm  Sim}_{i,j}\times {\rm P}_{n,n}[j]  $$  ​</li><li><p>补全矩阵有：</p>  $$  \begin{bmatrix}      \rm P_{1,1}& \rm P_{1,2}& \cdots  & \rm P_{1,B} \\      \rm P_{2,1}& \rm P_{2,2}& \cdots  & \rm P_{2,B} \\      \vdots & \vdots & \ddots & \vdots \\      \rm P_{B,1}& \rm P_{B,2}& \cdots  & \rm P_{B,B}    \end{bmatrix}  $$  ​</li></ul></li><li><p>计算任务b中输入<span class="math inline">\(x\)</span>在第<span class="math inline">\(i\)</span>个Adapter层的embedding与任务b在子空间<span class="math inline">\(i\)</span>的Prototype的乘积：</p>  $$  \left[\mathbf{P}_{b, 1}, \mathbf{P}_{b, 2}, \cdots, \mathbf{P}_{b, B}\right]^{\top} \Phi(\mathbf{x})=\sum_{i} \mathbf{P}_{b, i}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{i}\right)  $$  <ul><li><p>但因为任务b最主要学习的特征存放在第<span class="math inline">\(b\)</span>个Adapter层，因此进行加权：</p>  $$  \mathbf{P}_{b, b}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{b}\right)+\alpha \sum_{i \neq b} \mathbf{P}_{b, i}^{\top} \phi\left(\mathbf{x} ; \mathcal{A}_{i}\right)  $$​      </li></ul></li></ul><h2 id="高维投影增加可分性ranpac">1.6 高维投影增加可分性：RanPAC</h2><h3 id="简介-1">1.6.1 简介</h3><ul><li><p>前人工作的缺陷：对于NCM（即利用CP）而言，类与类之间存在一定的相关性，的相关系数会很高</p><figure><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/缺陷.png" alt="左图为CP和feature vector的相似度，右图为皮尔逊相关系数"><figcaption aria-hidden="true">左图为CP和feature vector的相似度，右图为皮尔逊相关系数</figcaption></figure></li><li><p>本篇工作的改进：基于随机投影矩阵和Gram Matrix增加类与类之间的可分性</p><ul><li>高斯分布假设数据具有各向同性，而直接从预训练模型提取的特征可能不符合高斯分布</li><li>随着随机投影的维度增加，输出更接近高斯分布，并且向量之间的内积会更大</li><li>借助Gram实现类似LDA的算法，增加类与类之间的可分性</li></ul></li><li><p>本篇工作的问题：与传统方法比，随机投影矩阵参数开销很大；公式的运用需要强大的预训练模型编码器</p></li></ul><h3 id="算法-1">1.6.2 算法</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/RanPAC.png"></p><ol type="1"><li><p>基于首个任务对PTM进行微调（参考了APER的做法）：首个任务的数据比预训练数据更好的拟合了下游任务。然而并没有使用PTM与AdaPTM的表示进行拼接，因为这对于随机投影而言效果不好</p><ul><li><strong>AdaptFormer</strong>：在预训练模型中添加少量的可训练参数</li><li><strong>SSF (Scaling and Shifting Features)</strong>：通过在特征提取过程中添加缩放和平移操作来调整特征表示</li><li><strong>VPT (Visual Prompt Tuning)</strong>：通过在模型输入中引入Prompts，并更新Prompt</li></ul></li><li><p>预测结果Basic（可以类比计算相似度），<span class="math inline">\(s_y\)</span>可以理解为预测的类别（为什么是<span class="math inline">\(\mathbf{G}^{-1}\)</span>，参考Appendix B） <span class="math display">\[y_{\text {test }}=\underset{y^{\prime} \in\{1, \ldots, K\}}{\rm argmax }s_{y^{\prime}}, \quad s_{y}:=\mathbf{f}_{\text {test }}^{\top} \mathbf{G}^{-1} \mathbf{c}_{y}\]</span></p><ul><li><p>K为T个任务后的最大类别数量</p></li><li><p>G为Gram矩阵，由空间内向量相互之间的内积组成，可以看作是没有减去均值的协方差矩阵： <span class="math display">\[\begin{pmatrix}    (a_{1},a_{1}) &amp; \cdots &amp; (a_{1},a_{n}) \\    \vdots &amp; \ddots &amp; \vdots \\    (a_{n},a_{1}) &amp; \cdots &amp; (a_{n},a_{n})  \end{pmatrix} \]</span></p></li><li><p><span class="math inline">\(c_y\)</span>为CPs with averaging dropped</p></li></ul></li><li><p>改进</p><ul><li><p>随机投影矩阵，符合标准正态分布：<span class="math inline">\(W \in \mathbb R^{L×M}\)</span></p></li><li><p>非线性激活函数：<span class="math inline">\(\phi(\cdot)\)</span></p></li><li><p>预训练模型对于任务<span class="math inline">\(\mathcal D_t\)</span>中的第<span class="math inline">\(n\)</span>个样本提取特征：<span class="math inline">\(f_{t,n} \in \mathbb R^L\)</span></p></li><li><p>定义特征经高维投影并非线性激活后的输出向量（可以理解为特征的<mark>高维投影</mark>）：<span class="math inline">\({\rm h}_{t,n}= \phi (f_{t,n}^{\rm T}W)\in \mathbb R^{M}\)</span></p></li><li><p>定义矩阵（由高维投影组成）：<span class="math inline">\({\rm H}\in \mathbb R^{M\times N}，N\)</span>代表样本数量，由<span class="math inline">\({\rm h}_{t,n}\)</span>组成</p></li><li><p>Gram矩阵：<span class="math inline">\(\rm G=HH^T\)</span>，因此这是个<mark>满秩矩阵</mark>，也可以写为<span class="math inline">\(\mathbf{G}=\sum \limits_{t=1}^{T} \sum \limits_{n=1}^{N_{t}} \mathbf{h}_{t, n} \otimes \mathbf{h}_{t, n}\)</span></p></li><li><p><span class="math inline">\(c_y\)</span>矩阵，即由所有Prototype组成：<span class="math inline">\(\mathbf{C}=\sum \limits_{t=1}^{T} \sum \limits_{n=1}^{N_{t}} \mathbf{h}_{t, n} \otimes \mathbf{y}_{t, n}\)</span></p></li><li><p>预测结果Revised，其中<span class="math inline">\({\rm \mathbf{I}}\)</span>为单位矩阵，该项<span class="math inline">\(\lambda{\rm \mathbf{I}}\)</span>借助岭回归中正则化的思想，用来防止过拟合： <span class="math display">\[s_y = \phi({\rm f_{test}}^{\rm T}W)(\mathbf{G} + λ\mathbf{I})^{−1}c_y\]</span></p></li></ul></li></ol><ul><li>记公式的后半部分为<span class="math inline">\({\rm W_o}=(\mathbf{G} + λ\mathbf{I})^{−1}{\rm \mathbf{C}}\)</span>，则<span class="math inline">\({\rm y_{pred} = h_{test}W_o}\)</span></li></ul><h3 id="结果">1.6.3 结果</h3><p><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/RanPAC结果.png"></p><h1 id="二moment">二、MOMENT</h1><h2 id="测试代码">2.1 测试代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/&quot;</span>)<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/TSCIL/&quot;</span>)<br>sys.path.append(<span class="hljs-string">&quot;/home/eee/zym/TSCIL/utils/&quot;</span>)<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <br><span class="hljs-keyword">from</span> momentfm <span class="hljs-keyword">import</span> MOMENTPipeline<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> lr_scheduler<br><span class="hljs-keyword">from</span> EarlyStop <span class="hljs-keyword">import</span> EarlyStopping<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader,Dataset<br><span class="hljs-keyword">from</span> stream <span class="hljs-keyword">import</span> IncrementalTaskStream,get_cls_order<br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>DATA=<span class="hljs-string">&quot;wisdm&quot;</span><br><span class="hljs-comment"># [&#x27;har&#x27;, &#x27;uwave&#x27;, &#x27;dailysports&#x27;, &#x27;grabmyo&#x27;, &#x27;wisdm&#x27;,&#x27;ninapro&#x27;, &#x27;sines&#x27;]</span><br><span class="hljs-comment"># ============================初始化MOMENT============================</span><br>model = MOMENTPipeline.from_pretrained(<br>    <span class="hljs-string">&quot;AutonLab/MOMENT-1-large&quot;</span>, <br>    model_kwargs=&#123;<br>        <span class="hljs-string">&#x27;task_name&#x27;</span>: <span class="hljs-string">&#x27;classification&#x27;</span>,<br>        <span class="hljs-string">&#x27;n_channels&#x27;</span>: <span class="hljs-number">3</span>,<br>        <span class="hljs-string">&#x27;num_class&#x27;</span>: <span class="hljs-number">18</span>,<br>        <span class="hljs-string">&#x27;freeze_encoder&#x27;</span>: <span class="hljs-literal">True</span>, <span class="hljs-comment"># Freeze the patch embedding layer</span><br>        <span class="hljs-string">&#x27;freeze_embedder&#x27;</span>: <span class="hljs-literal">True</span>, <span class="hljs-comment"># Freeze the transformer encoder</span><br>        <span class="hljs-string">&#x27;freeze_head&#x27;</span>: <span class="hljs-literal">False</span>, <span class="hljs-comment"># The linear forecasting head must be trained</span><br>    &#125;, <br>   )<br>model.init()<br>model.to(device)<br><br><span class="hljs-comment"># ============================获取数据集============================</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data, targets</span>):<br>        <span class="hljs-variable language_">self</span>.data = data<br>        <span class="hljs-variable language_">self</span>.targets = targets<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data[idx], <span class="hljs-variable language_">self</span>.targets[idx]<br><br>cls_order = get_cls_order(DATA, <span class="hljs-literal">False</span>)<br>task_stream = IncrementalTaskStream(data=DATA, scenario=<span class="hljs-string">&#x27;class&#x27;</span>, cls_order=cls_order,split=<span class="hljs-string">&#x27;all&#x27;</span>)<br>(x_train, y_train), (x_val, y_val), (x_test, y_test) = task_stream.setup_offline()<br>x_train,x_val,x_test=np.transpose(x_train,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)),np.transpose(x_val,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)),np.transpose(x_test,(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>))<br><br>train_dataloader = DataLoader(MyDataset(x_train,y_train), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br>val_dataloader = DataLoader(MyDataset(x_val,y_val), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(MyDataset(x_test,y_test), batch_size=<span class="hljs-number">64</span>, num_workers=<span class="hljs-number">4</span>,pin_memory=<span class="hljs-literal">True</span>,shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># ============================超参数设置============================</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br>scheduler=lr_scheduler.StepLR(optimizer, step_size=<span class="hljs-number">1</span>, gamma=<span class="hljs-number">0.95</span>)<br><br><br>EPOCH = <span class="hljs-number">100</span><br>patience=<span class="hljs-number">20</span><br>Acc,Loss,ValidLoss=[],[],[]<br><br><span class="hljs-comment"># ============================画图函数============================</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">Draw</span>(<span class="hljs-params">X,Y,Z</span>):<br>    plt.plot(X,Y,linestyle=<span class="hljs-string">&#x27;--&#x27;</span>,label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>    plt.plot(X,Z,label=<span class="hljs-string">&quot;acc&quot;</span>)<br>    plt.legend()<br>    plt.savefig(<span class="hljs-string">f&#x27;./<span class="hljs-subst">&#123;DATA&#125;</span>_res.png&#x27;</span>) <br><br><br><br><span class="hljs-comment"># ===============================定义测试函数=========================</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">epoch,mode</span>):<br>    <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;test&#x27;</span>:dataloader=test_dataloader<br>    <span class="hljs-keyword">elif</span> mode==<span class="hljs-string">&quot;val&quot;</span>:dataloader=val_dataloader<br><br>    model.<span class="hljs-built_in">eval</span>()<br>    correct,total,Validloss=<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data,label <span class="hljs-keyword">in</span> dataloader:<br>            output = model(data.to(torch.<span class="hljs-built_in">float</span>).to(device))<br>            Validloss+= criterion(output.logits, label.to(device))<br>            _, predicted = torch.<span class="hljs-built_in">max</span>(output.logits, dim=<span class="hljs-number">1</span>) <br>            total += label.size(<span class="hljs-number">0</span>)  <span class="hljs-comment"># 张量之间的比较运算</span><br>            correct += (predicted == label.to(device)).<span class="hljs-built_in">sum</span>().item()<br>        acc = correct / total<br>        ValidLoss.append(Validloss/<span class="hljs-built_in">len</span>(dataloader))<br>        <span class="hljs-keyword">if</span> mode==<span class="hljs-string">&#x27;val&#x27;</span>:<br>            Acc.append(acc)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[%d / %d]: 验证集准确率: %.1f %% &#x27;</span> % (epoch+<span class="hljs-number">1</span>, EPOCH, <span class="hljs-number">100</span> * acc)) <br>        <span class="hljs-keyword">else</span>:<span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;测试集准确率: %.1f %% &#x27;</span> % (<span class="hljs-number">100</span> * acc))<br>    <br><br><br><span class="hljs-comment"># ===============================训练=========================</span><br>early_stopping = EarlyStopping(patience=patience, verbose=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCH):<br>    model.train()<br>    EpochLoss=<span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data,label <span class="hljs-keyword">in</span> train_dataloader:<br><br>        <span class="hljs-comment"># forward [batch_size, n_channels, forecast_horizon]</span><br>        output = model(data.to(torch.<span class="hljs-built_in">float</span>).to(device))<br><br>        <span class="hljs-comment"># backward</span><br>        loss = criterion(output.logits, label.to(device))<br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br>        <br>        EpochLoss+=loss.item()<br>    Loss.append(EpochLoss/<span class="hljs-built_in">len</span>(train_dataloader))<br>    scheduler.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;学习率:<span class="hljs-subst">&#123;scheduler.get_last_lr()[<span class="hljs-number">0</span>]:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>    test(_,<span class="hljs-string">&#x27;val&#x27;</span>)<br><br>    early_stopping(ValidLoss[-<span class="hljs-number">1</span>], model)<br>    <span class="hljs-keyword">if</span> early_stopping.early_stop:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Early stopping&quot;</span>)<br>        <span class="hljs-keyword">break</span><br><br>test(<span class="hljs-literal">None</span>,<span class="hljs-string">&#x27;test&#x27;</span>)<br>Draw([_+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Loss))],Loss,Acc)<br>torch.save(model,<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;DATA&#125;</span>_ckpt.pt&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="测试结果">2.2 测试结果</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/uwave.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/har.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/sports.png"></div></div><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/grabmyo.png"></div><div class="group-image-wrap"><img src="/2024/09/04/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7II/wisdm.png"></div></div></div><h2 id="针对grabmyo的调参">2.3 针对<code>GrabMyo</code>的调参</h2><div class="center"><table><thead><tr class="header"><th style="text-align: center;">数据处理/训练部分</th><th style="text-align: center;">Batchsize</th><th style="text-align: center;">Lr（gamma）</th><th style="text-align: center;">Acc（%）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">concat/cls</td><td style="text-align: center;">512</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">11.6</td></tr><tr class="even"><td style="text-align: center;">concat/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-3（0.95）</td><td style="text-align: center;">12.1</td></tr><tr class="odd"><td style="text-align: center;">mean/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">/</td></tr><tr class="even"><td style="text-align: center;">mean/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">/</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-2（0.95）</td><td style="text-align: center;">6.2</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-3（0.95）</td><td style="text-align: center;">6.3</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/all</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-4（0.95）</td><td style="text-align: center;">10.0</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-4（0.95）</td><td style="text-align: center;">10.8</td></tr><tr class="odd"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">1e-4（0.95）</td><td style="text-align: center;">11.2</td></tr><tr class="even"><td style="text-align: center;">concat（20%）/cls</td><td style="text-align: center;">64</td><td style="text-align: center;">3e-5（0.95）</td><td style="text-align: center;">11.2</td></tr></tbody></table></div>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哈希表</title>
    <link href="/2024/09/04/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <url>/2024/09/04/%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>当需要查询一个元素是否出现过、一个元素是否在集合里、元素出现次数的时候，使用哈希表</p><ol type="1"><li><a href="https://leetcode.cn/problems/4sum-ii/">454. 4Sum II</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>哈希表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型架构、KV Cache、Scaling Law与涌现</title>
    <link href="/2024/09/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"/>
    <url>/2024/09/02/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="auto-encoder-vs-auto-regressive">1、Auto-encoder vs Auto-regressive</h1><p>从预训练模型的角度上来看，两者的模型结构可能是一样的，不同的是预训练的方式</p><ul><li>自编码模型：预训练时可以看到上下文信息，指BERT等；广义来讲是自编码器（AE），包含VAE</li><li>自回归模型：预训练时只能看到上文或者下文，指GPT、ELMO等；广义来讲就是利用过去观测值预测未来值的模型，也包含RNN、LSTM</li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://blog.csdn.net/weixin_43301333/article/details/128141716?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522170848482516800222831072%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&amp;request_id=170848482516800222831072&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-128141716-null-null.nonecase&amp;utm_term=encoder&amp;spm=1018.2226.3001.4450">Transformer Encoder-Decoer 结构回顾</a></li><li><a href="https://www.cnblogs.com/AudreyXu/p/17197943.html">自回归和自编码有什么区别？</a></li></ol><h1 id="大模型架构">2、大模型架构</h1><p><img src="https://i-blog.csdnimg.cn/blog_migrate/8aba45c21032e2d07fd783070ad8483a.png"></p><ul><li>Encoder-Decoder（T5）</li><li>Decoder Only（GPT）：在各种下游任务上zero-shot泛化性能最好，few-shot（上下文学习）泛化能力更强<ul><li>预训练任务：Next Token Prediction（任务更难，在数据足够多时，模型学习通用表征的上限更高）</li></ul></li><li>Encoder Only（BERT）：预训练任务只包含分类和预测，不擅长生成任务<ul><li>预训练任务：MLM（预测mask），NSP（句子二分类）</li></ul></li><li>Prefix LM（GLM）</li></ul><p><strong>参考</strong></p><ol type="1"><li><a href="https://www.zhihu.com/question/588325646/answer/2940298964">为什么现在的LLM都是Decoder only的架构？ - 苏剑林的回答 - 知乎</a></li><li><a href="https://www.zhihu.com/question/588325646/answer/3357252612">为什么现在的LLM都是Decoder only的架构？ - Sam多吃青菜的回答 - 知乎</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>编码器</tag>
      
      <tag>解码器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch加速训练MNIST</title>
    <link href="/2024/08/29/Pytorch%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83/"/>
    <url>/2024/08/29/Pytorch%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<style>.center {  width: auto;  display: table;  margin-left: auto;  margin-right: auto;  margin-top: 20px;}</style><h1 id="实验过程">0、实验过程</h1><p>为了对比Pytorch中不同方法对于训练速度的提升，采用最基础的<a href="https://blog.csdn.net/qq_45588019/article/details/120935828">MNIST数字识别</a>，基础配置如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">64</span><br>learning_rate = <span class="hljs-number">0.01</span><br>momentum = <span class="hljs-number">0.5</span><br>EPOCH = <span class="hljs-number">5</span><br><br>criterion = torch.nn.CrossEntropyLoss()  <br>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)  <br></code></pre></td></tr></table></figure><p>源码是CPU上跑的，改到GPU上作为一个Baseline：</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">方法</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">CPU</td><td style="text-align: center;">97.9</td><td style="text-align: center;">25.16</td></tr><tr class="even"><td style="text-align: center;">GPU（RTX3080）</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr></tbody></table></div><h1 id="dataloader">1、Dataloader</h1><p>看到<a href="https://blog.csdn.net/qq_28057379/article/details/115427052">网上</a>有说将<code>num_workers</code>设置成CPU数量一样，但也有的说并非越大越好，可能需要进行实验选择最佳的。在这里测试集准确率参考意义并不大，因为差距很小，并且dataloader不会对准确率造成显著的影响，可能与参数的初始化情况有关</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr><tr class="even"><td style="text-align: center;"><code>num_workers=4</code></td><td style="text-align: center;">98.3</td><td style="text-align: center;">7.54</td></tr><tr class="odd"><td style="text-align: center;"><code>pin_memory=True</code></td><td style="text-align: center;">98.7</td><td style="text-align: center;">21.32</td></tr><tr class="even"><td style="text-align: center;"><code>num_workers=4，pin_memory=True</code></td><td style="text-align: center;">98.4</td><td style="text-align: center;"><strong>6.14</strong></td></tr></tbody></table></div><h1 id="自动混合精度-amp">2、自动混合精度 AMP</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">scaler = GradScaler(enabled=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> batch_idx, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader, <span class="hljs-number">0</span>):<br>        inputs, target = data<br>        optimizer.zero_grad()<br><br>        <span class="hljs-keyword">with</span> autocast(enabled=<span class="hljs-literal">True</span>, dtype=torch.float16):<br>            outputs = model(inputs.to(device))<br>            loss = criterion(outputs, target.to(device))<br><br>            scaler.scale(loss).backward()<br>            scaler.step(optimizer)<br>            scaler.update()   <br></code></pre></td></tr></table></figure><p>在这个实验中AMP并未提速，反而还慢了，猜想是因为数据集规模太小。不过在之前的比赛中，在对大模型推理时使用混合精度确实有减少推理时间</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;"><strong>20.40</strong></td></tr><tr class="even"><td style="text-align: center;">AMP</td><td style="text-align: center;">98.5</td><td style="text-align: center;">21.83</td></tr></tbody></table></div><h1 id="优化器">3、优化器</h1><p>除了Baseline中设置了动量参数外，其余皆采用默认参数</p><div class="center"><table><thead><tr class="header"><th style="text-align: center;">参数</th><th style="text-align: center;">测试集准确率（%）</th><th style="text-align: center;">用时（s）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Baseline</td><td style="text-align: center;">98.4</td><td style="text-align: center;">20.40</td></tr><tr class="even"><td style="text-align: center;">Adam</td><td style="text-align: center;">95.2</td><td style="text-align: center;">20.43</td></tr><tr class="odd"><td style="text-align: center;">AdamW</td><td style="text-align: center;">97.5</td><td style="text-align: center;">20.62</td></tr><tr class="even"><td style="text-align: center;">RMSprop</td><td style="text-align: center;">97.2</td><td style="text-align: center;"><strong>20.04</strong></td></tr><tr class="odd"><td style="text-align: center;">Adagrad</td><td style="text-align: center;"><strong>98.8</strong></td><td style="text-align: center;">20.31</td></tr></tbody></table></div><h1 id="归一化">4、归一化</h1><h1 id="学习率">5、学习率</h1><h1 id="batchsize">6、BatchSize</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数组</title>
    <link href="/2024/08/28/%E6%95%B0%E7%BB%84/"/>
    <url>/2024/08/28/%E6%95%B0%E7%BB%84/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="双指针">1、双指针</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/squares-of-a-sorted-array/">977. Squares of a Sorted Array</a></p><p><img src="https://code-thinking.cdn.bcebos.com/gifs/977.%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E5%B9%B3%E6%96%B9.gif"></p></li></ol><h1 id="滑动窗口">2、滑动窗口</h1><ol type="1"><li><a href="https://leetcode.cn/problems/minimum-size-subarray-sum/">209. Minimum Size Subarray Sum</a></li></ol><h1 id="模拟行为">3、模拟行为</h1><ol type="1"><li><a href="https://leetcode.cn/problems/spiral-matrix-ii/">59. Spiral Matrix II：模拟顺时针画矩阵的过程</a></li></ol><h1 id="前缀和">4、前缀和</h1><p>重复利用计算过的子数组之和，从而降低区间查询需要累加计算的次数。在涉及计算区间和的问题时非常有用</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>数组</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>归一化</title>
    <link href="/2024/08/27/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>/2024/08/27/%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="batchnorm">1、BatchNorm</h1><h1 id="layernorm">2、LayerNorm</h1><p>针对一个样本所有特征计算均值和方差，然后对样本进行归一化</p><p>减少内部协变量偏移（Internal Covariate Shift）： 内部协变量偏移是指在深度神经网络的训练过程中，每一层输入的分布会发生变化，导致网络的训练变得困难。层归一化通过对每一层的输入进行归一化处理，可以减少内部协变量偏移，使得每一层的输入分布更加稳定。 <strong>稳定梯度</strong>： 层归一化有助于保持每一层输出的均值和方差稳定，从而使得梯度的传播更加稳定。这有助于减少梯度消失或梯度爆炸的问题，提高梯度在网络中的流动性，加快训练速度。<sup><a href="#ref1">[1]</a></sup></p><h1 id="instancenorm">3、InstanceNorm</h1><h1 id="rmsnorm">4、RMSNorm</h1><h1 id="参考">参考</h1><ol type="1"><li><a name="ref1"></a> <a href="https://www.cnblogs.com/shine-lee/p/11989612.html">Batch Normalization详解</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>归一化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>激活函数、损失函数与评价指标</title>
    <link href="/2024/08/26/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
    <url>/2024/08/26/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="激活函数">1、激活函数</h1><ol type="1"><li><p><code>sigmoid</code>：<span class="math inline">\(\sigma(x)=\frac{1}{1+e^{-x}}\)</span></p><ul><li><p><span class="math inline">\(\rm max\{\sigma&#39;(x)\}=0.25&lt;1\)</span>，容易出现梯度消失</p></li><li><p>输出均值不为0，导致收敛变慢</p><p>​ <a href="https://cloud.tencent.com/developer/article/1829621">非零均值？激活函数也太硬核了</a></p><p>​ <a href="https://liam.page/2018/04/17/zero-centered-active-function/">谈谈激活函数以0为中心 的问题</a></p></li></ul></li><li><p><code>ReLU</code></p></li><li><p><code>Tanh</code>：<span class="math inline">\({\rm tanh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}\)</span></p><ul><li>容易出现梯度消失</li><li>zero-centered，比<code>sigmoid</code>收敛速度快</li></ul></li></ol><h1 id="损失函数">2、损失函数</h1><h1 id="评价指标">3、评价指标</h1><h2 id="混淆矩阵-confusion-matrix">3.1 混淆矩阵 Confusion Matrix</h2><p><img src="https://i-blog.csdnimg.cn/blog_migrate/d7a4b71d1f2393e35df6988fd990c2d9.jpeg"></p><ul><li><p>精确率 Accuracy：评估模型总体，当数据分布不均衡时失效 <span class="math display">\[{\rm Acc=\frac{TP+TF}{TP+TF+FT+FP}}\]</span></p></li><li><p>F1分数 F1-score：评估模型总体，适合数据分布不均衡时，是正确率和召回率的调和平均数 <span class="math display">\[{\rm F1=2\cdot\frac{Precision\times Recall}{Precision+Recall}}\]</span></p></li><li><p>准确率 Precision：模型识别为正类的样本中，真正为正类的样本有多少 <span class="math display">\[{\rm Precision=\frac{TP}{TP+FP}}\]</span></p></li><li><p>召回率 Recall/灵敏度 Sensitivity：正类的样本中，被识别出来的有多少 <span class="math display">\[{\rm Recall=\frac{TP}{TP+FN}}\]</span></p></li><li><p>特异度 Specificity： <span class="math display">\[{\rm Specificity=\frac{TN}{FP+TN}}\]</span></p></li></ul><h2 id="auc-roc">3.2 AUC-ROC</h2><h1 id="参考">4、参考</h1><p><a href="https://blog.csdn.net/seagal890/article/details/105059498">[机器学习笔记] 混淆矩阵（Confusion Matrix）</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>激活函数</tag>
      
      <tag>损失函数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer</title>
    <link href="/2024/08/24/Transformer/"/>
    <url>/2024/08/24/Transformer/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="结构">1、结构</h1><h1 id="特点">2、特点</h1><ol type="1"><li>支持并行计算，提高推理速度👍</li><li>捕捉长距离依赖关系👍</li><li>计算成本高👎</li><li>对序列长度敏感👎</li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面经</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>链表</title>
    <link href="/2024/08/24/%E9%93%BE%E8%A1%A8/"/>
    <url>/2024/08/24/%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="链表删除头结点">1、链表删除头结点</h1><ul><li>设置虚拟头结点</li><li>将原来链表的头指针向后移动</li></ul><ol type="1"><li><p><a href="https://leetcode.cn/problems/remove-linked-list-elements/">203. Remove Linked List Elements</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 虚拟头节点法</span><br><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeElements</span>(<span class="hljs-params">self, head: <span class="hljs-type">Optional</span>[ListNode], val: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">Optional</span>[ListNode]:<br>        <span class="hljs-comment"># 创建虚拟头部节点以简化删除过程</span><br>        dummy_head = ListNode(<span class="hljs-built_in">next</span> = head)<br>        <br>        <span class="hljs-comment"># 遍历列表并删除值为val的节点</span><br>        current = dummy_head<br>        <span class="hljs-keyword">while</span> current.<span class="hljs-built_in">next</span>:<br>            <span class="hljs-keyword">if</span> current.<span class="hljs-built_in">next</span>.val == val:<br>                current.<span class="hljs-built_in">next</span> = current.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">else</span>:<br>                current = current.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-keyword">return</span> dummy_head.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol><h1 id="反转链表">2、反转链表</h1><figure><img src="https://code-thinking.cdn.bcebos.com/gifs/206.%E7%BF%BB%E8%BD%AC%E9%93%BE%E8%A1%A8.gif" alt="反转链表"><figcaption aria-hidden="true">反转链表</figcaption></figure><ol type="1"><li><p><a href="https://leetcode.cn/problems/reverse-linked-list/">206. Reverse Linked List</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 双指针法</span><br><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverseList</span>(<span class="hljs-params">self, head: ListNode</span>) -&gt; ListNode:<br>        cur = head   <br>        pre = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">while</span> cur:<br>            temp = cur.<span class="hljs-built_in">next</span> <span class="hljs-comment"># 保存一下 cur的下一个节点，因为接下来要改变cur-&gt;next</span><br>            cur.<span class="hljs-built_in">next</span> = pre <span class="hljs-comment">#反转</span><br>            <span class="hljs-comment">#更新pre、cur指针</span><br>            pre = cur<br>            cur = temp<br>        <span class="hljs-keyword">return</span> pre<br></code></pre></td></tr></table></figure></li></ol><h1 id="快慢指针删除链表倒数第n个节点">3、快慢指针删除链表倒数第n个节点</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/remove-nth-node-from-end-of-list/">19. Remove Nth Node From End of List</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params">self, head: ListNode, n: <span class="hljs-built_in">int</span></span>) -&gt; ListNode:<br>        <span class="hljs-comment"># 创建一个虚拟节点，并将其下一个指针设置为链表的头部</span><br>        dummy_head = ListNode(<span class="hljs-number">0</span>, head)<br>        <br>        <span class="hljs-comment"># 创建两个指针，慢指针和快指针，并将它们初始化为虚拟节点</span><br>        slow = fast = dummy_head<br>        <br>        <span class="hljs-comment"># 快指针比慢指针快 n+1 步</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n+<span class="hljs-number">1</span>):<br>            fast = fast.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-comment"># 移动两个指针，直到快速指针到达链表的末尾</span><br>        <span class="hljs-keyword">while</span> fast:<br>            slow = slow.<span class="hljs-built_in">next</span><br>            fast = fast.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-comment"># 通过更新第 (n-1) 个节点的 next 指针删除第 n 个节点</span><br>        slow.<span class="hljs-built_in">next</span> = slow.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>        <br>        <span class="hljs-keyword">return</span> dummy_head.<span class="hljs-built_in">next</span><br></code></pre></td></tr></table></figure></li></ol><h1 id="环形链表">4、环形链表</h1><ol type="1"><li><p><a href="https://leetcode.cn/problems/linked-list-cycle-ii/">142. Linked List Cycle II</a></p><p>这个题目首先要利用快慢指针（步长分别为1、2）求出一个圈的长度是多少，然后利用删除链表中倒数第n个节点中的思想再次使用快慢指针（一个指针领先头指针一个圈的长度）求出入圈节点；下图为另一种做法：上</p><figure><img src="https://code-thinking.cdn.bcebos.com/gifs/142.%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8II%EF%BC%88%E6%B1%82%E5%85%A5%E5%8F%A3%EF%BC%89.gif" alt="另一种做法"><figcaption aria-hidden="true">另一种做法</figcaption></figure></li></ol>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>力扣</tag>
      
      <tag>链表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TSCIL论文笔记·I</title>
    <link href="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/"/>
    <url>/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="阅读论文">〇、阅读论文</h1><ol type="1"><li><a href="https://arxiv.org/abs/1904.07734">增量学习的三种场景及方法：Three scenarios for continual learning</a></li><li><a href="https://arxiv.org/abs/2112.08654">预训练模型图像类增量学习：Learning to Prompt for Continual Learning</a></li><li><a href="https://arxiv.org/abs/2402.12035">时间序列类增量学习的基准：Class-incremental Learning for Time Series:Benchmark and Evaluation</a></li><li><a href="https://arxiv.org/abs/2204.04799">双提示增量学习：DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning</a></li><li><a href="https://arxiv.org/abs/2402.03885">时间序列大模型：MOMENT: A Family of Open Time-series Foundation Models</a></li><li><a href="https://arxiv.org/abs/2403.20317">卷积提示：Convolutional Prompting meets Language Models for Continual Learning</a></li></ol><h1 id="一相关工作">一、相关工作</h1><h2 id="增量学习的三种场景及方法">1. 增量学习的三种场景及方法</h2><h3 id="实验方法">1.1 实验方法</h3><ol type="1"><li>正则化（SI、EWC）</li><li>重放（Replay）<ol type="1"><li>用之前模型对当前任务的Input进行标注生成伪数据（LwF）</li><li>生成新数据（DGR、DGR+distill）</li><li>原始数据重现（可能会有隐私或内存问题）</li></ol></li><li>Exemplars（特征提取+最近类均值+重放：iCaRL）</li></ol><h3 id="实验结果">1.2 实验结果</h3><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/MNIST结果.png" alt="实验结果对比：正则化的效果普遍不太行"><figcaption aria-hidden="true">实验结果对比：正则化的效果普遍不太行</figcaption></figure><h2 id="预训练模型图像类增量学习">2. 预训练模型图像类增量学习</h2><h3 id="实验方法-1">2.1 实验方法</h3><ol type="1"><li>设Input为<span class="math inline">\(x\)</span>，通过<span class="math inline">\(q(x)\)</span>进行映射</li><li>计算<span class="math inline">\(q(x)\)</span>与Key的余弦相似度并选择TopN（同一Prompt被选择的次数越多，下次被选择的概率越小）</li><li>将TopN向量，以及进入Embedding层的Input拼接，并放入预训练模型</li></ol><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/L2P.png" alt="L2P"><figcaption aria-hidden="true">L2P</figcaption></figure><h4 id="vit">2.1.1 ViT</h4><p>在细读论文时需要一些ViT的知识，于是找到了这篇博客：<a href="https://blog.csdn.net/MengYa_Dream/article/details/126600748">ViT的cls token作用</a></p><ol type="1"><li>论文中假设输入的2D图像<span class="math inline">\(x \in \mathbb{R}^{H \times W \times C}\)</span>中<span class="math inline">\(H\)</span>，<span class="math inline">\(W\)</span>，<span class="math inline">\(C\)</span>分别代表高度，宽度和通道数</li><li><span class="math inline">\(f=f_r \circ f_e\)</span>是一个复合函数，指的是将输入embedding后送入自注意力层的结果</li><li>ViT中，图像需要切分为若干patch；在这里flattened 2D patches <span class="math inline">\(x_p \in \mathbb{R}^{L \times (S^2 \cdot C)}\)</span>，其中<span class="math inline">\(L\)</span>为patches个数，<span class="math inline">\(S\)</span>为patch size</li><li>为了简化表示，<span class="math inline">\(x_p\)</span>的第一个token被当做是<code>[class token]</code></li><li>embedding layer的作用<span class="math inline">\(f_e:\mathbb{R}^{L \times (S^2 \cdot C)} \rightarrow \mathbb{R}^{L \times D}\)</span>，其中<span class="math inline">\(D\)</span>是嵌入维度</li><li>一个Prompt可以看作是<span class="math inline">\(P_e\in \mathbb{R}^{L_p \times D} ,L_p\)</span>是Prompt的长度</li><li>将Prompt和Input拼接起来可以得到<span class="math inline">\(x_p=[P_e;x_e]\)</span></li><li>送入预训练模型后，即<span class="math inline">\(f=f_r(x_p)\)</span></li></ol><h4 id="prompt-pool">2.1.2 Prompt Pool</h4><p>为什么要采用Pool的原因有三：</p><ol type="1"><li>在推理的时候task identity是不得而知的，因此为每个task设置一个prompt是不可行的</li><li>即使推理时上述信息可以得知，那么task-independent prompt可能阻止了相似任务间的信息共享</li><li>如果只用一个prompt，则会造成灾难性遗忘</li></ol><p>于是定义Prompt集合：<span class="math inline">\(P = \{P_1, P_2, \cdots , P_M \}\)</span>，则ViT的输入为：<span class="math inline">\(x_p = [P_{s_1} ; \cdots ; P_{s_N} ; x_e], 1 ≤ N ≤ M\)</span></p><p>Prompt Pool中其实是Key-Value pair：<span class="math inline">\(\{(k_1, P_1), (k_2, P_2), \cdots , (k_M , P_M )\},k_i \in \mathbb{R}^{D_k}\)</span></p><div class="note note-info">            <p>根据论文来看，M为所有Prompt 的数量，是一个可以定义的参数</p>          </div><h4 id="prompt-query">2.1.3 Prompt Query</h4><ol type="1"><li>Query应仅跟输入实例有关，并且没有可学习的参数；定义Query Function：<span class="math inline">\(q : \mathbb{R}^{H\times W \times C} → \mathbb{R}^{D_k}\)</span></li><li>作者用了ViT作为feature extractor：<span class="math inline">\(q(x) = f (x)[0,:]\)</span>，其实就是<code>[class token]</code>对应的向量<ul><li>同时，作者指出用ConvNet也是可行的</li></ul></li><li>定义相似度计算函数<span class="math inline">\(\gamma : \mathbb{R}^{D_k} \times \mathbb{R}^{D_k} \rightarrow \mathbb{R}\)</span>，作者发现余弦相似度就很好</li><li>问题简化为<span class="math inline">\(K_x = \mathop{argmin}\limits_{\{s_i\}^N_{i=1}\in[1,M ]} \sum\limits_{i=1}^N \gamma (q(x), k_{s_i} )\)</span>，其中<span class="math inline">\(K_x\)</span>是所有Key的一个子集<ul><li>为了让每个Prompt都有机会被引用，训练时改写为<span class="math inline">\(K_x = \mathop{argmin}\limits_{\{s_i\}^N_{i=1}\in[1,M ]} \sum\limits_{i=1}^N \gamma (q(x), k_{s_i} )\cdot h_{s_i}\)</span></li></ul></li></ol><h4 id="loss-function">2.1.4 Loss Function</h4><p><span class="math display">\[\mathop{\rm min} \limits_{P,K,\phi} \mathcal{L}(g_{\phi}(f^{avg}_r (x_p)), y) + \lambda\sum\limits_{K_x}  \gamma (q(x), k_{s_i} ),\quad s.t. K_x在2.1.3中已经得到\]</span></p><h3 id="实验结果-1">2.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/CIFAR.png"></p><h2 id="时间序列类增量学习的基准">3. 时间序列类增量学习的基准</h2><h3 id="实验方法-2">3.1 实验方法</h3><h4 id="正则化">3.1.1 正则化</h4><p>基于正则化的方法（LwF、MAS、<span class="math inline">\(DT^2W\)</span>）只在UCI-HAR和UWave等任务较少的简单数据集中表现出明显的优势</p><h4 id="归一化bn-vs-ln">3.1.2 归一化：BN vs LN</h4><p>在大多数情况下使用LN的效果要比BN更好</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/Norm.png" alt="Norm"><figcaption aria-hidden="true">Norm</figcaption></figure><h4 id="rehearsal-baseder-vs-gr">3.1.3 Rehearsal-based：ER vs GR</h4><p>GR在简单数据上较好，甚至好过ER，复杂数据上表现不佳；可能是因为难以生成复杂数据，以及无法控制生成的类别</p><h4 id="meomory-budget在bn和ln上的差异">3.1.4 Meomory budget在BN和LN上的差异</h4><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/memory.png"></p><ul><li>基于ER的方法通常性能会随着Memory budget的增大而提高，到达一定程度会趋于饱和</li><li>在内存足够时，BN在除ASER的方法都与Offline有很大差异，因为随着任务数量增加<span class="math inline">\(B_{M}\)</span>中旧样本比例会减少；LN则接近Offline</li></ul><h4 id="分类器的选择">3.1.5 分类器的选择</h4><p>分类器选择取决于方法与数据集，在ER上的区别并不明显</p><h2 id="双提示增量学习">4. 双提示增量学习</h2><h3 id="实验方法-3">4.1 实验方法</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/dual.png"></p><ol type="1"><li>G-Prompt（唯一，学习任务的一般性）和E-Prompt（每个任务对应一个，学习任务的特殊性）</li><li>利用Prefix-Tuning（效果相比Prompt-Tuning更好，且未改变输入维度）将<span class="math inline">\(p\)</span>分解成<span class="math inline">\(p_{K},p_{V}\in \mathbb R^{L_{p}/2\times D}\)</span>，并且加到多头自注意力层中：<span class="math inline">\(f^{\rm Pre-T}_{\rm prompt}(p,h)={\rm MSA}(h_{Q},[p_{k};h_{K}],[p_{v};h_{V}])\)</span></li><li>训练时，利用<span class="math inline">\(\mathcal L_{\rm match}(x,k_t)=\gamma(q(x),k_t) ,x \in \mathcal D_t\)</span>更新<span class="math inline">\(k_t\)</span>，将G-Prompt和E-Prompt分别加到某些MSA中</li><li>推理时，Input通过Query function映射并选择最相似的E-Prompt</li></ol><h3 id="实验结果-2">4.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/dual_res.png"></p><h2 id="时间序列大模型">5. 时间序列大模型</h2><div class="group-image-container"><div class="group-image-row"><div class="group-image-wrap"><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/pile.png"></div><div class="group-image-wrap"><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_radar.png"></div></div></div><h3 id="time-series-pile">5.1 Time-series Pile</h3><h3 id="模型结构">5.2 模型结构</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment.png"></p><ol type="1"><li>将时间序列数据分为不重叠的固定长度序列patches（减少内存占用和计算复杂度），映射到D维向量</li><li>在预训练期间，通过使用特殊的掩码嵌入[MASK]来替换 patches嵌入，目标是学习 patches 嵌入</li><li>这些嵌入可以使用轻量级的重建头，而不是与编码器相同大小的解码器，来重建输入时间序列。以便在保持编码器的大部分参数和高级特征不变的同时，对有限数量的可训练参数进行任务特定的微调，从而进行必要的架构修改。</li></ol><h3 id="实验结果-3">5.3 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_res1.png"></p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/moment_res2.png" alt="零样本泛化能力"><figcaption aria-hidden="true">零样本泛化能力</figcaption></figure><h2 id="卷积提示">6. 卷积提示</h2><h3 id="实验方法-4">6.1 实验方法</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/卷积.png"></p><ol type="1"><li>GPT-3给出每个任务中类别的visual features，利用BERT进行word embedding</li><li>根据当前任务<span class="math inline">\(t\)</span>和之前任务中类别的visual features计算相似度，取最大相似度的平均值为<span class="math inline">\(sim_{t}\)</span></li><li><span class="math inline">\(J_{max}\)</span>为每个任务的最大Prompt Generator数量，于是$J_{t}=(1-sim_{t}) J_{max} $</li><li>输入图像，<span class="math inline">\(PN_{\phi}\)</span>（唯一）将前一层的<code>[CLS]</code>投影到和Prompt Keys（共M个，唯一） 相同维度并计算余弦相似度得到<span class="math inline">\(s_{i}\)</span></li><li>对于每一层、每一个头有一套Shared Embedding（唯一）和一套Prompt Generator（M个，对应于Prompt Keys；训练时之前任务对应的Generator会被冻结）；对于<span class="math inline">\(G^K_{h,l,m}\)</span>、<span class="math inline">\(G^V_{h,l,m}\)</span>分别与<span class="math inline">\(SE^{K}_{l,h}\)</span>，<span class="math inline">\(SE^{V}_{l,h}\)</span>进行卷积后，和<span class="math inline">\(s_{i}\)</span>相乘求和，将结果concat到下一层：<span class="math inline">\(P_{l+1}^{K}=\sum\limits_{m=1}^{M}s_{l,m}PC^{K}_{l,h,m}\)</span>，<span class="math inline">\(P_{l+1}^{V}=\sum\limits_{m=1}^{M}s_{l,m}PC^{V}_{l,h,m}\)</span></li></ol><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/相似类别.png" alt="根据visual features计算相似度"><figcaption aria-hidden="true">根据visual features计算相似度</figcaption></figure><h3 id="实验结果-4">6.2 实验结果</h3><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/con_cifar100.png"></p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/con_imagenet.png"></p><h1 id="二l2p">二、L2P</h1><h2 id="踩坑">1. 踩坑</h2><p>在服务器上部署之后，Github上提到没有CIFAR-100的话，在<code>datasets.py</code>更改如下字段：</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/教程.png" alt="Github上的教程"><figcaption aria-hidden="true">Github上的教程</figcaption></figure><p>但其实不用改的，本来就是<code>True</code></p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/源码1.png" alt="datasets.py"><figcaption aria-hidden="true">datasets.py</figcaption></figure><p>不过在运行的时候会报这个错：</p><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/报错.png" alt="报错说没有cifar100_12p这个路径"><figcaption aria-hidden="true">报错说没有cifar100_12p这个路径</figcaption></figure><p>后来找到了一个脚本文件<code>train_cifar100_l2p.sh</code>执行，但本地并没有<code>local_datasets</code>路径：</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/路径.png"></p><p>这个报错上面提到，这个数据集是在<code>torchvision</code>里面<code>cifar.py</code>下载的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">[rank0]:   File <span class="hljs-string">&quot;/home/eee/zym/l2p-pytorch/datasets.py&quot;</span>, line <span class="hljs-number">107</span>, <span class="hljs-keyword">in</span> get_dataset<br>[rank0]:   dataset_train = datasets.CIFAR100(args.data_path, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform_train)<br>[rank0]:   File <span class="hljs-string">&quot;/home/eee/anaconda3/envs/zym/lib/python3.8/site-packages/torchvision/datasets/cifar.py&quot;</span>, line <span class="hljs-number">66</span>, <span class="hljs-keyword">in</span> __init__<br>[rank0]:     <span class="hljs-variable language_">self</span>.download()<br></code></pre></td></tr></table></figure><p>新建了<code>local_datasets</code>后运行脚本文件，还是报错；最后发现脚本路径写的有问题，改成下面即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">--data-path ./local_datasets/ \<br></code></pre></td></tr></table></figure><h2 id="分析">2. 分析</h2><h3 id="代码结构">2.1 代码结构</h3><figure><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/结构.png" alt="代码结构"><figcaption aria-hidden="true">代码结构</figcaption></figure><p>关于为什么要出现<code>original_model</code>和<code>model</code>，是因为作者在这里做了消融实验，微调了model中的部分参数。不过从结果来看，并没有比原始论文的要好。</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/冻结.png"></p><h3 id="运行结果">2.2 运行结果</h3><p>注意到Loss出现了负值，是因为采用的多元交叉熵损失函数。</p><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/loss.png"></p><h1 id="三初步设计">三、初步设计</h1><p><img src="/2024/08/17/TSCIL%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%C2%B7I/流程.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文</tag>
      
      <tag>类增量学习</tag>
      
      <tag>微调</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BIGAI1-基于Langchain构建用于数据生成Agent</title>
    <link href="/2024/08/06/%E5%9F%BA%E4%BA%8ELangchain%E6%9E%84%E5%BB%BA%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90Agent/"/>
    <url>/2024/08/06/%E5%9F%BA%E4%BA%8ELangchain%E6%9E%84%E5%BB%BA%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90Agent/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><p>类似的文章已经在<a href="https://blog.csdn.net/sersama/article/details/139179060?spm=1001.2014.3001.5502">CSDN</a>发布过了，在这里进行简单总结。</p><h1 id="背景">背景</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>实习</tag>
      
      <tag>BIGAI</tag>
      
      <tag>数据生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>百度-基于大模型的广告检索</title>
    <link href="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/"/>
    <url>/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="数据分析以及初赛方法">数据分析以及初赛方法</h1><h2 id="query落地页核心词长度分布">Query、落地页、核心词长度分布</h2><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/query.png" alt="query"><figcaption aria-hidden="true">query</figcaption></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/lp.png" alt="lp"><figcaption aria-hidden="true">lp</figcaption></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/ct.png" alt="ct"><figcaption aria-hidden="true">ct</figcaption></figure><h2 id="广告id分布">广告ID分布</h2><p>在发现Query中的ID远少于广告ID数量后，我想统计ID的分布。最开始采用柱状图，但由于数据量过大，最后绘制出的图像不直观。因此，我向Kimi问到了一种估计分布密度的方法<a href="https://blog.csdn.net/weixin_39910711/article/details/107307509">KDE</a>，这是一种非参数估计方法，相对于参数估计（如：最大似然估计）不需要对数据的分布有先验的认识：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> gaussian_kde<br><br>json file =<span class="hljs-string">&#x27;/home/zym/桌面/FEIJIANG/Doc/Doc_train_ultra.json&#x27;</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(json file,<span class="hljs-string">&#x27;r&#x27;</span>,encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>)<span class="hljs-keyword">as</span> file:<br>    js =[json.loads(line)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file]<br>    <br>tgt_values=[ _[<span class="hljs-string">&#x27;tgt&#x27;</span>]<span class="hljs-keyword">for</span> <span class="hljs-keyword">in</span> js]<br>tgt_array=np.array(tgt_values,dtype=np.float64)<br><br><span class="hljs-comment"># 将列表转换为NumPy数组</span><br>tgt_array = np.array(tgt_values, dtype=np.float64)<br><br><span class="hljs-comment"># 计算核密度估计，使用高斯核</span><br>kde = gaussian_kde(tgt_array)<br><br><span class="hljs-comment"># 绘制平滑的曲线</span><br>x = np.linspace(np.<span class="hljs-built_in">min</span>(tgt_array), np.<span class="hljs-built_in">max</span>(tgt_array), <span class="hljs-number">1000</span>)<br>y = kde(x)<br></code></pre></td></tr></table></figure><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/ID分布.png" alt="初赛数据的广告ID分布"><figcaption aria-hidden="true">初赛数据的广告ID分布</figcaption></figure><h2 id="训练过程">训练过程</h2><p>在论文中</p><h1 id="baseline">Baseline</h1><p>数据集中有88wDoc，因为论文中提到直接索引效果较好，因此取落地页连续的短语作为Doc；特别地，观察到Doc长度分布不均，因此在生成Doc时设定为长度越长、采样次数越多。最终得到880wDoc，在第一阶段进行索引学习时4000steps已趋于收敛，遂暂停。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/简单Doc.png" alt="简单构建Doc"><figcaption aria-hidden="true">简单构建Doc</figcaption></figure><p>对于Query，按Click进行增强，从500w得到1200w数据。在上一步得到的权重上训练，效果还可以。</p><h1 id="尝试一semantic-docid">尝试一：Semantic DocID</h1><p>官方所给的数据集中ID是没有规律的，不同的广告内容ID可能邻近，相似的广告ID可能很远，因此这对生成式模型而言，像是在做一个“多分类”的任务。在这篇<a href="https://arxiv.org/abs/2309.13335">Model-enhanced Vector Index</a>论文中提到利用Residual Quantization进行层次聚类效果可能会好，于是考虑用模型对广告内容进行Embedding，但因为本地没有显卡用，并且在AIstudio又不能用Pytorch，因此采用<a href="https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers/ERNIE/contents.html">Ernie-3.0-mini</a>。</p><h2 id="q1embedding文件太大">Q1：Embedding文件太大</h2><p>即使采用mini，在对约29000个句子（3%）进行Embedding之后，我的json文件就已经达到了220MB</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/mini.png"></p><p>文件存储格式如下</p><p><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/json.png"></p><h2 id="q2内存过大">Q2：内存过大</h2><p>因为词袋方法耗时更长，占用内存更大，后来又再次尝试使用Embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;/home/aistudio/embed.json&quot;</span>,<span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    data=[json.loads(line)<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br></code></pre></td></tr></table></figure><p>居然在读取的时候内存就满了。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/内存.png" alt="内存满了"><figcaption aria-hidden="true">内存满了</figcaption></figure><h1 id="尝试二词袋">尝试二：词袋</h1><p>这里使用Paddle里的Taskflow进行分词。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./embed.json&quot;</span>,<span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> f:<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,<span class="hljs-built_in">len</span>(res)+<span class="hljs-number">1</span>)):<br>        seg = Taskflow(<span class="hljs-string">&quot;word segmentation&quot;</span>)temp = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x:x!=,<span class="hljs-string">&#x27;,seg(res[str(i)])))</span><br><span class="hljs-string">        # 使用counter计算每个分词出现的次数</span><br><span class="hljs-string">        word count= counter(temp)</span><br><span class="hljs-string">        #按照出现次数进行升序排序</span><br><span class="hljs-string">        sorted word count = dict(sorted(word count.items(), key=lambda item: item[1]))</span><br><span class="hljs-string">        json.dump(&#123;&quot;tgt&quot;:i,&quot;bow&quot;: sorted word count&#125;,f,ensure ascii=False)</span><br></code></pre></td></tr></table></figure><p>最终分词结果文件很大，没有进一步处理。</p><figure><img src="/2024/08/02/%E7%99%BE%E5%BA%A6-%E5%9F%BA%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B9%BF%E5%91%8A%E6%A3%80%E7%B4%A2/分词.png" alt="分词结果"><figcaption aria-hidden="true">分词结果</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型</tag>
      
      <tag>百度</tag>
      
      <tag>搜广推</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BIGAI2—微调Qwen-7B生成1-Hop描述</title>
    <link href="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/"/>
    <url>/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="背景">背景</h1><p>在生成QA时，我们希望生成的数据具有<mark>一定的复杂度</mark>，并且具有<mark>生活化</mark>的特点。</p><p>最基本的一个想法是：当我们需要某个子图（Region Graph）中的物体信息，便直接从已有信息Info中查找，并送入GPT生成QA；然而，基于这种方法所生成的QA十分的简单。我利用GPT-4简单生成了一个Demo，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Base.png" alt="Demo:直接通过知识图谱生成的QA"><figcaption aria-hidden="true">Demo:直接通过知识图谱生成的QA</figcaption></figure><p>很显然这种数据不具有<mark>多样性</mark>，也不满足我们的要求。我们认为在若干定语修饰后的QA可以极大地丰富数据多样性：</p><ul><li>地毯左面紧挨的儿童椅和行李箱旁的双人床哪个更大？</li><li>空调下面的挂画和墙壁右边的衣服哪个数量更多？</li></ul><p>本质上，我们希望对原有数据增加一定的定语——<mark>将程式化的短语转变为流畅丰富的句子</mark>，效果如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Improve.png" alt="1-Hop描述（右）可以丰富QA内容"><figcaption aria-hidden="true">1-Hop描述（右）可以丰富QA内容</figcaption></figure><p>生成大批量数据离不开大模型的帮助。出于对计算资源以及生成效率的考虑，我们没有选择API或是参数量更大的模型，而是在本地部署Qwen-7B，并期待通过微调可以让其生成我们希望的1-Hop描述，如下图框出部分：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/流程.png" alt="生成QA数据集流程"><figcaption aria-hidden="true">生成QA数据集流程</figcaption></figure><h1 id="方法">方法</h1><h2 id="原理浅析">原理浅析</h2><p>除了SFT之外，人们采用更少计算资源进行微调的方法称为PEFT，其中又包含若干种方法。</p><p>我在网上找到的一个时间轴，可以很好地展示发展历程，如下图：</p><figure><img src="https://oscimg.oschina.net/oscnet/9ac87619-bf4f-41d6-9c94-d7a8cb030acb.png" alt="主流微调方法的发展"><figcaption aria-hidden="true">主流微调方法的发展</figcaption></figure><p>因为显卡资源的限制，我们采用P-tuning来进行微调，从而生成1-Hop描述，<a href="https://arxiv.org/abs/2103.10385">论文原文</a>。</p><p>与Prefix-tuning直接给Prompt增加前缀不同，P-tuning在Prompt中随意加入Embedding后的Virtual Token，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/P-tuning.png" alt="P-tuning结构示意图"><figcaption aria-hidden="true">P-tuning结构示意图</figcaption></figure><h2 id="训练数据">训练数据</h2><p>考虑到时间成本以及Token成本，我们决定在Info中选取<mark>部分数据</mark>，将这一部分数据和利用GPT-4生成的1-Hop描述拼接在一起，形成了微调用的训练数据集。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/数据.png" alt="准备微调数据"><figcaption aria-hidden="true">准备微调数据</figcaption></figure><p>通过将人工制作的模板（如下图）与Info中的信息作为Prompt，我们利用GPT-4进行生成；生成的内容需要进一步通过正则表达式的处理，最终成为1-Hop描述。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT1.png" alt="模板1"><figcaption aria-hidden="true">模板1</figcaption></figure><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT2.png" alt="模板2"><figcaption aria-hidden="true">模板2</figcaption></figure><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GPT3.png" alt="模板3"><figcaption aria-hidden="true">模板3</figcaption></figure><h2 id="训练过程">训练过程</h2><p>Qwen-7B接收到的Prompt由另一套模板以及Info中的信息组成，如下图：</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/Prompt.png" alt="询问功能的模板"><figcaption aria-hidden="true">询问功能的模板</figcaption></figure><p>代码需要放到HGX集群上跑，但是需要排队很久。之前微调的时候loss从14下降到8左右。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/HGX.png" alt="GPU集群"><figcaption aria-hidden="true">GPU集群</figcaption></figure><h1 id="结果">结果</h1><p>整个对Info进行增强生成1-Hop描述的过程很慢，最终也没有将整个map的3000个视角（1000个场景*3个视角）遍历完。</p><figure><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/结果.png" alt="结果"><figcaption aria-hidden="true">结果</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>微调</tag>
      
      <tag>实习</tag>
      
      <tag>BIGAI</tag>
      
      <tag>数据生成</tag>
      
      <tag>大模型</tag>
      
      <tag>P-tuning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
