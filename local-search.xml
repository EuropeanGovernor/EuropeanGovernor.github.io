<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BIGAI2—微调Qwen-7B</title>
    <link href="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/"/>
    <url>/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/</url>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在生成QA时</p><p><img src="/2024/07/31/%E5%BE%AE%E8%B0%83Qwen-7B/GenRetr.drawio.png" alt="生成QA数据集流程"></p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>除了SFT之外，人们采用更少计算资源进行微调的方法称为PEFT，其中又包含若干种方法。</p><p>在网上我找到的一个时间轴，可以很好地展示发展历程，如下图：</p><p><img src="https://oscimg.oschina.net/oscnet/9ac87619-bf4f-41d6-9c94-d7a8cb030acb.png" alt="主流微调方法的发展"></p><h2 id="P-tuning"><a href="#P-tuning" class="headerlink" title="P-tuning"></a>P-tuning</h2><p>在生成1-Hop描述时，我们采用P-tuning进行微调。</p><p>[原论文]: <a href="https://arxiv.org/abs/2103.10385">https://arxiv.org/abs/2103.10385</a>“论文”</p><p>与BERT在预训练任务时进行完型填空（在离散空间中）不同，P-tuning在输入前加入Virtual Token并直接转化为Embedding，如下图：</p><p><img src="/P-tuning.png" alt="P-tuning结构示意图"></p><h1 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h1>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BIGAI</tag>
      
      <tag>大模型</tag>
      
      <tag>实习</tag>
      
      <tag>数据生成</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
